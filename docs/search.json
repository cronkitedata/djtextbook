[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Reporting",
    "section": "",
    "text": "Preface\nThis book serves as a compilation of of handouts, websites and tutorials that I have created in my data reporting class at ASU’s Cronkite School of Journalism and Mass Communication, IRE and NICAR conferences, and as an adjunct at the Columbia University Graduate School of Journalism. Some of the material will be useful in other courses or for self-study, but it is primarily aimed at the investigative journalism masters’ students at Cronkite.\nIt covers:\nIt doesn’t cover:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Data Reporting",
    "section": "",
    "text": "Reporting and replication in data journalism\nAnalyzing data for stories using R and R Markdown\nFinding and creating data for stories\n\n\n\nData visualization for publication\nWorking with non-tabular data such as images, sound or document collections\nCreating news applications\nFreedom of Information and public records techniques\nData science. This is a journalism book, not a programming or statistics book.\n\n\nNavigation\nEach major section on the left can be expanded to see the chapters included. The search box above the navigation index is best used to find sections and headings rather than phrases. Skip to sections of the current chapter using the menu on the right.\n\n\nR or Python?\nIf you ask a data scientist or technologist which language you should learn first, you’ll start a heated debate between advocates of R, Python, Javascript , SQL, Julia and others. Ask the same question of a data journalist and the answer will be: “Choose one that is free and that your colleagues use so you can get help.” For our purposes, it really doesn’t matter – any of the standard languages will do.\nMy only rule is that you stick to your first language for a little while before trying a new one. It would be like trying to learn Portuguese and Spanish at the same time, when you know neither one to begin with. They’re related, but very different.\nMost employers won’t care which programming language you know because it’s relatively easy to learn another once you’re comfortable with the concepts and good data journalism habits. In a few cases, such as the Associated Press, R is preferred. In others, like the Los Angeles Times, it’s a little easier to work with the team if you work in Python. Visualization teams work primarily in Javascript. But they’ll mainly just be happy that you are reasonably self-sufficient in any language.\nI chose R because I find it a little easier to use when trying to puzzle something out step by step, and it is particularly good at working with the weird and varied forms of data thrown at us, but it’s really just a matter of taste and comfort.\nThat said, this book is oriented toward the “tidyverse”, which comprises a whole host of methods that are designed to work together using common syntax and concepts. There are many other ways to do almost everything presented in this book, which we won’t cover.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#conventions-used-in-this-book",
    "href": "index.html#conventions-used-in-this-book",
    "title": "Data Reporting",
    "section": "Conventions used in this book",
    "text": "Conventions used in this book\n\nKeyboard translations\nThis book is written using a Mac desktop keyboard, meaning there may be key combinations you don’t have.\nGenerally, when Windows users see the key cmd or CMDCMD, they should use ctl or ^ instead; when you see OPTOPT, use ALTALT instead.\nKeyboard combinations are generally shown like this: CMD + SHIFT + i , where the + indicates the keys at once, such as cmd-Shift-icmd-Shift-i or Control-Shift-iControl-Shift-i\n\n\nConventions used in tutorials\n\nExample code\nHere is what some code looks like in the book. There are sometimes explanatory notes to go along with specific lines of code. Clicking on the explanation will highlight the matching line of code.\n\n1library (tidyverse )\n\n\n1\n\nLoad the tidyverse library\n\n\n\n\nYou can copy the code chunk whenever you see the clipboard in it, but I implore you to try it yourself for 10 minutes. It will pay off handsomely if you have to piece together what the code is doing and if you have to learn to read the error messages.\n\n\n\nChanges from previous years\nPreviously, the conceptual and story examples were scattered throughout tutorials. This made it hard for students to know what they had to do – read the concepts or go through the tutorials? They tended to skip the more journalistic portions of the text. This year, I’ve split them out and interleaved those chapters where they made sense (at least to me).\n“In this chapter:” sections were removed. No one seemed to pay attention to it, and the section just made each chapter that much longer. Instead, I’ve moved the “On this page” section to the right, making each page a little narrower but making in-chapter navigation easier.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Data Reporting",
    "section": "Colophon",
    "text": "Colophon\nI’m grateful to all of the trainers, experts and collaborators who have made their training materials open to the world, and they are linked prominently throughout this book. Any errors or omissions are my own.\nThis year’s book was reorganized to remove much of the conceptual work from spreadsheets to more generic chapters as needed, such as “Filtering and sorting to find stories”. I’ve also removed the math chapter, under the idea that anyone taking this class understands the need for basic arithmetic and statistics. Both of these sections are still available in the appendices.\nSome chapters were updated with new, useful versions of functions, but others were left using the more traditional methods. These changes were and were not made for purposes of teaching – I chose which one to use based on ease of use vs. understanding code you will find in the wild.\n\n\n\n\n\n\nA note on language\n\n\n\nI’ll use the words “read” and “write” in a generic sense. I intend them to cover the range of journalistic media, including listening, watching, and news applications or graphics.\n\n\nThis book was written using Quarto version 1.3 in RStudio with version 4.3 of R. . The complete source is available on Github. Previous editions are saved as branches in the github repo, and also saved as static releases.\n\n\n\n– Sarah Cohen, Winter 2023-24 sarah.h.cohen@asu.edu",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "start.html",
    "href": "start.html",
    "title": "Reporting with data",
    "section": "",
    "text": "Data reporting has come to mean many things. One of the best descriptions that fits this course is the moniker, “Empirical Journalism”. For us, that means taking a systematic approach to finding, acquiring, evaluating and analyzing all kinds of data for the purpose of uncovering information that was hidden or otherwise ill-understood by the general public. It will teach you to use documents and data as sources like any other, with flaws and motivations that might thwart or help you report a story.\nThis introductory section reminds you that data reporting is, indeed, reporting.",
    "crumbs": [
      "Reporting with data"
    ]
  },
  {
    "objectID": "start-story.html",
    "href": "start-story.html",
    "title": "1  Learn a new way to read",
    "section": "",
    "text": "1.1 Story development\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.1\nSometimes, the hypothesis might be more formal.\nIn 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations.\nThat story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#story-development",
    "href": "start-story.html#story-development",
    "title": "1  Learn a new way to read",
    "section": "",
    "text": "Compared to what?\nInvestigative stories are based on the idea that something is happening that shouldn’t. This suggests that the journalists can tell the difference between a “good” and “bad” outcome. You should look for that comparison in stories.\nThese reporters often look for a legal or regulatory standard, such as the number of violations allowed before shutting down a restaurant, to define their comparisons. Sometimes it’s a standard that lawmakers make for themselves by promising results from a program or law. Sometimes they can find industry standards, such as licensing rules for professionals. But you might end up comparing your area to others, or today versus yesterday.\nAt the New York Times, in a story on the problem of police-involved domestic violence, we struggled to find the right “compared to what?” Statistics wouldn’t help, since they might have come from police departments ignoring the problem. Instead, we found procedures recommended by a police chiefs’ association that defined best practices, and used them to compare the behavior of the largest departments in the U.S. We also compared disciplinary actions for domestic violence against other violations, such as testing positive for marijuana or stealing from the precinct.\nWhen you read an investigative story, be sure to watch for any comparisons. There’s no point in publishing an investigation if it didn’t find something newsworthy. No trend or number is newsworthy on its own – only in context or in comparison with something else.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#go-beyond-the-numbers",
    "href": "start-story.html#go-beyond-the-numbers",
    "title": "1  Learn a new way to read",
    "section": "1.2 Go beyond the numbers",
    "text": "1.2 Go beyond the numbers\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nWhen I was analyzing some housing court data for The New York Times, one fact hit me as I was looking at a timeline of eviction cases: The most cases ever filed in one of the city’s courts happened during a Thanksgiving week one year. It was the kind of detail that could have been compelling in a story if it had been more recent. But other anecdotes led directly to lines of reporting in the story. One was the profile of a process server who filed court papers claiming to have delivered six eviction notices in 12 minutes, seemingly acquiring “some of the salient skills of a mountain goat”.2\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, we examined press releases and other easy-to-get sources before calling and visiting the recipients or towns.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#reading-tips",
    "href": "start-story.html#reading-tips",
    "title": "1  Learn a new way to read",
    "section": "1.3 Reading tips",
    "text": "1.3 Reading tips\nYou’ll get better at reading investigations and data-driven work over time. For now, try to answer these questions about an investigative story as you read it.\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data , or a big idea that was illustrated with characters ?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nWhat is their comparison? Nothing is newsworthy unless it’s compared to something that isn’t. What’s that something?\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controvesial? How might they have been derived? What might have been the investigative hypothesis?\nDid the story give critics their due and how did reporters try to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#analyze-data-for-story-not-study",
    "href": "start-story.html#analyze-data-for-story-not-study",
    "title": "1  Learn a new way to read",
    "section": "1.4 Analyze data for story, not study",
    "text": "1.4 Analyze data for story, not study\n\nAs journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#exercises",
    "href": "start-story.html#exercises",
    "title": "1  Learn a new way to read",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nIf you’re a member of Investigative Reporters and Editors, go to the site and find a recent prize-winning entry (usually text rather than broadcast). Get a copy of the IRE contest entry from the Resources page. Try to match up what the reporters said they did and how they did it with key portions of the story.\nThe next time you find a good data source, try to find a story that references it. If your data is local, you might look for a story that used similar data elsewhere, such as 911 response times or overdose deaths. But many stories use federal datasets that can easily be localized. Look at a description of the dataset and then the story to see how the data might have been used.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-story.html#footnotes",
    "href": "start-story.html#footnotes",
    "title": "1  Learn a new way to read",
    "section": "",
    "text": "Note to self: check this with Walt. It’s how I remember it, but I’m not positive.↩︎\n“On March 24, 2017, Mr. Moise reported, he delivered notices to six Dunbar tenants in 12 minutes.The Dunbar has 44 stairwells and 536 apartments. Each stairwell has its own front door. There are no elevators. Yet Mr. Moise claimed that, in those 12 minutes, he raced from the third floor of one stairwell to the fourth floor of another stairwell, to the sixth floor of a third stairwell, to the third floor of a fourth stairwell, to the third floor of a fifth stairwell and finally back to the fifth floor of the fourth stairwell. Each time, he needed to ring a doorbell and be let inside the building.”↩︎",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Learn a new way to read</span>"
    ]
  },
  {
    "objectID": "start-math.html",
    "href": "start-math.html",
    "title": "2  Newsroom math",
    "section": "",
    "text": "2.1 Why numbers?\nUsing averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#why-numbers",
    "href": "start-math.html#why-numbers",
    "title": "2  Newsroom math",
    "section": "",
    "text": "Overcome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#overcoming-your-fear-of-math",
    "href": "start-math.html#overcoming-your-fear-of-math",
    "title": "2  Newsroom math",
    "section": "2.2 Overcoming your fear of math",
    "text": "2.2 Overcoming your fear of math\nWhen we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nI had written it a even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, I checked the price for an Olive Garden meal at the time for four people.)",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#put-math-in-its-place",
    "href": "start-math.html#put-math-in-its-place",
    "title": "2  Newsroom math",
    "section": "2.3 Put math in its place",
    "text": "2.3 Put math in its place\nFor journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes , and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#going-further",
    "href": "start-math.html#going-further",
    "title": "2  Newsroom math",
    "section": "2.4 Going further",
    "text": "2.4 Going further\n\nTipsheets\n\nSteve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\n\n\n\nReading and viewing\n\n“Avoiding Numeric Novcain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com\nT. Christian Miller’s “Writing the data-driven story”\nA viral Twitter thread:\n\n\n\nWhat happens in your head when you do 27+48?\n\n— Gene Belcher (@Wparks91) June 25, 2019",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-math.html#exercises",
    "href": "start-math.html#exercises",
    "title": "2  Newsroom math",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nImagine that someone gave you $1 million and you could spend it on anything you want. Write down a list of things that would add up to about that amount. That should be easy. Now, imagine someone gave you $1 billion and you could spend it on whatever you want, but anything left over after a year had to be returned. How would you spend it? (You can give away money, but it can’t be more than 50% of a charity’s annual revenues. So you can’t give 10 $100 million gifts!) See how far you get trying to spend it. A few homes, a few yachts, student loan repayments for all of your friends? You’ve hardly gotten started.\nImagine it is Jan. 1, 2020 and you are tasked with writing the annual weather story, summarizing the high and low points of the previous year. Using this daily summary of temperatures, rain and wind for Phoenix, try to find three interesting facts for your story. If you want to download your own data from NOAA, choose “Local Climatalogical Data,” and keep only the rows that refer to “SOD,” or “Summary of Day”.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Newsroom math</span>"
    ]
  },
  {
    "objectID": "start-data-def.html",
    "href": "start-data-def.html",
    "title": "3  Defining “Data”",
    "section": "",
    "text": "3.1 The birth of a dataset\nIn “The Art of Access”, David Cuillier and Charles N. Davis describe a process of tracking down the life and times of a dataset. Their purpose is to make sure they know how to request it from a government agency. The same idea applies to using data that we acquire elsewhere.\nAs reporters, we usually deal with data that was created in the process of doing something else such as conducting an inspection or paying a parking ticket. These datasets are created in government as part of carrying out their work. They form the basis of much investigative reporting and they are often the subject of public records and FOIA requests. They were born as part of the government doing its job, without any thought given to how it might be used in another way. These are often called “administrative records”.\nAnother type of data might be considered “digital trace” data, which often refers to social media posts, online publications and other items that are born in electronic form and posted publicly for anyone to view and use.\nFinally, there are datasets that are compiled or collected specifically for the purpose of studying something. They might be collected in the form of a survey or a poll, or they might be monitoring systems such as pollution or weather stations. In this instance, the information has intrinsic value AS information.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#granular-and-aggregated-data",
    "href": "start-data-def.html#granular-and-aggregated-data",
    "title": "3  Defining “Data”",
    "section": "3.2 Granular and aggregated data",
    "text": "3.2 Granular and aggregated data\nOne of the hardest concepts for a lot of new data journalists is the idea of granularity of your data source. There are a lot of ways to think about this: individual items in a list vs. figures in a table; original records vs. compilations; granular data vs. statistics.\nGenerally, an investigative reporter is interested in getting data that is as close as possible to the most granular information that exists, at least on computer files. Here’s an example , which might give you a little intuition about why it’s so important to think this way:\n\nExample: Death certificates\nWhen someone dies in the US, a standard death certificate is filled out by a series of officials - the attending physician, the institution where they died and even the funeral direcor.\n\n\n\ndeath certificate\n\n\nHere is a blank version of the standard US death certificate form – notice the detail and the detailed instructions on how it is supposed to be filled out.\nA good reporter could imagine many stories coming out of these little boxes. Limiting yourself to just to COVID-19-related stories: You could profile the local doctor who signed the most COVID-19-related death certificates in their city, or examine the number of deaths that had COVID as a contributing, but not underlying or immediate, cause of death. Or maybe you would want to map the deaths to find the block in your town most devastated by the virus.\nEarly in the pandemic, Coulter Jones and Jon Kamp of the Wall Street Journal examined the records from one of the few states that makes them public, and concluded that “Coronavirus Deaths were Likely Missed in Michigan, Death Certificates Suggest”\n\nBut you probably can’t do that. The reason is that, in most states, death certificates are not public records and are treated as secrets. 2. Instead, state and local governments provide limited statistics related to the deaths, usually by county, with no detail.\nHere’s an example from Arizona — note that we can only see statistics in the way the data source has decided we want to examine them, without access to the underlying information. There’s no way to look at age and race and gender combined for each county, just the generalized statistics for each category alone.\nHere are some of the typical (not universal) characteristics of granular vs. aggregated data:\n\n\n\n\n\n\n\nGranular\nAggregate\n\n\n\n\nIntended for some purpose other than your work\nIntended to be presented as is to the public\n\n\nMany rows (records), few columns (variables)\nMany columns (variables), few rows (records)\n\n\nRequires a good understanding of the source\nExplanatory notes usually come with the data\n\n\nEasy to cross-reference and compile\nOften impossible to repurpose\n\n\nHas few numeric columns\nMay be almost entirely numerical\n\n\nIs intended for use in a database\nIs intended for use in a spreadsheet\n\n\n\nWe often have to consider the trade-offs. Granular data with the detail we need - especially when it involves personally identifiable information like names and addresses - can take months or years of negotiation over public records requests, even when the law allows it. It’s often much easier to convince an agency to provide summarized or incomplete data. Don’t balk at using it if it works for you. But you lose flexibility and have to live with the statistics compiiled for you if you have to settle..\nFor our purposes, it’s important to remember that we can always create aggregated numbers like the ones shown in the Arizona COVID page out of individual items, but you can never de-aggregate statistics into more granular data like the boxes filled out in death certificates.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#unit-of-analysis-know-your-nouns",
    "href": "start-data-def.html#unit-of-analysis-know-your-nouns",
    "title": "3  Defining “Data”",
    "section": "3.3 Unit of analysis: Know your nouns",
    "text": "3.3 Unit of analysis: Know your nouns\nThat brings us to one of the most important things you must find out about any data you begin to analyze: What noun describes each row? In statistics, these rows might be called observations or cases. In data science, they’re usually called records. Either way, every row must represent the same thing – a person, a place, a year, a water sample or a school. And you can’t really do anything with it until you figure out what that noun is.\nSometimes the only level you can obtain creates problems. In 2015, we did a story at The New York Times called “More Deportation Follow Minor Crimes, Records Show” . The government had claimed it was only removing hardened criminals from the country, but our analysis of the data suggested that many of them were for minor infractions.\nIn writing the piece, we had to work around a problem in our data: the agency refused to provide us anything that would help us distinguish one individual from another. All we knew was that each row represented one deportation, not a person. Without a column that uniquely identified people – say, name and date of birth, or some scrambled version of an their DHS number – we had no way to even estimate how often people were deported multiple times. If you read the story, you’ll see the very careful wording, except when we had reported out and spoken to people on the ground.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#further-reading",
    "href": "start-data-def.html#further-reading",
    "title": "3  Defining “Data”",
    "section": "3.4 Further reading",
    "text": "3.4 Further reading\n\n“Basic steps in working with data”, the Data Journalism Handbook, Steve Doig, ASU Professor. He describes in this piece the problem of not knowing exactly how the data was compiled.\n“Counting the Infected” , Rob Gebellof on The Daily, July 8, 2020.\n“Spreadsheet thinking vs. Database thinking”, by Robert Kosara, gets at the idea that looking at individual items is often a “database”, and statistical compilations are often “spreadsheets”.\n“Tidy Data”, in the Journal of Statistical Software (linked here in a pre-print) by Hadley Wickham , is the quintessential article on describing what we think of as “clean” data. For our purposes, much of what he describes as “tidy” comes when we have individual, granular records – not statistical compilations. It’s an academic article, but it has the underlying concepts that we’ll be working with all year.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#exercises",
    "href": "start-data-def.html#exercises",
    "title": "3  Defining “Data”",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nGet a copy of a parking ticket from your local government, and try to imagine what a database of those would look like. What would every row represent? What would every column represent? What’s missing that you would expect to find, and what is included that surprises you?\nThe next time you get a government statistical report, scour all of the footnotes to find some explanation of where the data came from. You’ll be surprised how often they are compilations of administrative records - the government version of trace data.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-def.html#footnotes",
    "href": "start-data-def.html#footnotes",
    "title": "3  Defining “Data”",
    "section": "",
    "text": "I flipped the order of these two definitions!↩︎\nSee “Secrecy in Death Records: A call to action”, by Megain Craig and Madeleine Davison, Journal of Civic Information, December 2020↩︎",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Defining \"Data\"</span>"
    ]
  },
  {
    "objectID": "start-data-document.html",
    "href": "start-data-document.html",
    "title": "4  Documenting your work",
    "section": "",
    "text": "4.1 Scripted journalism\nChristian McDonald at the University of Texas has described “scripted journalism” as data reporting that is “repeatable, transparent and annotated”.\nWe will be accomplishing those goals through the use of Quarto documents, which combine writing with analysis and results. By the end of this book, you will know how to produce a site like McDonald’s Major League Soccer analysis on your own as a way to document your work.\nSome news organizations and editors talk instead about the “data diary”. It’s the same idea, but accomplished by keeping the journalism equivalent of the lab notebooks common in the sciences. Sometimes you’ll do work that doesn’t fit into the scripted journalism idea, such as a quick spreadsheet sort or some hand-cleaning of data. When you do that, be sure to keep a running list of all of the steps you took. Unlike programming languages, there’s no reminder of your mouse clicks.\nI generally work on two levels: For stories, I have a Google or other document that lists all of the datasets I’ve attempted to use to answer my questions, where I tried to get them, and what experts I interview say about them. I also list stories and studies that used the data in these documents. But I almost exclusively work in programming languages to test, clean and analyze data. And to the extent possible, I try to share any results using Quarto documents so that I always know everyone is working from the same version.\nThink of the data work the same way you think about interview notes or transcripts and any other research for a story. You wouldn’t quote a court case without reading it and probably talking to some of the participants. You’d make sure you know where to find the documents and what people say about them. You will consult those documents during your fact-checking. All data work – even the most short-lived – should be documented in at least the same detail. Ideally, someone reading through your notes would be able to repeat your work and understand what it means.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documenting your work</span>"
    ]
  },
  {
    "objectID": "start-data-document.html#elements-of-your-documentation",
    "href": "start-data-document.html#elements-of-your-documentation",
    "title": "4  Documenting your work",
    "section": "4.2 Elements of your documentation",
    "text": "4.2 Elements of your documentation\nYou may not document all of these for smaller stories, but try to keep track of as many of these elements as possible.\n\nData sourcing\nMake sure to write out, in detail, where you got your data and any material you reviewed about it. This includes:\n\nThe source of YOUR data, and how you know it’s authentic. Be specific. Don’t pretend you got it from the original source when you found it elsewhere, such as in this textbook or in a Github repository.\nDescribe the ORIGINAL source of the data and how it is collected and released.1\nTake notes on other stories and studies that use this or similar data. Include interview notes, advice, warnings and findings along with stories that have already been done.\nIdentify alternative sources for this and similar or related datasets or documents.\nSpecifically write down where you have stored all of this and how you have organized your work. You want to make sure you can get back to the latest version easily, and that you have all of the supporting documents you need to check it.\n\n\n\nData documentation and flaws\nI usually put the details of data documentation in a data cleaning Quarto document, which includes the code and the links to my files in one place. But no matter where you do it:\n\nInclude links or copies of any original documentation such as a record layout, data dictionary2 or manual. If there isn’t one, consider making a data dictionary with what you’ve learned.\nDocument the ways you checked the integrity of the data. There are many ways it might be inaccurate. Try to reconcile the number of rows and any totals you can produce to match other reports created by the source, or other reports that have used it. On longer stories, you’ll also check for impossible combinations (10-year-olds with DUIs), missing data and invalid data such as corrupted date columns.\nRecord any questions (and answers as you get them) about the meaning of fields or the scope of the data.\nDocument decisions you’ve made about the scope or method of your analysis. For example, if you want to look at “serious” crimes, describe how and why you categorized each crime as “serious” or “not serious.” Some of these should be vetted by experts or should be verified by documenting industry standards. You might note whether you’ve agreed as a team on these decisions, and when.\nInclude a list of interviews conducted / questions asked of officials and what they said.\n\n\n\nProcessing notes\nSome projects require many steps to get to a dataset that can be analyzed. You may have had to scrape the data, combine it with other sources or fix some entries. Some common elements you should document:\n\nHand-made corrections. Try to list every one, but it’s ok if you describe HOW you did it, such as clustering and hand-entering using OpenRefine. Link to any spreadsheet, document or program you used. Just be sure to always work on a copy of the data.\nGeocoding (affixing geographic coordinates to addresses). Note how many were correct, how many missing, and what you did about it.\nA description of how you got messy data into a tabular form or a form suitable for analysis. For example, you may have had to strip headings or flip a spreadsheet on its head. Make sure to write down how you did that if it’s not in a program that can be easily replicated.\n\n\n\nThe good part: Your analysis\n\nEach question you asked of your data, and the steps you took to answer it. If you use programming notebooks, write it out in plain language before or after the query or statements.\nVetting of your answers: who has looked them over, commented on them\nWhy they might be wrong!",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documenting your work</span>"
    ]
  },
  {
    "objectID": "start-data-document.html#further-reading",
    "href": "start-data-document.html#further-reading",
    "title": "4  Documenting your work",
    "section": "4.3 Further reading",
    "text": "4.3 Further reading\nData cleaning will come up a lot in the future, but it’s closely intertwined with documenting your work. Here’s an email exchange between me and Craig Silverman, now at ProPublica, about the process I used at The New York Times in reporting and fact-checking. This isn’t the same as a process for replication, but it discusses the kinds of things that should be in it.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documenting your work</span>"
    ]
  },
  {
    "objectID": "start-data-document.html#footnotes",
    "href": "start-data-document.html#footnotes",
    "title": "4  Documenting your work",
    "section": "",
    "text": "If you want to see a project with a lot of data sources and how they might be documented in your notes, take a look at the New York City housing data sources we considered for a project in 2016.↩︎\nA data dictionary lists every table and column in the database, along with definitions. It may be very straightfoward but can become quite complex.↩︎",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Documenting your work</span>"
    ]
  },
  {
    "objectID": "start-hunt.html",
    "href": "start-hunt.html",
    "title": "5  Finding the right data for your story",
    "section": "",
    "text": "5.1 An example: News21 “Hate in America”\nIn 2018, News 21 – the multi-university investigative reporting fellowship hosted by ASU’s Cronkite School of Journalism – chose “Hate in America” as its topic for the year. It was a risk because others had been reporting on the subject for more than a year, making it more difficult for News 21 to break new ground. It was also difficult because it became clear quite quickly that no one had documented every case of hate crimes or hate-driven incidents in the U.S.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding the right data for your story</span>"
    ]
  },
  {
    "objectID": "start-hunt.html#an-example-news21-hate-in-america",
    "href": "start-hunt.html#an-example-news21-hate-in-america",
    "title": "5  Finding the right data for your story",
    "section": "",
    "text": "Data News 21 used\nThat meant that the team had to find some creative way to quantify the problem. Some of the sources they used included:\n\nRaw data from the National Crime Victimization Survey, an annual survey of crime victims that asks whether hate was an element of the crime. Reporters Catherine Devine and Allie Bice could have used data from a report produced by the Justice Department, but instead analyzed the raw data in a new way to show that about twice as many incidents may have been motivated by hate than previously acknowledged. That analysis was thoroughly vetted by experts in the survey, in hate crimes, and in criminology. It also created a structure around the entire package and provided a newsy lead to the overview story\nA database created by a team of reporters who monitored two weeks’ of social media activity from users associated with white nationalists, new-Nazis and other far-right groups on sites including Twitter, Facebook, Gab and VK. It enabled Kia Gardener to write:\n\n\nNews 21 monitored the daily social media activity of various far-right users, including white nationalists and neo-Nazis, from June 10 to June 24. Those tracked had more than 3 million followers combined. Reporters recorded and compiled more than 2,500 posts on popular platforms, such as Twitter and Facebook, and emerging social media platforms, including Gab and VK.\n\n\nAbout half the posts were directed at specific demographics or communities, from black Americans and Latinos to Jewish people and LGBTQ members….\n\n\n– Social Media: Where voices of hate find a place to preach, News 21, August 2018\n\n\nFederal prosecutions of hate crimes under the various federal statutes. Reporter Lenny Martinez scraped all of the Justice Department’s hate crime-related press releases to find cases the government bragged about. Those cases were supplemented by a list of cases extracted from Westlaw federal case database. The team logged each case in a Google sheet to show what kinds of incidents were pursued by federal prosecutors, and where.\nProPublica’s “Documenting Hate” project, which, with the Southern Poverty Law Center, tried to compile as many stories as they could about hate incidents. ProPublica’s database was a tip sheet, not a quantification. But it served one key goal of any data source: a source reporters could consult when seeking specific types of examples in specific locations.\nThe FBI Uniform Crime Report’s Hate Crime series. They quickly learned that the data is seriously flawed because of non-response from local police departments and a squishy definition of what should be included. Another flaw was that others, including ProPublica, had thoroughly reported on those flaws and the trends in the data, meaning it failed the test of newsworthiness.\n\n\n\nData the team didn’t use\nThere were also sources that the team considered but didn’t pursue, sometimes because of the difficulty and sometimes because they were less useful to the project than expected:\n\nThe Justice Department’s U.S. attorney case management system, which provided details on cases that the government chose not to pursue along with those they did. (A subsequent analysis showed that the vast majority of these cases were rejected by prosecutors, but vetting the analysis proved too difficult in the time available.)\nDatabases of graffiti maintained by local police departments. This would have required public records requests to each department for records that usually aren’t clearly public. The team also contacted Google and other companies that publish street level images to see if it would be possible to isolate the hate symbols. Companies declined release images that their users had flagged as offensive.\nHistorical questions from the Roper Center for Public Opinion Research and the General Social Survey that might have shed light on attitudes about race and religion over time. These proved to be difficult to match up over the years and didn’t really provide much insight.\n\nThese are just some of the ways the News 21 team looked far and wide for any sources that could be methodically used to document their stories. As with any project of this type, the search often failed but along the way the whole team learned more and more about the topic and got to know experts in a way they wouldn’t have if they were just seeking quotes.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding the right data for your story</span>"
    ]
  },
  {
    "objectID": "start-hunt.html#finding-data-and-documents",
    "href": "start-hunt.html#finding-data-and-documents",
    "title": "5  Finding the right data for your story",
    "section": "5.2 Finding data and documents",
    "text": "5.2 Finding data and documents\nA big part of data reporting is finding, creating or acquiring records you can use electronically.\nSome sources of readily available data could include:\n\nGovernment agencies and open government sites.\nHobbyists and interest groups.\n\nData aggregators and data collectors\nAcademic researchers who might share their data\nMicrodata from surveys and some government programs, such as the Census, Medicare, the General Social Survey and several other standard sites.\nSocial data through API’s from Spotify, Twitter and other services.\nDetails scraped from online data sources that aren’t available in bulk.\n\nThere are also more difficult ways to find data:\n\nPublic records requests\nWhistleblower leaks\nHome made databases created from documents, and free text or image document collections.\nResponses to a survey that you conduct yourself.\nYour own testing on issues such as water quality or soil contamination.\n\nWhen you start on a project, you’ll usually rely on experts and advocates to lead you to a lot of the possible data sources. But you can also use these strategies to troll for interesting datasets that might make for good stories or practice.\nListen to any caveats and warnings. You may decide that they’re not important, but you don’t want to be blindsided by them in the end. And be sure to ask what they would do if they were you – often, people who have expertise in data have story or project ideas that they can’t get funded or approved, and would be happy for someone else to do them.\nWhen you search using Google, try to use the advanced commands to more precisely hit your target. This tipsheet goes through all of the Google advanced search operators. It changes a lot.\n\nGovernment agency sites\nTry to guess what government agencies – state, local and federal – have an interest in your topic. Browse through their websites to find “Publications” or “Data and research”, or any searchable database. You’ll often find downloadable data there. Once you learn more, you can also evaluate how hard it will be to scrape the data you want. Don’t limit yourself to the jurisdications you care about. If one city or state has a good dataset, there is a strong chance that your local government will have the same thing.\nMost agencies and local governments have sites they consider “open government” sites – these are least common denominators of data that they WANT the public to have. But they’re often good starting points. Phoenix, for example, has basic information about police operations in its open site. Simply Googling the agency or jurisdiction name with “open data” will usually find these.\nIn addition, look for map data from agencies. These often send you to a company called ESRI, which produces interactive maps for government. If it’s on the ESRI site, you can request the underlying data.\nLook at federal agency sites to find a least common denominator database – they are usually compiled from more detailed state or local reports.\nEven if you can’t find the database, you might be able to find the name of a datset that is maintained internally in audits, footnotes of reports, or IT initiatives.\nOnce you know a good agency to search, use advanced Google searches for filetype:csv or filetype:xlsx, and limit the site to an agency or city site to bring up datasets that they are letting users download.\n\n\nNews reports\nOne of the most useful sources to find the names of databases and their original sources is news reports that relied on the data, or refers to a data source quoted by experts. It doesn’t matter if you’re looking at your own area or others – most places have the same kinds of information collected and stories are similar across geographic areas.\nYou should get good at using all of the resources as precisely as you can. That means getting very familiar with advanced searching in Google, and using LexisNexis and other news databases provided by the ASU library. These offer much more targeted searching than the usual Google search, and will result in much more on-point stories. When you find a good story, consider logging it in a spreadsheet or in doc, and identify:\n\nWho wrote it and when\nWhat government sources of data are explicitly mentioned.\nWhat analysis of that data was done by the news outlet, or what research it depended on.\nAny terms of art that seem to be used around your topic. For example, hate crimes are more frequently referred to as “bias” crimes in many articles – searching for “hate” might not surface them.\n\n\nIRE.org tipsheets\nAnother source for information on news stories that used data reporting is IRE, which has two ways to search for more details: the ire.org tip sheets and story archive. Log into IRE.org and choose the tipsheets to look for guides from other reporters; choose the story database to look for stories on your general topic and then click into the form that the reporters filled out that go through their sources. You’ll often find a pair of them – a story, and a tip sheet – that were done by the same person the same year.\n(The database library is currently undergoing some review, so a lot of the data listed there could be out of date. But it might also point you to standard sources for data.)\n\n\n\nAcademic articles\nMake sure to do a Google Scholar search for your topic. You will often find one or two researchers who have delved into your subject or a single source. This is often a great shortcut. For example, in the News 21 example, a search of hate crimes in Google Scholar identified an article called “Documenting Hate Crimes in the United States: Some consideration on data sources,” from APA PsycNet. Although this was specifically about sexual orientation and gender diversity, it cataloged the different ways that scholars try to document bias crimes. Once Devine settled on the crime victimization survey, another Google scholar search surfaced an expert on the survey who wrote about how it had changed over the years. He turned out to be the former chief of the Justice Department section that ran the survey, and was one of the project’s best sources. Another source led her to the book, “Statistics for Criminology and Criminal Justice.” One of the authors of that book also provided advice.\nAnother value of this approach is that it will help you find the technical jargon for the topic you’re studying. It’s often very difficult to do literature searches without knowing that term.\n\n\nThink tanks / interest groups\nTry to find some interest groups that care a lot about your topic on all sides. They often have websites with recent research on your topic and might have experts you can consult. Take their advice cautiously because they often have a point to prove and are unabashed about twisting data to make their point. However, you can often use their raw data to draw your own conclusions. Some news organizations frown on this, so be sure to be transparent about who they are and what they’ve done.\nAnother good way to use interest groups and think tanks is to get initial versions of public records from them while you wait for your own requests to be processed. At The Washington Post, we used an old version of a weapons trace database for a year while we fought the government for our own; we also used a copy of Agriculture subsidies acquired by the Environmental Working Group while we were waiting for our own public records requests to be completed.\nSometimes, gathering the Tweets from advocates can provide a rich dataset, and it’s relatively easy to do. For example, I once used the Twitter posts from the Police Misconduct project out of the Cato Institute to get a list of all of the stories they’d compiled on the topic.\n\n\nData collectors and hobbyists.\nSeveral sites are trying to make businesses out of collected and maintaining databases. Others make available data that they have collected in the past. Two common examples of these are &lt;kaggle.com&gt; and the R “tidy Tuesdsay” competition. These are often old data and have not been authenticated or checked. They exist for data scientists to practice their techniques. But they can give you good ideas.\nBe sure to look at the original source for any data you find there. You wouldn’t say a news article came from Google News or Lexis, and you wouldn’t say a dataset came from Google Data Search. If it’s not documented at all, you might have to contact the owner for more detail.\nBe careful of most of these. They’re often old, undocumented and poorly vetted. But they will give you a sense of what you might be able to get from a more reliable source, or give you ideas for your own data collection effort.\n*data.world** wants to be the Facebook or Instagram of data. It has both private and public accounts, and users upload data they want to share. This means it’s as varied as the people who are in it.\nIf your newsroom is an AP member, you might have access to its data.world feed, which contains its curated and documented data that local newsrooms can use for their own stories. Some reporters also use data.world to store their public records. Some government agencies are posting their data directly to data.world. But in other cases, they’re undocumented hobbyists.\nVet these the same way you would Google results.\n*Journalists’ sites** You can often find individual journalists or journalism organizations in various sharing sites, including Github (which doesn’t show up in default Google searches), data.world and other versioning. Look through their sites to see what they have collected – it’s there to share. Fivethirtyeight, ProPublica and the Los Angeles Times have particularly active data archives.\n*Google data search** is, well, the Google of data. In general, data search has limited sources and is more and more frequently logging datasets that are posted by state and local government sources.\nIt makes no attempt to curate the search, though, so be cautious when you find something.\nOne use for the dataset search is to see what other cities and counties have voluntarily released. When you see that, it often means your local or state government might have similar data you can request.\nFor example, searching for police shootings brings up a dataset released by the Orlando Police Department, which contains far more detail than the same dataset released by Phoenix in 2018:\n\n\n\norlando pd\n\n\nBe sure to look for different terms meaning the same thing. For example, searching “use of force” brings you to completely different sets of data than “police shootings”.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding the right data for your story</span>"
    ]
  },
  {
    "objectID": "start-hunt.html#vetting-data-provenance",
    "href": "start-hunt.html#vetting-data-provenance",
    "title": "5  Finding the right data for your story",
    "section": "5.3 Vetting data provenance",
    "text": "5.3 Vetting data provenance\nBefore you even open a dataset, you should know how your dataset was collected, who it originally came from and how current it is. A future chapter will go through many of the ways reporters check data they’ve found for completeness, mistakes or other problems.\nAt first blush, look for anything that precludes using the data because you can’t identify who is responsible for it or how it was collected. This is the same basic vetting you’d do on any source you hope to use.\nLook for:\n\nThe original source. If you are getting it from a secondary source, look to see how hard it will be to get from original. If it’s from a secondary source, how reliable is it? Are you going to be comfortable crediting them for the data? If you can’t identify where or how the data was collected, you probably can’t use it.\nHow others have used it and what criticisms were made of that use.\nThe timeliness of the data. Anything more than two or three years old will be effectively useless for a news article. If it’s old, you should have a plan for how it will be updated.\nData definitions, data dictionaries or record layouts. These are maps to the underlying data, and those definitions can prove difficult to understand.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Finding the right data for your story</span>"
    ]
  },
  {
    "objectID": "start-build-own.html",
    "href": "start-build-own.html",
    "title": "6  Build your own database",
    "section": "",
    "text": "6.1 The evolution of a home-made database\nThe day in December 2015 that a San Bernadino couple killed 14 people, The New York Times published a short story called “How Often Do Mass Shootings Occur? On Average, Every Day, Records Show”.\nThat daily story spurred the Times to embark on a project to document each mass shooting in America for a year. Five months later, it published this story:\nHere is how Sharon LaFraniere, Daniela Porat and Agustin Armendariz described the results of their work about 10 paragraphs into the story. (I suggest you also read the lede on your own - it’s an exquisite example of framing a lede anecdote with detail and context.)\nNotice how the authors weave the details that were chronicled in their database with the data points. Now, try to imagine how their dataset might have been organized to allow for such a rich description of the phenomenon.\nThe database, built by Armendariz, and mostly reported by Porat, was designed to anticipate this writing phase:\nWhat didn’t they do?\nThey didn’t bother to standardize names and addresses into their pieces – they had no interest in counting how many “Smith”s were in the database, and didn’t care how often they occurred a Main Street.\nIn other contexts, these fields might be important, but they were only required for filtering and sorting, not for counting. There was no reason to make it more difficult to fill out the database than necessary.\nThey also didn’t try to publish the full dataset. That’s an important consideration, especially if the data you are collecting contains sensitive or potentially erroneous information. Getting it to that level of accuracy might have added several months to the project, and probably would not have served readers any better. (The bare bones list of cases, with a few exceptions, was already available and updated elsewhere.)",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "start-build-own.html#the-evolution-of-a-home-made-database",
    "href": "start-build-own.html#the-evolution-of-a-home-made-database",
    "title": "6  Build your own database",
    "section": "",
    "text": "Seeking deeper insight into the phenomenon, The New York Times identified and analyzed these 358 shootings with four or more casualties, drawing on two databases assembled from news reports and citizen contributors, and then verifying details with law enforcement agencies.\nOnly a small handful were high-profile mass shootings like those in South Carolina and Oregon. The rest are a pencil sketch of everyday America at its most violent.\nThey chronicle how easily lives are shattered when a firearm is readily available — in a waistband, a glove compartment, a mailbox or garbage can that serves as a gang’s gun locker. They document the mayhem spawned by the most banal of offenses: a push in a bar, a Facebook taunt, the wrong choice of music at a house party. They tally scores of unfortunates in the wrong place at the wrong time: an 11-month-old clinging to his mother’s hip, shot as she prepared to load him into a car; a 77-year-old church deacon, killed by a stray bullet while watching television on his couch.\nThe shootings took place everywhere, but mostly outdoors: at neighborhood barbecues, family reunions, music festivals, basketball tournaments, movie theaters, housing project courtyards, Sweet 16 parties, public parks. Where motives could be gleaned, roughly half involved or suggested crime or gang activity. Arguments that spun out of control accounted for most other shootings, followed by acts of domestic violence.\nThe typical victim was a man between 18 and 30, but more than 1 in 10 were 17 or younger. Less is known about those who pulled the triggers because nearly half of the cases remain unsolved. But of those arrested or identified as suspects, the average age was 27.\nMost of the shootings occurred in economically downtrodden neighborhoods. These shootings, by and large, are not a middle-class phenomenon.\nThe divide is racial as well. Among the cases examined by The Times were 39 domestic violence shootings, and they largely involved white attackers and victims. So did many of the high-profile massacres, including a wild shootout between Texas biker gangs that left nine people dead and 18 wounded.\nOver all, though, nearly three-fourths of victims and suspected assailants whose race could be identified were black. Some experts suggest that helps explain why the drumbeat of dead and wounded does not inspire more outrage.\n\n\n\n\nThe database was split into two separate tables – one that detailed the 358 events and another that detailed the 1,592 victims.\nIt included links to original FOIA requests and documentation they’d need for fact-checking.\nSome columns were categories or items that would be summarized – the ages and ethnicity of the victims, the severity of the injury, and whether it was solved. But much of the data included was detailed descriptions that could be searched using sophisticated filters. Still others were tagged with one-word descriptions that allowed the reporters to pluck just the right examples for just the right part of the story, using words like “suspected gang”, “child”, or “party”. Over time, these tags were reviewed and revised, which is common on small databases like this.\nFact-checking and information for publication was included in the database. For example, the database logged photos, interview and contact notes, and specific entries for name spelling and fact checks. That way the reporters could focus on what was NOT ready for publication, rather than review things they’d already checked.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "start-build-own.html#when-to-build-your-own-database",
    "href": "start-build-own.html#when-to-build-your-own-database",
    "title": "6  Build your own database",
    "section": "6.2 When to build your own database",
    "text": "6.2 When to build your own database\nThere are a few common reasons to design and build your own data for stories:\n\nThere is a long-running story that is periodically updated with new documents or events that you want to track.\nOne reporter created a spreadsheet to log each event related to Jack Kevorkian, a doctor who became famous for helping people commmit suicide. His list made it easy for him to write a story every time another person died, because he had the full list of people and circumstances and knew what he’d already fact-checked.\nMy first data-driven set of stories came from following the actions of George Steinbrenner, the former principle owner of the New York Yankees, whose family owned a failing shipbuilding company in Tampa in the early 1990s. After I pressed the “sort” button, I discovered that each time he helped the company gain new Navy contracts by lending it money, he demanded repayment as soon as the contract was signed, sending it back into a downward spiral.\nYou are getting information from disparate sources and you need an easy way to search them, arrange them chronologically, and keep track of what you need to verify. Examples include reviewing court cases across jurisdictions or compiling death records from many medical examiners’ officers. This would also work for tracking your own FOIA requests. You’ve read about this in Michael Berens’ story of a serial killer in Illinois.\nYou want to fill in details for every item on a list, like the mass shootings story above. This is quite common – you might have a list of opioid overdose deaths from the medical examiner, but you want to fill out the details of this case. At USA Today, Anthony DeBarros did this after 9/11 to tell the story of every person killed in the World Trade Center, including where they were when the planes hit.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "start-build-own.html#tools-for-building-databases",
    "href": "start-build-own.html#tools-for-building-databases",
    "title": "6  Build your own database",
    "section": "6.3 Tools for building databases",
    "text": "6.3 Tools for building databases\nThe simplest tool for a one-table database is just Google Sheets or Excel. In both Google and Microsoft 365, it’s possible to create a data entry form that will feed into a form, so you can make it a little more structured than just a free-form spreadsheet. But when it gets a little more complicated or you want more control over the data types and choices, you might choose to use a different product.\nAirtable is one option (ASU has an enterprise account, which will kick in when you create an account with your school email address). More recently, Microsoft created “Lists” to your 365 account, and Google added “Tables” to your Google account. Airtable and Tables are quite limited in the free edition – so limited that you may find it doesn’t meet your needs. But even if you can’t use it for your full dataset, it might be useful as a sandbox for you to test different ways to set up your dataset.\nThese products let you set up related tables, such as the events events and people tables used in the Times story, and create tags or other structured items for you spreadsheet. They’re also good for working in teams.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "start-build-own.html#how-to-start",
    "href": "start-build-own.html#how-to-start",
    "title": "6  Build your own database",
    "section": "6.4 How to start",
    "text": "6.4 How to start\nWork with everyone who might use the dataset before you start to set out goals. Think about the full range of issues that might come up. Most importantly, how are you going to get the information and how long will it take? Is it just a list of things in chronological order or to provide a quick overview of your reporting? Or are you trying to count specific types of events, such as police shootings by race or gender or the lawyer who has had the most disciplinary actions taken?\nThe difference is whether you are primarily using your dataset for sorting and filtering versus grouping and counting by category.\nYou should also assume that you won’t be able to get all of the information you’d hoped, and that real life doesn’t often fit into neat rectangular boxes. So it’s fine to put in some aspirational columns in case you can get the details, but be sure to stay realistic. If it takes too long to fill out a row in your database, you won’t do it.\nHere are some other considerations:\n\nTry to find an interest group or academic researcher who has already tried to tally the information you’re collecting. They may have good structures that you can adapt to your project. If you can’t, try to find a standard that you want to measure your results against – was a policy followed or not, or was a case solved or not? These are the key statistics that will identify the newsworthiness of your results.\nCarefully define your “universe”. In the case of the Washington Post’s Pulitzer Prize-winning “Fatal Flaws” series on deadly police shootings, the reporters chose to focus only on deaths that were the result of gun discharges in the line of duty. That means they aren’t able to talk about all of the people killed by police, nor all of the people killed in custody, because some happen off hours and others are strangulations or other causes of death. In other cases, you may choose to FOIA the top 50 cities or counties and ignore all the rest. It’s ok to limit your universe. Just be sure that your entire team knows and agrees to the definitions and the limits that places on the results.\nWhat is your unit of analysis, or the noun you use to describe a row in your database? Do you want to count events, people, cases, years, or something else? If so, you should have one and only one row for each of those things, which may mean splitting your work inot more than one data frame or table, the way the Times reporters did for people and events.\nBuild your data dictionary before you start filling out the database, and keep adjusting it as you have to adapt to the real world. Make sure to include a detailed data type (eg, a list separated by semicolons, long text, category, number, date….), and list any standardized words or codes you plan to use (“Y” or “N” for yes and no, for example) .\nReduce the number of columns by smartly combining categories into tags, and considering the way you’ll use a field. For example, sorting by street name isn’t usually very useful (especially when you can filter for it), so there may be no reason to split the street number, name, etc. into different columns. If you enter names in a standard format (eg, Last, First Middle Suffix), then they’re easy to split later on but can be kept in one column and still be sorted. In other words, consider whether you want to be able to sort, filter or count entries. Each of them requires a different level of standardization.\nAnticipate errors. One of the more common problems in creating your own dataset is that the whole purpose of it is to sort or arrange it by date, so you have to enter dates properly. But we rarely do get the exact date for every item in the list. There are several strategies for this, such as entering year, month and day in different columns; or entering an approximate date, and flagging it as “approximate” in a separate field (my preference).\nBuild in fact-checking. If nothing else, be sure to include the source of the information in the row, and provide a way to get back to the original quickly. For example, if you are typing in events from a court case, enter a link to the case folder in one field, and the page number of the item you’re entering in another. If I’m publishing anything from the dataset, I include columns for name checks, fact checks and even whether the narrative has been copy edited.\nHow many columns do you really want? On a spreadsheet, things can bet pretty unwieldy pretty fast. Try to avoid having more than about 15 columns, and try to define them so that most are filled out.\n\nThis is an example of a spreadsheet created to log the first 100 days of the Obama administration. The “subject_tags” column let the reporters enter a variety of categories, which were then normalized when it came time to use them.\n\n\n\n100 days\n\n\nThose tags resulted in it being relatively easy to create graphics like this by filtering for related tags and ordering it by date:\n\n\n\nreversals",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "start-build-own.html#an-example",
    "href": "start-build-own.html#an-example",
    "title": "6  Build your own database",
    "section": "6.5 An example",
    "text": "6.5 An example\nIn 2013, the New York Times and Frontline collaborated on a story on police-involved domestic violence. Most of the story was a narrative of a single case. But it was important to show that this was not the first time the sheriff in the Florida county was slow to investigate his deputies. Another case several years earlier had the same telltale problems.\nThe Times obtained several key documents in the other case.* You can read a copy of the internal affairs report yourself, but you quickly come away realizing that it’s hard to follow, repetitive and you’re never quite sure what’s happened. Here is the record layout of the table that we built, and a small snippet of what it looks like.\n\n\n\ndata dict\n\n\nAnd here is what a few rows look like. Notice that they are not organized chronologically, but are organized in the order that they were listed in the underlying document. The “sort” button turns it into a chronology.\n\n\n\nsjso-example\n\n\nAnd finally, here is what was written about the case in the final story:\n\nA year before that, Sheriff Shoar’s disciplinary posture had been called into question in a domestic violence case involving a deputy named Halford (Bubba) Harris II.\nTwo supervisors learned of accusations that Mr. Harris had abused his wife. But no investigation was immediately opened, records show.\nOne sergeant did prepare an affidavit documenting the accusations. But he was told by his supervisor to hold it back, so he stuck it under the visor in his squad car, where it remained, even after another officer became aware of further incidents, according to Mr. Harris’s internal affairs file.\nThe case came to a head on Christmas Eve, when his wife fled their house and called the police. Internal affairs officers uncovered other possible acts of domestic violence before his hiring, records show. His wife said that before they married, he had held a knife to her throat and hit her. His ex-wife said he had threatened her with a gun. No charges were filed.\nCol. Todd R. Thompson, the sheriff’s director of law enforcement, recommended that Mr. Harris be fired, saying his actions were “particularly egregious and trouble me deeply.”\n\nWas it necessary to create a spreadsheet logging almost 100 events in this case? No. But we had to go through every document in detail anyway, and this meant we didn’t have to do it over and over again.",
    "crumbs": [
      "Reporting with data",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Build your own database</span>"
    ]
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Programming quick start",
    "section": "",
    "text": "This is probably your first introduction to coding. Don’t be worried. With effort, much of what reporters do in coding can be learned in a few weeks. In “Learning how to Learn”, Jesse Lesy says:\nWhen you get very frustrated – and you WILL get very frustrated at times – walk away from the computer for a little while. You may see a solution staring right back at you. But if you still can’t imagine what to do next, get help. As Lecy says, your morale is a valuable commodity and it shouldn’t be squandered.\nThe good news for journalists is that you can accomplish most of what we need with the vocabulary of a three-year-old. Much of what we do in R is accomplished using six verbs: filter, mutate, arrange, group by, summarize and join.\nThis section goes in depth in a couple of areas that flummox some people: Installing and getting used to the screens in R and RStudio; and getting data into the program by importing. Then it shows off some of the things you’ll be able to do in just a couple of weeks.\nDon’t worry if you’re just copying and pasting at this point. It’s important to get used to the screens and the process without worrying about substance at first. That’s why programming language tutorials often start with printing, “Hello, world!” – it’s something that every program does, but a does it a little differently.",
    "crumbs": [
      "Programming quick start"
    ]
  },
  {
    "objectID": "quickstart.html#section",
    "href": "quickstart.html#section",
    "title": "Programming quick start",
    "section": "",
    "text": "If this is your first programming language, you will get frustrated at times. Take a step back and remember that after a semester of Spanish you can only operate at the level of a three-year old. You only know one verb tense, a few dozen verbs, and several hundred words. You have so many emotions that you can’t express in your new language!",
    "crumbs": [
      "Programming quick start"
    ]
  },
  {
    "objectID": "quickstart.html#credits",
    "href": "quickstart.html#credits",
    "title": "Programming quick start",
    "section": "Credits",
    "text": "Credits\nSome of the material in these chapters relies on work done by Andrew Heiss and Christian McDonald.",
    "crumbs": [
      "Programming quick start"
    ]
  },
  {
    "objectID": "quickstart-install.html",
    "href": "quickstart-install.html",
    "title": "7  Installing R and RStudio",
    "section": "",
    "text": "7.1 Install R and RStudio\nR is the programming language itself, and has to be installed first. Andrew Heiss has called it the “engine” behind your work. RStudio, made by the company posit.co, is the way we’ll interact with the programming language. Install it second. Heiss calls it the “fancy car” that you drive.\nMost students prefer to install the software on their own laptops, but there are other options in the backup plan section.\nThe video is a little old. Here are the differences today:",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "quickstart-install.html#install-r-and-rstudio",
    "href": "quickstart-install.html#install-r-and-rstudio",
    "title": "7  Installing R and RStudio",
    "section": "",
    "text": "Follow along with the tutorial in this link to install R, RStudio and the package called the tidyverse on your computer.\nhttps://learnr-examples.shinyapps.io/ex-setup-r/#section-welcome\n\n\n\nThere are now two versions of R for Mac: One for older (pre-2022) computers with an Intel chip, and another called ARM64 for Macs with M1 or M2 chips. Be sure to choose the one for your computer. You can tell which one you have by clicking on the apple symbol, and choosing “About this Mac”. If it says “Intel”, then you have the older version.\nThe download link for RStudio is now https://posit.co/download/rstudio-desktop/\nIf you go all the way through the tutorial, installing the tidyverse will take a while and look like it’s stuck. It’s not. Just get some coffee and come back in 10 minutes. This only has to be done once.\n\n\n\n\n\n\n\nStop if it isn’t working\n\n\n\nThis should be easy, but don’t spend more than about 30 minutes trying to get this to work, and don’t try to Google your way out of any problems – the answers could be old, wrong, or make it harder. This isn’t worth your time or frustration. Contact me if there’s any problem and we’ll work it out.\n\n\n\nDetails on installing R\nLink to find your version: https://cloud.r-project.org/. As of December 2023, the version is 4.3.\nMac OS:\n\n\n\nThe version you see will be higher than “4.1”. As of December 2023, it’s 4.3\n\n\nParts of RStudio depend on “command line tools” in a Mac. If you’re asked to install it, say “Yes”. If you get a warning or error about it, ask me to help you. It’s a simple fix. RStudio will actually work without it, but you’ll get a lot of annoying warnings.\nWindows 10/11:\nYou only need the “base” version for our class. You do not need to install RTools. Try to install it as an administrator – we’ve had a little trouble finding the right paths when it’s installed at the user level.\n\n\nBackup plan\n\nLabs in school\nYou do not need to own a computer to take this class. R and RStudio should be set up properly n our classroom and the lab in Cronkite 320. ASU’s computing sites such as the lab in UCENT L-102 across from Cronkite also have the basic software installed but you may need to tweak it each time you sign on. Let me know if you are using university labs and I’ll help you make it a little more efficient.\n\n\nPosit cloud\nPosit Cloud allows you to use R and RStudio without installing it on your computer. Use it as a last resort. Believe me, this will be a pain, but it can work to get you started. We’ll work together to find a solution longer term.\nSome limitations:\n\nYou’ll have to upload everything you want to use to the cloud, then download the final (saved) versions when you want to turn them in.\nHistorically, it hasn’t worked very well on the Quarto documents that we’ll be using. You might not be able to see your finished document, which is a big deal for us.\nYou can only use 1GB RAM - this won’t be enough for some projects.\nYou only get 25 hours per month to use it. During the key part of our course, you should run out of time and won’t be able to access your work until you pay. (The $5/month plan would work.)\nYou have to remember to save your work and periodically restart R. It doesn’t remind you and you might lose everything by closing out a tab. If you don’t save and process your documents right before turning in your work, there’s a good chance I’ll see an earlier draft.\n\nYou will have to upgrade to a paid account (minimum $5/month) if you try to use it for the whole the semester. I’ve never understood the plan costs and limits – they’re quite strict, and you won’t be able to log into your account if you try to go beyond them.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "quickstart-install.html#set-up-rstudio-for-data-reporting",
    "href": "quickstart-install.html#set-up-rstudio-for-data-reporting",
    "title": "7  Installing R and RStudio",
    "section": "7.2 Set up RStudio for data reporting",
    "text": "7.2 Set up RStudio for data reporting\nStaying organized is one of the challenges of data reporting – you’re constantly re-downloading and re-jiggering your analysis and it’s easy to get your material separated. This setup helps ensure that you always know where to find your work and can move it to another comptuer seamlessly.\nBefore you start, decide on a folder you’ll use to store all of your R work.\n\n\nCreate a folder called maij-working within your Documents folder, or another folder that you will use for your coursework. Make sure you know how to find it.\nStart up RStudio (NOT R) , and choose Tools -&gt; Global options (in a Mac)\n\nUnder Tools &gt; Global options, UN-check all of the options to save and load .Rdata and other files on startup, and choose “Never” for saving the workspace.1\n\n\n\nHere’s a video of what you should do. It also gives you a little reminder of how your computer is organized in folders.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "quickstart-install.html#footnotes",
    "href": "quickstart-install.html#footnotes",
    "title": "7  Installing R and RStudio",
    "section": "",
    "text": "This makes sure that you don’t depend on a previous session; your programs are self-sustaining. Posit Cloud users don’t have this, and they often forget to save.↩︎",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "quickstart-program.html",
    "href": "quickstart-program.html",
    "title": "8  A gentle intro to programming",
    "section": "",
    "text": "8.1 Building blocks",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A gentle intro to programming</span>"
    ]
  },
  {
    "objectID": "quickstart-program.html#building-blocks",
    "href": "quickstart-program.html#building-blocks",
    "title": "8  A gentle intro to programming",
    "section": "",
    "text": "Variables\nA variable is a container with a name. You pour things into the container Sometimes, those things are simple like your name. Sometimes they’re quite complex, such as the contents of a Spotify playlist. In R, many people use the terms variable and object interchangeably.1\nThe three most common structures of variables are:\n\nLiterals\nThe most basic kind of object is a single variable that contains a single value of a specific type:\n\n\"Sarah\" (text)\n1.0 (number)\ntrue (logical)\n2017-01-24. (date)\n\nThese are called “literals”. When you want to use the actual letters of some text, enclose them in quotes. When you want to use the named variable, don’t enclose it in quotes: my_name &lt;- \"Sarah\"\n\n\nLists or vectors\nMost languages have some concept of a list of items called an array, vector or dictionary. In R, you create a vector using the “c” operator, short for “combine”. Once you have your items in a list or a vector, you can apply the same function across all of them or work on them in order. :\n\nc(1, 17, 2, 120, 4)\nc(\"Sarah\", \"Richard\", \"Rachel\")\n\n\n\nData frames\nA data frame is the R equivalent of a spreadsheet page. It’s tabular, rectangular data with columns and rows. You don’t need to know much about this yet, but just understand that it will become an important part of your vocabulary.\n\n\n\nOperators & assignment\nOperators are simple arithmetic or similar operations, like adding, subtracting, dividing and multiplying. Some common operators are:\n\n\n\n\n\n\n\nOperator\nWhat it does\n\n\n\n\n \nArithmetic\n\n\n+\nAdd\n\n\n-\nSubtract\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n \nComparison (in R)\n\n\n==\nEqual - Notice the TWO equal signs\n\n\n&gt;\nGreater than (or &gt;= greater than or equal to)\n\n\n&lt;\nLess than (or &lt;= less than or equal to)\n\n\n!=\nNot equal to\n\n\n \nAssignment\n\n\n&lt;- or =\nPush a value in to a variable name. In R, most people assign a value to a variable using the &lt;- operator, and save = to provide information to functions. There are a lot of ways to think of this – Some might call it “naming a value”, and others might say it’s “setting a variable”. Visualizing the &lt;- arrow pushing something into a variable name can sometimes help.\n\n\n\n\n\nFunctions\nA function is a verb or a set of instructions wrapped up into a single word. Use its name, followed by some parentheses. If it requires information from you, those arguments go inside the parentheses.\nIf you think of it as a sentence, the function is the verb, and the arguments are the nouns and adjectives. It genearally takes the form:\nfunction ( data, option_name=\"value\")\nMany of the functions we use are already built into R, or are in libraries that we borrow – someone else has already written, tested and packaged them up for you.\nExamples include:\n\nimporting text or Excel data\ncalculating the average or sum\ncounting\nfinding phrases within text\n\nSome functions work only with numbers; others work only with text or dates or some other data type.\n\n\nLoops\nA loop is a way to repeat your instructions over and over without having to re-write them every time. They usually work by stepping through some kind of a list, like a directory of files, or by using a counter, such as every year between 2005 and 2018.\nExcel and Google Sheets don’t really have loops, unless you learn the more complicated programming language behind them. This is the motivation behind learning a programming language for a lot of people.\nOne type of loop commonly used in programs is a for loop, which says, “for every one of something, follow these instructions”.\nSee if you can follow the pseudocode of putting sunglasses or hats on monsters depending on their shape:\n\n\n\nAlison Horst\n\n\nCommon tasks that reporters do with loops include combining annual spreadsheets from a government website or downloading and importing many files at once; and scraping web pages that require clicking into each link one by one. We’ll get to this at the end of the semester.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A gentle intro to programming</span>"
    ]
  },
  {
    "objectID": "quickstart-program.html#an-algorithm-to-make-an-omelet",
    "href": "quickstart-program.html#an-algorithm-to-make-an-omelet",
    "title": "8  A gentle intro to programming",
    "section": "8.2 An algorithm to make an omelet",
    "text": "8.2 An algorithm to make an omelet\n\n\n\nKatie Smith via Unsplash\n\n\nAn algorithm is a set of instructions that are carried out step by step. Brushing your teeth, making coffee or starting your car can all be expressed on algorithms if you do them the same way day after day.\nSuppose you want to make an omelet. Before you even start, you need to know at least two things: which ingredients you have on hand, and what kind of omelet you want to make.\nAn algorithm that creates an omelet might look something like this:\n\nDecide what kind of omelet you want\nCheck to make sure you have ingredients (and stop if you don’t)\nPrepare the fillings\nWhisk the eggs\nMelt some butter\nPour in the eggs\nAdd ingredients\nFlip the omelet\nPut it on a plate\n\n\nOpen a restaurant with loops\nSay you saved your steps into a function called make_omelet().\nYou’d have to repeat this over and over if you had a restaurant. It might look like this:\n\nmake_omelet (ingredients,kind)\nEdit the list of ingredients left and the kind for the next diner\nmake_omelet (ingredients2, kind2)\nEdit your lists again\nmake_omelet (ingredients3, kind3)\n. and so on.\n\nYou’d have a program hundreds of lines long – one for each customer. Instead, you could loop through the customers and do the same thing:\n\ncustomers &lt;- c(\"Bob\", \"Jamal\", \"Christine\", \"Lauren\")\n\nfor each customer in the list of customers {\n  request what kind they want\n  make_omelet (ingredients_on_hand, kind_this_customer_wants)\n  give omelet to customer\n  update your ingredients list \n  }",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A gentle intro to programming</span>"
    ]
  },
  {
    "objectID": "quickstart-program.html#more-resources",
    "href": "quickstart-program.html#more-resources",
    "title": "8  A gentle intro to programming",
    "section": "8.3 More resources",
    "text": "8.3 More resources\n\n“Beyond Binary, Lesson 1 from a Google engineer’s drag queen persona named Anna Lytical. This video gives you a good handle on what a computer program does.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A gentle intro to programming</span>"
    ]
  },
  {
    "objectID": "quickstart-program.html#footnotes",
    "href": "quickstart-program.html#footnotes",
    "title": "8  A gentle intro to programming",
    "section": "",
    "text": "Statisticians usually refer to columns in a data set as “variables” – literally, something that varies. Other things in their programs are called “objects”. ↩︎",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A gentle intro to programming</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html",
    "href": "quickstart-startup.html",
    "title": "9  Getting started with R",
    "section": "",
    "text": "9.1 Getting to know the RStudio screen\nThis is what your screen probably looks like:",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html#getting-to-know-the-rstudio-screen",
    "href": "quickstart-startup.html#getting-to-know-the-rstudio-screen",
    "title": "9  Getting started with R",
    "section": "",
    "text": "Open the application called RStudio ,\nNOT R .\n\n\n\n\n\n\nconsole\n\n\n\n\nThe Console\nThe Console is where you can type commands and interact directly with the programming language. Think of it as a very powerful calculator at first. One reason to use it is to install packages.\nIf you followed the installation demo, you’ve already used the console to install one package using the command install.packages(\"tidyverse\"). Go back and do that part now if you skipped it. It might take several minutes to finish.\n\n\nFiles tab\nWe won’t be using many of the tabs in the lower right, but the Files tab can help you if you’re having trouble navigating your work. Under the More button, you can choose “Go to working directory”, since that’s where R thinks you’ve parked all of your work. This can be confusing in R, which is why we’ll be working in “projects” that bundle up all of your work in one place. (I don’t find the Help sections in this area very helpful. Instead, I usually rely on written manuals elsehwere, such as tidyverse official site. )\n\n\nEnvironment\nThe upper right screen is the Environment, which is where your active objects live. An object, sometimes called a “variable” in R, is a named thing. It might be a word, a list of words or numbers, or a data frame (spreadsheet). It can even be a little program called a function that you write yourself.\nAnything that you want to use has to be loaded or created into that environment first. That might involve importing data from Excel or a text file, or creating data yourself in a program.\n\n\nTyping into the console\nWhen you type this: 5+5 after the &gt; prompt and press return, you’ll see: [1] 10\nWhen you type this: \"Sarah\" , you’ll get this back: [1] \"Sarah\"\nTo create a new variable, you’ll use the assignment operator &lt;- (two characters : A less than sign and a hyphen). Here is how I would create the variable called my_name (lower case, no spaces). Notice how it appears in the Environment after being created. Then I can print it by typing the name of the variable instead of the letters of my name in quotes:\n\n\n\n\n\n\n\nPro tip\n\n\n\nUse the keyboard shortcut  to create the &lt;- assignment operator .\n\n\nThe console remembers your commands, but you have to type them one at a time and you can’t save them for later, or start over without entering them all again. That’s why we’re going to work in programs called Quarto documents most of the time.1",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html#take-a-tour",
    "href": "quickstart-startup.html#take-a-tour",
    "title": "9  Getting started with R",
    "section": "9.2 Take a tour",
    "text": "9.2 Take a tour\n\nWatch Andrew Heiss’s tour of RStudio.\n\n\nWe will not be using the R scripts he shows in the end of the video, but many other examples and resources do. Also, as we get into working with Quarto, the “code chunks” work the same way as a script.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html#unlocking-packages-and-the-tidyverse",
    "href": "quickstart-startup.html#unlocking-packages-and-the-tidyverse",
    "title": "9  Getting started with R",
    "section": "9.3 Unlocking packages and the tidyverse",
    "text": "9.3 Unlocking packages and the tidyverse\nThe real power of R comes with packages. Packages are bundles of programs that others have found useful to extend the base R language. R is almost useless to a normal person without them. There are more than 10,000 packages available for R, each doing a special job.\nIf you followed along with the tutorial in the last chapter, the final step was to install a package called the tidyverse. Almost everything we do from now on depends on that.\nThe tidyverse is a system of packages, or libraries, that work together with similar grammar and syntax. It’s particularly useful for the kind of work reporters do – importing, cleaning and analyzing data that we get from others. For many reporters (myself included), R was too difficult to learn before the tidyverse came around. We’ll be working almost exclusively within the tidyverse in this course.\n\n\n\n\n\n\nSearching for help\n\n\n\nWhen you search for help, be sure to put “tidyverse” somewhere in your query. If you don’t , you’re likely to get inscrutible answers.\nperplexity.ai and ChatGPT are two artificial intelligence sites that can help you with your code. These AI sites are particularly good at troubleshooting code – describe your data, copy and paste your code, and ask it what’s wrong! I tend to use the Perplexity site. Some of the methods shown in this book are newer than the ChatGPT cutoff date.2\nEither one is usually better than just Googling. If you do use Google, be sure to set a date cutoff – R has changed a LOT in the last five years.\n\n\n\nInstalling packages for this course\nWe need to update your packages and install a few more for our work.\nIf this is the first time you’ve installed it, the tidyverse will take a while to install, and it will look like it’s just stopped. Be patient. It can take 5 minutes or more.\nYou only have to install packages once on each machine you use. Later on, you’ll learn how to invoke them one by one.\n\nOption 1: Use the console\n\nCopy these commands into your Console. This list of packages contains all of the packages used in this book – I think! Don’t worry about what they all mean – we’ll use them later on.\n\nIf it asks you if you want to restart, say “Yes”.\nIf it asks you if you want to build from source, say “No”.\n\n\nupdate.packages(ask=FALSE)\ninstall.packages(c(\"tidyverse\", \"janitor\" , \n                \"lubridate\", \"quarto\", \"swirl\", \"pacman\", \n                \"DT\", \"reactable\", \"gt\", \n                \"leaflet\", \"sf\", \"tmap\", \n                \"gapminder\", \"plotly\", \n                \"gtsummary\", \"tigris\"\n                ))\n\n\n\n\nOption 2: Use RStudio “Packages” tab\n\nUnder the Packages tab in the lower right panel:\n\nStart by updating all of the packages you might already have installed by pressing the “Update” button.\nUnder the Install tab, search for the packages listed in the install.packages() code above.\n\n\n\n\npackages\n\n\n\n\n\nOptional hands-on tutorial:\nOne of the packages you just installed is called swirl, which has some interactive tutorials. The first one might be useful.\n\nMake sure your cursor is in the Console and type swirl::swirl().\nSelect the first chapter called R Programming: The basics of programming in R, and just the first section, called Basic Building Blocks.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html#relax",
    "href": "quickstart-startup.html#relax",
    "title": "9  Getting started with R",
    "section": "9.4 Relax!",
    "text": "9.4 Relax!\n\n\n\n\n\n\nFigure 9.1: Relax by Silwia Bartyzel via Unsplash\n\n\n\nYou’re all set up and we’re ready to start programming. Congratulate yourself - everything is new, nothing is intuitive and the screen is intimidating. You’ve come a long way.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-startup.html#footnotes",
    "href": "quickstart-startup.html#footnotes",
    "title": "9  Getting started with R",
    "section": "",
    "text": "These used to be “R Markdown”. Quarto is the newer implementaiton of it, but you’ll often see R Markdown referenced in other places.↩︎\nIf you have a Github account, you might be able to use Github Copilot in RStudio. I’ve found it to be more annoying than helpful.↩︎",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Getting started with R</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html",
    "href": "quickstart-quarto.html",
    "title": "10  Using Quarto",
    "section": "",
    "text": "10.1 Quarto documents\nQuarto is a document format that lets you combine your writing, images, computer code and its results into complete documents. We’ll only be using Quarto for R using RStudio, but it also works in other languages such as Python and Javascript. Some news organizations do much of their internal work using Quarto and its predecessor, RMarkdown.\nThis entire book is written as a series of Quarto documents!",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html#quarto-documents",
    "href": "quickstart-quarto.html#quarto-documents",
    "title": "10  Using Quarto",
    "section": "",
    "text": "Quarto is modern RMarkdown\n\n\n\nMany help pages and tutorials in R use the format called RMarkdown (.Rmd) instead of Quarto (.qmd), which was introduced in mid-2022. There are very few substantive differences.\n\n\n\nThe structure of a Quarto document\nThere are four parts to a Quarto document:\n\nNarrative and document features, including headlines, subheads, images and other elements that you’d normally put in any document.\nThe “front matter”, or YAML1, instructions for the entire document, such as a title or the output format. You will often copy and paste this part rather than trying to get it right freehand.\nLittle portions of your R programs in code chunks, similar to the lines you entered in the Console in the last chapter.\nThe results of code chunks - what you get back when you execute the code, including charts, graphs, tables and maps.\n\n\n\nMarkdown, an editing language\nIn Quarto, your narration is written using Markdown, which was invented as an easy way for early Wikepedia editors to write simple documents that would automatically be rendered into the more complicated HTML of the web. Once you get used to Markdown, you may never go back to Google Docs or Word.\n\nReview the first five sections of the markdown basics instructions from Quarto, paying attention to:\n\nText formatting, like bold and italic\nLinks\nHeadings\nLists (ordered and unordered)\nOptional: Tables\n\nWe probably won’t use anything below the Tables section in this class.\n\n\n\nFront matter / YAML at the top\nThe way the R knows how to process the page is by reading the very top of the file and looking at the section between three dashes. This is called “front matter”, and it’s extremely picky about indentations and the exact words you use.\nThe default setup is very basic. For this class, I’ll ask you to use the following options, and edit them to correct the title and author each day. This will already be filled out for you in some of the early assignments.2\n---\ntitle: \"A title for your page\"\nauthor: \"Your name\"\nformat: \n  html: \n    theme: cosmos\n    embed-resources: true\n    toc: true\n    code-tools: true\n    page-layout: full\n    df-print: paged\n---\n\n\nIncluding R code and its results\nSo far, nothing is different from a simple Google doc or Word document – there is no computer code or data involved. But the value of Quarto is that it allows you to mix your analysis with your writing in one place.\nYou’ll do that through code chunks – little snippets of working code that you insert within your narrative which, when processed, will also include the results. Each code chunk has a Play button in its upper-right corner. Once you’re done typing, you’ll press that button to run just that code chunk.\nTo insert a code chunk:\n\nUse the +C button on the top right of your screen. ( )\nUse the keyboard shortcut  (Cmd-Opt-i)\n\n\nThe following code chunk creates a new variable called my_variable, another called my_name.\n\n```{r}\n# Anything between ```{r} at the beginning and ``` at the end is the code chunk.   \n# A hashtag within an R program or code chunk means the line is a \"comment\". \n# R will ignore it.\n\n\nmy_variable &lt;- 13\nmy_name &lt;- \"Sarah\"\n```\n\nWhen you press the Play button, nothing comes out. That’s because you saved your variables. Once they’re saved, you can print them by just typing their names:\n\n1my_name\n\nmy_variable\n\n\n1\n\nPrint the value that’s stored in the variable called my_name, then print the value that’s stored in my_variable\n\n\n\n\n[1] \"Sarah\"\n[1] 13\n\n\nThis might not seem like much, but think about how it helps you do your analysis. You can write all of your notes right where you do the work. You don’t have to copy and paste information from one place to another, or share out-of-date spreadsheets with your teammates.\n\n\nRendering a document\n The document remains in markdown format until you process it by pressing the “Render” button at the top of the page.Quarto converts your document to a web page and saves it into your project.\n\n\nSwitch to Source mode\nRStudio usually creates a Quarto document in “Visual” mode, which hides the underlying coding from you and allows you to use point-and-click menus to structure your document.\nThis tutorial will have you switch to “Source” mode so that you can see the underlying structure.\n\nIn practice, visual mode makes data tables look all squished and difficult to navigate until you’ve rendered them. You can set up RStudio to default to source mode by changing the global options under Tools -&gt; Global Options , then look for R Markdown, and the Visual tab.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html#your-first-project",
    "href": "quickstart-quarto.html#your-first-project",
    "title": "10  Using Quarto",
    "section": "10.2 Your first project",
    "text": "10.2 Your first project\nOne of the things that trips up new users of R is that it’s hard to tell the program where to find things on your computer. In RStudio, projects are used to isolate all of your work to one folder in your computer.\nWe’ll be working entirely in projects in this course. This means you will never double-click on your Quarto document to open RStudio. Instead, you’ll open RStudio and then choose or create a project.\n\n\nCreate a new project by hitting the second green + sign at the top left of your screen, or choose File &gt; New Project.\nWhen prompted, create a project in a New Directory. Make sure that you save it WITHIN your maij-working folder. Name it rlessons-01. This creates a file with the extension .RProj in a folder with the name of your project.\nOnce you’ve created it, shut down RStudio.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html#your-first-document",
    "href": "quickstart-quarto.html#your-first-document",
    "title": "10  Using Quarto",
    "section": "10.3 Your first document",
    "text": "10.3 Your first document\n\n\nOpen the RStudio project you just created by first opening the app, then choosing the project from the drop-down menu at the top right. Get in the habit of opening your projects this way.\nCreate a new document (File -&gt; New -&gt; Quarto document) with three things about yourself in a list, a picture of an animal you love or some other image you like, and a code chunk.\n\n\n\nStep by step\n\nFind an image of your favorite animal or another image you like, and save it in the project folder that you created above.\nOpen RStudio first, then open the project using the drop-down at the top right of your screen (or through the File -&gt; Open project menu item).\nCreate a new Quarto document.\nSwitch to Source mode from Visual mode .\nDelete the entire default entry, including the stuff at the very top.\nCopy the code above to the very top of the page. Don’t leave any blank lines above it.\nChange your name and the title in that top section.\nSave the new Quarto document using the name yourlastname-firstquarto\nThe area below the three dashes is your document. In it, create:\n\nA subhed introducing a subtopic, called “About me”\nWithin that heading, a list of three things about yourself or about a topic you love, in an unordered (bulleted) list.\nAnother subhed , called “An image I love”, with some text below it describing why you love it\nThe image that you saved into your project folder\nAnother subhed called “Some R code”\nA code chunk, in which you create a variable , assign it a value, and then print it out.\n\nSave your work\nRender the document to a final html file. (Rendering automatically saves, but it’s good practice to do it yourself.)\n\n\n\nCommon problems\n\nYou haven’t saved the document yet.\nThere is an error in that top YAML section. Copy the code above and try again. It’s really picky.\nThere is an error in your R code. Look for a visual hint that shows you what’s wrong.\nYou deleted the ending back-ticks at the end of the code chunk. Your whole screen turned gray and there’s no Play button. Add three back-ticks after your code to get back on track.\nEverything’s too smushed together. The subheds, for example, have to have a space after the hashtag, and the lists have to have a blank line above them. Give your document room to breathe by inserting blank lines between elements. If you’re having trouble, switch back to visual mode and fix it there, then look at it again in Source mode.\n\n\n\nExample document\nYou can look at the original Quarto code and the rendered document, but remember that if you were doing this yourself, you’d have to have the image saved in the same folder as your project. I changed the theme so you could see some of the variations.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html#quarto-resources",
    "href": "quickstart-quarto.html#quarto-resources",
    "title": "10  Using Quarto",
    "section": "10.4 Other resources",
    "text": "10.4 Other resources\n\nQuarto’s Getting Started guide\nHeiss’s video on R Markdown, which is very similar to Quarto. (13 minutes). It’s great about showing you the glitches you might run into. The differences between Quarto and RMarkdown are :\n\nThe output: html_document line should be format: html\nThe name of the file ends with .qmd, not .Rmd\nThe “knit” button is really a “render” button.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-quarto.html#footnotes",
    "href": "quickstart-quarto.html#footnotes",
    "title": "10  Using Quarto",
    "section": "",
    "text": "Literally, “Yet Another Markup Language”. Sigh↩︎\nThree key things it does: Make better-looking printouts; allow me to download all of your code; and keep any data or images embedded into the document so you don’t have to send me all of the supporting files.↩︎",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Using Quarto</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html",
    "href": "quickstart-data-import.html",
    "title": "11  Getting and saving data",
    "section": "",
    "text": "11.1 Billboard Hot 100\nChristian McDonald, a data journalism professor at the University of Texas, has compiled a list of Billboard’s Hot 100 hits back to the 1950s, and made it available in his Github account. We’ll be using the data on McDonald’s Github repository at https://raw.githubusercontent.com/utdata/rwd-billboard-data/main/data-out/hot100_assignment.csv\nHere is how he has described it:",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#billboard-hot-100",
    "href": "quickstart-data-import.html#billboard-hot-100",
    "title": "11  Getting and saving data",
    "section": "",
    "text": "The Billboard Hot 100 singles chart has been the music industry’s standard record chart since its inception on 8/2/1958. The rankings, published by Billboard Media, are currently based on sales (physical and digital), radio play, and online streaming. The methods and policies of the chart have changed over time.\n\n\nThe data we will use here is a combination of data collected by Kaggle user Dhruvil Dave, along with some scraping and merging by Prof. McDonald. It is stored on the code sharing website Github for safe keeping.\n\n\nThe data dictionary\nAs of December 2023, this version of runs from its inception in August 1958 through mid-June 2023, before most of Taylor Swift’s Eras tour sent her Hot 100 numbers even higher.\nHere is a data dictionary , sometimes called a record layout to go along with the data.\n\n\n\n\n\n\n\n\nvariable name\ntype\ndescription\n\n\n\n\nCHART WEEK\ndate\nThe release date of the chart in the form “m/d/yyyy”\n\n\nTHIS WEEK\nnumber\nThe rank (1 through 100) of the song that week\n\n\nTITLE\ncharacter\nSong title\n\n\nPERFORMER\ncharacter\nPerformer, as published in the chart. There could be different spellings over time.\n\n\nLAST WEEK\nnumber\nLast week’s ranking\n\n\nPEAK POS.\nnumber\nHighest ranking ever on the Top 100 chart\n\n\nWKS ON CHART\nnumber\nNumber of weeks it has appeared on the chart, not necessarily consecutive.\n\n\n\nHere are some things to note about it:\n\nThe names of columns are in upper case and contain spaces and punctuation. That makes our life a little more complicated, and we’ll want to fix it.\nIt provides the type of data – number, date or text – that should be contained in each column.\nDates are shown in a way we normally read them in the U.S., such as “12/3/2023”\nEach row represents a song in the designated week. This means that there will be lots of rows for each week, and a row for each song every time it was on the hot 100.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#set-up-your-quarto-program",
    "href": "quickstart-data-import.html#set-up-your-quarto-program",
    "title": "11  Getting and saving data",
    "section": "11.2 Set up your Quarto program",
    "text": "11.2 Set up your Quarto program\nYou’ll do this every time you start a new document or project in R.\n\n\nCreate an R project in your maij-working folder called hot-100 .\nCreate a new Quarto document and delete everything.\nIf you’re not already there, switch to the Source view.\nCopy and paste the top section from the Quarto lesson delete everything else. Try rendering it to check for errors before you go any further – this part is really picky.\nCopy and paste the code chunk below into your document and press the “Play” button to run the code.\n\n\nThis assumes you have installed the packages from the Starting R chapter. Go back and do that now if you skipped it. That step has to be done only once on each computer you use.\n\n```{r}\n#| label: setup\n#| message: false\n\nlibrary(tidyverse)              \nlibrary(janitor)                \nlibrary(lubridate)   \n```\n\nThis is a common first code chunk. The first option at the top names it “setup”, which tells it to run this code chunk before it does anything else. The second option suppresses some very annoying messages.\nThis code chunk activates three packages using the library() function. You have to invoke packages you want to use in each document, even though you only have to install them on your computer once.\nHere’s what your page might look like after playing the first code chunk (visual mode):\n\n\n\n\nafter running first chunk\n\n\n\nDon’t worry about any messages that came out after you ran the first chunk – they simply show you some information about what is in the libraries you just loaded. They might even be red!",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#import-the-hot-100-data-file",
    "href": "quickstart-data-import.html#import-the-hot-100-data-file",
    "title": "11  Getting and saving data",
    "section": "11.3 Import the Hot 100 data file",
    "text": "11.3 Import the Hot 100 data file\nTo “import” data means to pour outside data into an R data frame, which is an R object that contains columns and rows, just like a spreadsheet.\nSome people call this “reading” or “loading” data. This file is a CSV file, which stands for “comma-separated values”. Some other forms are shown in Appendix C: File Types in the Wild\nWhen you import the file, R will guess what each column is – text (“chr”), numbers (“num” “), and date or date/time (”date” or “POSIX” in R).\n(From now on, you’ll just see the R code that goes inside a code chunk you create, not the part with the backticks and brackets.)\n\n\nCreate a new code chunk using the Insert Chunk button  at the top right of your document, or using the keyboard shortcut .\nUse the clipboard icon below to copy the code and paste into your code chunk. You might not be able to see the full name of the web address, but it’s there.\nPress the Play button or use the keyboard shortcut control-shift-Returncontrol-shift-Return to run the code chunk.\n\n\nClick on the numbers underneath the code chunk to highlight each row with an explanation of what it does:\n\n1top100 &lt;-\n2  read_csv(\n3    \"https://raw.githubusercontent.com/utdata/rwd-billboard-data/main/data-out/hot100_assignment.csv\"\n4    )\n\n\n1\n\nCreate a new object in R called top100 using the assignment operator &lt;-\n\n2\n\nStart the function read_csv()…\n\n3\n\nto access a comma-separated data file stored in Prof.McDonald’s Github site. Use just the name of the file, in quotes, if it’s already saved in your projects folder.\n\n4\n\nDon’t forget to close your parentheses.\n\n\n\n\nAfter running the code chunk, you should see something like this:\n\n\nRows: 341300 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): CHART WEEK, TITLE, PERFORMER\ndbl (4): THIS WEEK, LAST WEEK, PEAK POS., WKS ON CHART\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo R found:\n\n341,100 rows (or records, or observations).\n7 columns ,\n3 of which it thinks are character types, and 4 that it thinks should be numbers.\nIt named the columns using the first row of the file.\n\nThis method of reading a file accepts the default options for importing. You may need to override some of them at times, especially when R guesses your data types incorrectly.\n\nStop a minute and think about what you just did in one second – you imported a dataset with MORE THAN a quarter of a MILLION rows without any complaint!\n\nThere is now a new object in your Enviornment tab, under “Data”, called top100, with “341,100 obs. of 7 variables”.\n\nImport tips\n\n\n\n\n\n\nPosit cheat sheets\n\n\n\nThe “Data import with the Tidyverse” cheat sheet goes through the details of importing data. You may need some help deciphering it until you get used reading reference instructions.\n\n\n\n\n\n\n\n\nSpreadsheets\n\n\n\nYou have to use different functions to import spreadsheet files.\nFor Excel, you would use the readxl library, which you have to activate at the top of the program.\nFor Google Sheets, you’d use the googlesheets4 library, which is a little harder to use because of Google Drive permissions. For the time being, consider downloading your Google Sheet to a csv in your project folder.\n\n\n\n\n\n\n\n\nDefault to character columns\n\n\n\nIf you’re not sure about the data types of each column, use an option to import them all as text, or character, columns. Then you can fix them one by one. Text will (almost) never be a problem. The option looks like this:\nread_csv( \"your file name or URL\", col_types=c(.default=\"c\") )",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#look-at-the-data-you-imported",
    "href": "quickstart-data-import.html#look-at-the-data-you-imported",
    "title": "11  Getting and saving data",
    "section": "11.4 Look at the data you imported",
    "text": "11.4 Look at the data you imported\nThere are several ways to examine the data you just imported.\n\nCheck it in the environment tab\nThere’s now an object listed in your Environment tab with a blue arrow. Expand it, and you’ll see the column names and what types it found.\n\n\n\nClick on it to scroll, filter and sort\nTo browse the data, click on its name in the environment panel.\nAny filters or sorts that you do here won’t last – they’re just on while you’re glancing at the data. But this is useful just to get a little preview of what columns look like and to search for things you know should be there. This search is NOT case-sensitive, which is helpful when you want to get a handle on what’s in your data.\n\n\n\nUse functions to examine the rows and columns\nBut these methods are no better (and really a little worse) than just viewing your data and clicking around in Excel or Google sheets. To share your view of the data and describe it to others, you must use R code.\nThere are a few standard ways to get a quick view of what’s in your data using functions. First, you can look at the top and bottom of it using the head() and tail() functions. Try to guess what these mean:\n\nhead(top100)\n\ntail(top100, n=10)\n\n\n  \n\n  \n\n\n\nUse the little arrow at the top right of the listing to look at columns that don’t fit on the page.\nThis list isn’t in chronological order– it’s just showing happened to be on the top and bottom of the original file.\nOr, you can look at a list of columns along with their types and a few examples from the top of the file using the glimpse() function.\n\nglimpse(top100)\n\nRows: 341,300\nColumns: 7\n$ `CHART WEEK`   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/20…\n$ `THIS WEEK`    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, …\n$ TITLE          &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The …\n$ PERFORMER      &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives…\n$ `LAST WEEK`    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 1…\n$ `PEAK POS.`    &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12…\n$ `WKS ON CHART` &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, …\n\n\nHere, you might notice that the names of some of the columns have back-ticks around them. That’s because they don’t follow our rules for column names – they should be lower case, with no spaces or special characters.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#introducing-the-pipe",
    "href": "quickstart-data-import.html#introducing-the-pipe",
    "title": "11  Getting and saving data",
    "section": "11.5 Introducing the pipe",
    "text": "11.5 Introducing the pipe\nYou’ll often use code in this format:\n  data_set_name |&gt;\n     verb (  ) |&gt;\n     verb (  ) |&gt;\n     etc...\nThat little symbol, |&gt; is called the “pipe”. It means “Take what is on this line, and use it to do the next thing” You can use the shortcut keys Command-Shift-MCommand-Shift-M instead of typing it out.\n\n \n\n(The pipe character changed a little in recent years; %&gt;% is largely the same as |&gt; )\nUsing a pipe can help make your code easier to read and write by separating each step into its own command.\nFor example, glimpse(top100) does the same thing as top100 |&gt; glimpse(). The second version tells R, “Start with the top100 data frame and then glimpse it. I try to separate as much as is reasonable into steps because I can then troubleshoot problems one line at a time.\n\n\nAdd a code chunk that you’ll edit to clean the column names and fix the dates the data.\nCopy and paste the code below\n\n\n\n1top100 |&gt;\n2  clean_names()  |&gt;\n3  glimpse()\n\n\n1\n\nStart with the top100 data frame that we saved earlier, and then\n\n2\n\nUse a function called clean_names() to convert them to computer-friendly names that won’t require special handling, and then\n\n3\n\nTake a look at it!\n\n\n\n\nRows: 341,300\nColumns: 7\n$ chart_week   &lt;chr&gt; \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch…\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",…\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, …\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49…\n\n\n\n\n\n\n\n\n|&gt; not &lt;-\n\n\n\nDon’t confuse the pipe (|&gt;) with the assignment operator (&lt;-). The pipe says, “keep going”. The assignment operator says, “save this for later” .",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#cleaning-up-and-save-for-later",
    "href": "quickstart-data-import.html#cleaning-up-and-save-for-later",
    "title": "11  Getting and saving data",
    "section": "11.6 Cleaning up and save for later",
    "text": "11.6 Cleaning up and save for later\nThis section gives you a little taste of the key verbs you’ll be using throughout this class:\n\nselect, which picks and rearranges columns\n\nmutate, which lets you create new columns out of old ones.\n\nThe code chunk also uses function called mdy(), which stands for “month-day-year”, which in turn tells R that the character chart_week column starts out in that format, as opposed to year-month-day or something else.\n\nTry typing this code chunk into your document instead of copying and pasting. You won’t see anything come out of this when you run it. Instead, it’s saved as new object in your Environment tab.\n\n\n1top100_clean &lt;-\n  top100 |&gt;\n  clean_names () |&gt;  \n2  mutate ( chart_date = mdy(chart_week)) |&gt;\n3  select ( chart_date, title, performer, this_week, last_week,\n           wks_on_chart, peak_pos)\n\n\n1\n\nCreate a new data frame using the assignment operator out of what follows. Leave off this row until you know the rest works.\n\n2\n\nUse the function mdy, which converts text in the form of month-day-year into true dates, to transform the chart week into a date variable called chart_date.\n\n3\n\nPick out the columns to save in the order you want to see them.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#check-your-work-and-save",
    "href": "quickstart-data-import.html#check-your-work-and-save",
    "title": "11  Getting and saving data",
    "section": "11.7 Check your work and save",
    "text": "11.7 Check your work and save\n\n\nCheck your data by “glimpsing” it:\n\n\ntop100_clean |&gt; \n  glimpse()\n\nRows: 341,300\nColumns: 7\n$ chart_date   &lt;date&gt; 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-…\n$ title        &lt;chr&gt; \"All I Want For Christmas Is You\", \"Rockin' Around The Ch…\n$ performer    &lt;chr&gt; \"Mariah Carey\", \"Brenda Lee\", \"Bobby Helms\", \"Burl Ives\",…\n$ this_week    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17…\n$ last_week    &lt;dbl&gt; 1, 2, 4, 5, 3, 7, 9, 11, 6, 13, 15, 17, 18, 0, 8, 25, 19,…\n$ wks_on_chart &lt;dbl&gt; 50, 44, 41, 25, 11, 26, 24, 19, 24, 15, 31, 18, 14, 1, 49…\n$ peak_pos     &lt;dbl&gt; 1, 2, 3, 4, 1, 5, 7, 6, 1, 10, 11, 8, 12, 14, 7, 16, 12, …\n\n\nNote the new data type for the chart_date, and the order of the columns along with the clean names.\n\nNow save your data as an R data file (called an “RDS” file) like this:\n\n\nsaveRDS(top100_clean, file=\"hit100.RDS\")\n\n\nTry to process your Quarto document into a self-contained HTML page by pressing the Render button at the top of the document. You might get some errors – that’s ok for now. You’ve already done a lot.\n\n\nYou’ll see how to load data that starts as an R data file in the next chapter.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#what-we-did",
    "href": "quickstart-data-import.html#what-we-did",
    "title": "11  Getting and saving data",
    "section": "11.8 What we did",
    "text": "11.8 What we did\n\nCreated a new Quarto document and added the packages (libraries) we plan to use.\nImported a comma-separated text file from the web into a data frame object called top100.\nTook a look at it in several different ways.\nCreated a second data frame from the first, with names and dates fixed, with only some columns picked out and displayed in a new order.\nSaved it into the project for use in another program.\n\n\nWhat you should do next\nBefore you put away your Quarto file, remember the principles of scripted data and documentation. Take a few minutes to clean up and document your work by narrating each step, including:\n\nSub-headings for each of the steps\nIntroductory text describing what you are about to do\nMaybe: Some text after the chunks that describe meaningful output or things you notice about the result.\n\nIf you’re stuck, you can use the template I created as a starting point.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-data-import.html#and-breathe",
    "href": "quickstart-data-import.html#and-breathe",
    "title": "11  Getting and saving data",
    "section": "And breathe",
    "text": "And breathe\nYou’ve now created a full, working R program in Quarto format that can serve as a model for everything you do in the future. Congratulate yourself and take a break!\n\n\n\nMax van den Oetelaar via Unsplash",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Getting and saving data</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html",
    "href": "quickstart-verbs.html",
    "title": "12  A quick tour of verbs",
    "section": "",
    "text": "12.1 Starting a new Quarto document",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#starting-a-new-quarto-document",
    "href": "quickstart-verbs.html#starting-a-new-quarto-document",
    "title": "12  A quick tour of verbs",
    "section": "",
    "text": "Open or re-open the project you created in previous chapters. This starts you with a clean slate in the correct location.\nCreate a new Quarto document. Using the program you wrote in the last chapter, copy everything in through the setup chunk and delete anything else. Edit the YAML section to create a new title , and save it as top100-02-analysis.qmd.\nInstead of importing from a text file, this time you’ll read in the R file you saved in the last lesson using the readRDS() function.\n\n\n      top100 &lt;- readRDS(\"hit100.RDS\")",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#filter-and-arrange",
    "href": "quickstart-verbs.html#filter-and-arrange",
    "title": "12  A quick tour of verbs",
    "section": "12.2 Filter and arrange",
    "text": "12.2 Filter and arrange\nfilter is the same idea as filtering in Excel, but it’s much more picky. In R, you have to match words exactly, including the upper- or lower-case.\narrange is R’s version of Excel’s sort.\nHere’s how you would pick out all of Taylor Swift’s appearances on the Billboard top 100 list, in chronological order.\n\n\n\n\n\n\nCaution\n\n\n\nIn R, a condition is tested in a filter by using two equal signs, not one.\n\n\n\ntop100 |&gt;\n  filter ( performer == \"Taylor Swift\") |&gt; \n  arrange (  chart_date )   \n\n\n  \n\n\n\nSo Taylor Swift has songs on the Billboard Hot 100 list more than 1,300 times since 2008.\n\n\n\n\n\n\nDon’t count on filtering to count items!\n\n\n\nDepending on your output formats, there are hard-coded limits to the number of rows that R will show you. With “paged” output like this, it will only show you the first 10,000 and won’t tell you how many more there are. We’ll come back to this.\n\n\nHere’s how you’d list only her appearances at the top of the list – No. 1 is the lowest possible value for this_week, indicating the rank , then pick out just a few columns to list in order:\n\n\ntop100 |&gt;\n  filter (performer==\"Taylor Swift\" & this_week == 1) |&gt;\n  arrange ( chart_date)  |&gt;\n  select ( this_week, title, chart_date)\n\n\n  \n\n\n\n\nHer first No. 1 hit was in 2012, and her most recent was in November 2023",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#summary-statistics-in-r",
    "href": "quickstart-verbs.html#summary-statistics-in-r",
    "title": "12  A quick tour of verbs",
    "section": "12.3 Summary statistics in R",
    "text": "12.3 Summary statistics in R\nListing each item that meets a condition might show you how many match your criteria, but it doesn’t help you aggregate your data. For that, you need the summarize() verb, calling specific summary functions:\n\nn() – the number of rows.\nsum()\nmean()\nmedian()\nmin() and max()\n\nsummarize() answers the questions, “How many?” and “How much?”, or “Smallest” and “Largest”.\nThis code chunk will show the summary for all 341,000 rows:\n\ntop100 |&gt;\n1  summarize ( number_of_entries = n() ,\n              first_entry = min(chart_date),\n              last_entry= max(chart_date)\n              )\n\n\n1\n\nEach row defines a made-up descriptive name for the new columns created from summary functions.\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nSingle vs. double “=”\n\n\n\n\nUse a single equals sign when you are naming a new column\nUse a double equals sign to see if one thing is the same as another\nDon’t ever name the new column the same thing as an existing column.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#grouping-with-summary-statistics-aggregating",
    "href": "quickstart-verbs.html#grouping-with-summary-statistics-aggregating",
    "title": "12  A quick tour of verbs",
    "section": "12.4 Grouping with summary statistics (aggregating)",
    "text": "12.4 Grouping with summary statistics (aggregating)\nYour questions will often be around the idea of “the most” or “the biggest” something. The group_by() verb creates separate analyses, or piles, for each unique item in the column. You then create a summary for those groups. We’ll go into this in a lot more detail in future chapters.\n\ntop100 |&gt;                                         \n1  group_by ( performer) |&gt;\n2  summarize ( times_on_list = n() ) |&gt;\n3  arrange (desc ( times_on_list ))  |&gt;\n  head (10)\n\n\n1\n\nCollapse the data into one row for each performer.\n\n2\n\nAnd fill it with the number of times that performer was on the list\n\n3\n\nSort it with the most popular performer first.\n\n\n\n\n\n  \n\n\n\nAdding the song title to the group_by() creates a more detailed list of tracks, but still collapses the weeks:\n\ntop100 |&gt;\n  group_by ( title, performer)  |&gt;\n  summarize ( times_on_list = n() , \n              last_time_on_list = max(chart_date),\n              highest_position = min(this_week)\n            ) |&gt;\n  arrange ( desc ( times_on_list) ) |&gt;\n  head(25) \n\n\nSo some of the titles that were on the list the longest never made it to #1. (You’ll learn later on how to make tables that are more easily navigated.)\n\n\n\n  \n\n\n\nWe’ll go over how to make better-looking, more readable results in a few weeks. It’s more complicated than it should be.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#make-a-chart",
    "href": "quickstart-verbs.html#make-a-chart",
    "title": "12  A quick tour of verbs",
    "section": "12.5 Make a chart!",
    "text": "12.5 Make a chart!\nOne motivator in learning R is its very sophisticated graphics. We’ll come back to this later in the semester, but you can just copy and paste this code to see how it might work.\nI’ve made a dataset for you that contains the 26 No.1 songs that stayed on the Hot 100 for at least 52 weeks in this century. (Songs released for the first time 2023 aren’t eligible for the list)\n\n# read the data\nreadRDS( \n  url (\"https://cronkitedata.s3.amazonaws.com/rdata/top_songs.RDS\")\n  ) |&gt; \n# start the plot\n  ggplot ( \n        aes ( x=chart_date, y=hit, color=this_week ) \n        ) +\n       geom_point(  size= .25) +\n# make it look a little better\n  labs( color = \"Hot 100 #\") +\n  theme_minimal( ) +\n  theme(axis.title.x = element_blank(),\n          axis.title.y = element_blank(), \n        legend.position= \"bottom\")",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#thoughts-on-the-verbs",
    "href": "quickstart-verbs.html#thoughts-on-the-verbs",
    "title": "12  A quick tour of verbs",
    "section": "12.6 Thoughts on the verbs",
    "text": "12.6 Thoughts on the verbs\nYou’ve now seen most of the key verbs of the tidyverse, and how they can be put together. They are:\n\nmutate , which you saw in the last chapter, to create new columns.\nselect, to pick out columns in the order you want to see them\narrange, to sort a data frame\nfilter, to pick out rows based on a condition\nsummarize to compute summary statistics like “how many?” and “how much? or”smallest” and “largest”\ngroup_by to create a single row for each unique item in a list.\n\nDon’t worry if you don’t understand how this works or how to do it yourself. This walkthrough is just intended to show you how much you can do with just a few lines of code.",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "quickstart-verbs.html#footnotes",
    "href": "quickstart-verbs.html#footnotes",
    "title": "12  A quick tour of verbs",
    "section": "",
    "text": "Technically, these are in a library called dplyr, but it’s always included with the tidyverse↩︎",
    "crumbs": [
      "Programming quick start",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>A quick tour of verbs</span>"
    ]
  },
  {
    "objectID": "rdepth.html",
    "href": "rdepth.html",
    "title": "Verbs of the tidyverse",
    "section": "",
    "text": "Practice data\nThis section uses Paycheck Protection Program loans from 2020-2021 as its practice data. It has some characteristics that make it a good candidate, despite its age:\nBy using the same data all the way through these exercises, you won’t face the cognitive load of learning the details of a new dataset each time. Previous students have said that it doesn’t matter to them what the data is, just that it is useful for their learning and that it has in the past been reasonably newsworthy. I took them at their word.",
    "crumbs": [
      "Verbs of the tidyverse"
    ]
  },
  {
    "objectID": "rdepth.html#practice-data",
    "href": "rdepth.html#practice-data",
    "title": "Verbs of the tidyverse",
    "section": "",
    "text": "It was well documented – the columns and rows aren’t mysteries.\nIt contains names, addresses, dates and numbers – all elements that are important in data journalism. At the same time, it doesn’t unnecessarily publicize details about individuals.\nIt’s old! That means we don’t have to worry too much about it changing midstream.\nThere were a lot of good stories done about the program, AND a lot of stories that were never done but could have been.\nSome of the columns are reasonably standardized and clean, and others are just a mess.\nThe agency published some summary data about the program that we can try to replicate, which is one of the ways reporters check their data work.",
    "crumbs": [
      "Verbs of the tidyverse"
    ]
  },
  {
    "objectID": "rdepth.html#asking-good-questions",
    "href": "rdepth.html#asking-good-questions",
    "title": "Verbs of the tidyverse",
    "section": "Asking good questions",
    "text": "Asking good questions\nYou can and should seek help in the #dj-sos channel on Slack, but it takes some work to ask a good question. Here are the three parts of a good question:\n\nDescribe what you are attempting to accomplish. Example: “I want to pick out loans in Tempe”.\nCopy and paste or take a screen shot of the code you’re using to try to do that. It’s fine if it doesn’t work, or if it’s incomplete. But do try it yourself for a little while before reaching out.\nTake a screen shot of the unexpected result or of the error message. Part of your learning is getting familiar with scary-sounding error messages, so try to read it before you freak out and post it. It might actually make sense.\n\nOften, the process of crafting a good question leads you to the answer yourself. But don’t hesitate to ask. If we’ve already gone over it, I’ll refer you to material that you might have forgotton. You’re never expected to memorize code.\nMost of your homework and projects can be accomplished using the material we will have already covered in class and in this book. But I don’t expect you to memorize everything we do, so I will sometimes refer you to where in the book or in the homework you’ve already seen your problem or something very similar. Take it in the spirit it’s offered – it’s help, not criticism or rebuke.\n\n\n\n\n\n\nUsing ChatGPT or other AI for help\n\n\n\nYou can and often should use all of the avenues available to you for help. But the R language is vast and we’re only looking at a tiny piece of it in this course. Googling and using AI can be frustrating because they will likely take a different approach than the simple one we’re using. Whenever you look for help, be sure to:\n\nInclude the word “tidyverse” in your query. If the answers don’t look familiar, try again using a specific verb or function name.\nIn ChatGPT, consider copying and pasting your code and asking what’s wrong. It’s better at that than starting from scratch.1\nIn Google, be sure to look only at answer from the last few years. I usually set the date to the most recent year under “Tools”.\n\n\n\nYou’re responsible for both getting the “right” answers and accomplishing it using the methods taught in class. This stuff is hard enough to understand without trying to understand it done six different ways.\nThe bots can also sends you down a rabbit hole without helping, so just move on if there is a problem. For example, I tried to get Bing (which uses ChatGPT4) and &lt;perplexity.ai&gt; to troubleshoot a code chunk in which I forgot to use the pipe, resulting in a common error. It failed even after fiddling with the prompt – they both eventually provided the right answer, but never diagnosed the problem correctly.",
    "crumbs": [
      "Verbs of the tidyverse"
    ]
  },
  {
    "objectID": "rdepth.html#footnotes",
    "href": "rdepth.html#footnotes",
    "title": "Verbs of the tidyverse",
    "section": "",
    "text": "ChatGPT is a little out of date in R – a lot changed just after its last cutoff. Consider using perplixity.ia or Github Copilot instead.↩︎",
    "crumbs": [
      "Verbs of the tidyverse"
    ]
  },
  {
    "objectID": "r-tidy.html",
    "href": "r-tidy.html",
    "title": "13  Tidy data principles",
    "section": "",
    "text": "13.1 What is tidy data?\nThe key characteristics of tidy data are:\nHere is how Wickham visualizes these rules:\nTidy data tends to have a lot of repeated information in the rows, but has few columns, just like the “database” thinking in Kosara’s piece. Practice looking at any data you find with an eye toward converting it into a tidy structure.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tidy data principles</span>"
    ]
  },
  {
    "objectID": "r-tidy.html#what-is-tidy-data",
    "href": "r-tidy.html#what-is-tidy-data",
    "title": "13  Tidy data principles",
    "section": "",
    "text": "Each column contains one kind of information – race, sex, address, etc. Data scientists call these “fields” or “variables” or “attributes”.\nEach row contains one example of each of those columns, all at the same unit of analysis. In other words, they each refer to the same noun. Data scientists call these “observations” or “cases” or “records”.\nEach value must have its own cell.\n\n\n\n\n\nSource: R for Data Science",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tidy data principles</span>"
    ]
  },
  {
    "objectID": "r-tidy.html#understanding-data-types",
    "href": "r-tidy.html#understanding-data-types",
    "title": "13  Tidy data principles",
    "section": "13.2 Understanding data types",
    "text": "13.2 Understanding data types\nWhen each column holds the same piece of information, it makes sense that each value in that column should be of the same type. Many of the errors or problems in your programs will come from trying to use text when something else is required, or trying to use text when a number is required.\n\n\n\n\n\n\nGet the data dictionary!\n\n\n\nWhenever you get a dataset, you should look for a “data dictionary”, “record layout” or similar document. They tell you what each column is called and what type of data is held in it.\n\n\nHere are the four most common data types you’ll see in reporting:\n\nText. Text or “character” columns can come in long or short form. When they are standardized (the values can contain only one of a small list of values), they’re called “categorical”.In reporting, we frequently treat any value that you don’t want to use in a mathematical formula as text. For instance, there is no concept of the sum of Zip Codes. We would treat them as text values to keep them separate from values. Text is the least common denominator of data – all other types can be represented as text, but the reverse isn’t true.\nDon’t confuse a text value with the name of a variable or object. For example, the column zipcode might contain the value “01243”. The name of the column is unquoted, and the value within it is.\nNumbers. These are pure numbers with no commas, dollar signs or other embellishments. You can present them as formatted values that are easier to look at, but underneath they’re just numbers.In R, these could be integers (whole numbers), or double-precision values (those that have a lot of numbers after the decimal point, such as geographic coordinates or the results of computations).\nThere actually aren’t that many pure numbers in the granular datasets that we obtain via FOIA – dollar values and test measurements, for example. In R, as in most programming languages, trying to put text into a column defined as numbers will yield an error or a missing value.\nLogical: This is a subset of text. It can take one of only two values – yes or no, true or false. There is no “maybe”.\nDate and time: These are actual dates on the calendar, which have magical properties. You see a date or time that makes sense to you, but the computer sees a pure number. A date value in R is the number of days since Jan. 1, 1970; in Excel, it’s the number of days since Jan 1 1900. There are different date and time systems, but the lubridate package in R will handle most of them. The key to date values is that if they are stored as a date, they will sort correctly even though “February” comes before “January” in the alphabet. They also have attributes that you can extract, like the day of the week.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tidy data principles</span>"
    ]
  },
  {
    "objectID": "r-tidy.html#real-world-non-tidy-data",
    "href": "r-tidy.html#real-world-non-tidy-data",
    "title": "13  Tidy data principles",
    "section": "13.3 Real world non-tidy data",
    "text": "13.3 Real world non-tidy data\nWickham laid out several ways that we see data when it’s not tidy. The three most common are:\n\nColumn headings are values, not variables. When a column heading shows each year, it is showing the value of the year variable, not something separate. This happens most of the time when you obtain data that is intended for publication, not analysis.\nThis is the most common type of untidy data seen in newsrooms, because so many of our datasets come in the form of spreadsheets from sources or the government, or from printed tables in PDF documents that are converted into spreadsheets.\nMultiple variables are stored in one column. You have to decide what the important level of detail is: Full name, or first name and last name? More frequently, this is a problem in data that comes from the Census Bureau, which has columns that combine, for example, sex and age. (Male, 25-54 years old)3\nRows might contain different levels of detail. This also comes from getting printouts rather than original datasets, where subtotals and totals are shown in rows, not in a separate table. Again, this is most common in statistical reports intended for printing rather than for analysis.\nSeparate sheets for the same data. It’s common for government agencies to provide data with one page for each year, or one page for each county. These usually have to be stacked on top of one another.\n\nYour first instinct in creating your own database will likely have one or more of these problems.\nNow that you have some of the basic concepts, it’s time to put them into practice!",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tidy data principles</span>"
    ]
  },
  {
    "objectID": "r-tidy.html#footnotes",
    "href": "r-tidy.html#footnotes",
    "title": "13  Tidy data principles",
    "section": "",
    "text": "Hadley is still the person who decides whether a package deserves to be included in the tidyverse.↩︎\nObservable is a platform for Javascript developers that can also be integrated with Quarto.↩︎\nThe tidycensus package takes care of this for us!↩︎",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Tidy data principles</span>"
    ]
  },
  {
    "objectID": "r-filter-sort-story.html",
    "href": "r-filter-sort-story.html",
    "title": "14  Sorting and filtering to find stories",
    "section": "",
    "text": "14.1 A sorting miracle\nAfter Ferguson, Mo., police killed Michael Brown in 2014, advocates and journalists began examining the racial and ethnic gap between police departments and the communities they served. The New York Times found a 7-year-old survey conducted by the Justice Department that allowed it to compare the data for major cities in a standalone graphic that it published later that year.\nWhen newer data reflecting departments’ makeup in 2012 was released a year later, Matt Apuzzo and I hoped it would show some differences. It didn’t. So we were left trying to find news in the data that was clearly of public interest.\nAfter matching up the demographics of police departments with their cities, I started sorting, filtering and Googling. Could there be news in the outliers on the list? Which departments most closely represented their communities? Which ones had unusually large gaps?\nI quickly stumbled on telling anecdote to frame the story: Inkster, Mich. had one of the least representative departments in the country, and had recently hired a new police chief to help mend the department’s fraught relationship with its largely African-American community. Where had he come from? Selma, Ala., one of the most representative police departments in the nation. Interviews with the chief, William T. Riley III, suggested one reason for some cities’ disparities: there was no state or federal money to pay for training new police officers.\nThe story, “Police Chiefs, Looking to Diversity Forces, Face Structural Hurdles” helped explain the persistent gap between the makeup of police in some areas and the communities they served.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "r-filter-sort-story.html#a-sorting-miracle",
    "href": "r-filter-sort-story.html#a-sorting-miracle",
    "title": "14  Sorting and filtering to find stories",
    "section": "",
    "text": "Chief William T. Riley III. Credit: Laura McDermott for The New York Times",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "r-filter-sort-story.html#sorting-and-filtering-as-a-reporting-tool",
    "href": "r-filter-sort-story.html#sorting-and-filtering-as-a-reporting-tool",
    "title": "14  Sorting and filtering to find stories",
    "section": "14.2 Sorting and filtering as a reporting tool",
    "text": "14.2 Sorting and filtering as a reporting tool\n“Sorting” is the process of re-arranging the rows of your data from low to high, or high to low. “Filtering” is the process of picking out items from your data using specific criteria.\nSorting and filtering can:\n\nNarrow your focus to specific items that you want to examine in your story.\nShow you rows containing the highest and lowest values of any column. That can be news or it can be errors or other problems with the data.\nLet you answer quick “how many?” questions, with a count of the rows that match your criteria. (In the next lesson, you’ll see that counting this way isn’t always the most efficient way to answer your “how many?” questions.)\n\nDon’t underestimate the power of a simple filter and sort to help you hone in on anecdotes that will make your story come alive, by finding the oldest or youngest, the nearest or farthest, or the smallest or largest examples from your data.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Sorting and filtering to find stories</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html",
    "href": "r-verb-filter.html",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "",
    "text": "15.1 select columns\nThe select verb allows you to pick out columns by name or by their order in the data frame. Selecting columns is case-sensitive: a column called amount is completely different than a column named Amount. That’s one reason to use our style of always converting column names to lower case.\nselect is the easiest verb to understand it so it’s shown here first. But in practice, I usually include it as my last verb in a code chunk because some columns that I don’t care about in the end are needed for filtering or creating new columns.\nThere a lot of ways to pick out column names that are based on their name, their position, their type or other characteristics. Here are a few:\nYou can rename columns at the same time you select them by typing the new name, one equals sign and the old name, for example:\nThis example selects just a handful of columns to look at in depth. The original data hasn’t changed – we’ve just picked out the columns we want to use, and poured them into a new data frame called ppp_small.\n1ppp_small &lt;-\n   ppp_orig |&gt; \n  select (borrower_name, borrower_address, borrower_city,\n          project_county, date_approved, amount, \n          forgiveness_amount)\n\n\n2glimpse(ppp_small)\n\n\n1\n\nCreates a NEW data frame instead of printing the result onto the screen. After it is run, a separate command…\n\n2\n\nglimpse() lets you look at the saved data frame.\n\n\n\n\nRows: 169,259\nColumns: 7\n$ borrower_name      &lt;chr&gt; \"SFE HOLDINGS LLC\", \"NAVAJO TRIBAL UTILITY AUTHORIT…\n$ borrower_address   &lt;chr&gt; \"9366 East Raintree Drive\", \"Po Box 170\", \"2999 N44…\n$ borrower_city      &lt;chr&gt; \"Scottsdale\", \"Fort Defiance\", \"Phoenix\", \"Tucson\",…\n$ project_county     &lt;chr&gt; \"MARICOPA\", \"APACHE\", \"MARICOPA\", \"PIMA\", \"MARICOPA…\n$ date_approved      &lt;date&gt; 2020-04-10, 2020-04-11, 2020-04-11, 2020-04-29, 20…\n$ amount             &lt;dbl&gt; 10000000, 10000000, 10000000, 10000000, 10000000, 1…\n$ forgiveness_amount &lt;dbl&gt; 10108219, 9882528, 10133389, 10131667, 10126027, 10…",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#select-columns",
    "href": "r-verb-filter.html#select-columns",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "",
    "text": "select (date_approved, borrower_name) picks out those columns and rearranges them from left to right in the order you mention them.\nselect (1:10) picks out the first 10 columns. Think of the colon as the word “through”\nselect ( 2, 4:6, business_type) combines position and name so you can use whichever is easier.\n\n\nselect ( given_date = date_approved, \n        borrower = borrower_name)",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#arrange-rows",
    "href": "r-verb-filter.html#arrange-rows",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "15.2 arrange rows",
    "text": "15.2 arrange rows\nThe arrange verb sorts your data in different ways depending on the type of column. They can be alphabetical (character columns), in numeric order ( number or double columns), or chronologically (date or date/time columns) . Reverse the order by using desc() :\n\nppp_small |&gt;\n  arrange ( date_approved, desc(amount) )",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#filter-rows",
    "href": "r-verb-filter.html#filter-rows",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "15.3 filter rows",
    "text": "15.3 filter rows\nfilter uses Boolean logic to allow quite sophisticated and powerful conditions to pick out just the rows you want to see or to further examine.\nWhen you use a filter, you are telling R: “Give me back only the rows where the conditional formula I give you results in true”\nThe conditions are extremely picky about characteristics like upper and lower case, spacing and punctuation. For example, “PIZZA HUT OF ARIZONA, INC” is completely different than “PIZZA HUT” or “Pizza Hut of Arizona Inc” . A simple extra character will prevent a match.1\nHere are the operators , or symbols, you’ll use to make the conditions you want to test. Your column name goes on the left, and the value you want to test is on the right:\n\n\n\n\n\n\n\n\n\nOperator\nData types\nMeaning\n\n\n\n\n==\nAll\nEquals, exactly. Note the double-equals sign, which distinguishes it from an assignment operator, such as setting new column name.\n\n\n&gt; , &gt;=\ndate, numeric\nGreater than / greater than or equal to.\n\n\n&lt;, &lt;=\ndate, numeric\nLess than / less than or equal to.\n\n\n!=\nall\nNot equal to\n\n\n%in%\nall\nMatches any of the items in a list that you type out, such as fruit %in% c(\"apple\", \"orange\").\n\n\n\n\nIn each of these cases, the column name goes on the left of the operator, and the condition goes on the right.\nYou can combine them using the Boolean operators and and or, which are used in other contexts like advanced Google searches :\n\n& means “and”. which means that both conditions must be true. It narrows your search. Think of it as a fish net, with smaller openings so only littler fish can get through. You will sometimes see a comma instead of & – they both work.\n| (the verticle bar, near the Return key) means “or” , which means that either condition can be true. It widens your search so both big and little fish can get through.\n%in% is the equivalent of an “or” condition and is used when you want to test a lot of items within one column but reduce the amount of repetitive typing.\n\nThis is sometimes confusing because it’s the exact opposite of the way we describe it in English. If we want “apples and oranges”, we have to search for an apple OR an orange in each row: fruit == 'apple' | fruit == 'orange'\nYou often need to use parentheses to tell R what order you want to evaluate the conditions when you combine “AND” and “OR” conditions.\n\n\n\n\n\n\nUnmatched parenthese cause weird errors\n\n\n\nMatching parentheses can be tricky. To make it a little easier, you can set “rainbow parentheses” in the RStudio options, which will show you the matching opening or closing brackets and parentheses in different colors when your cursor is placed next to one of them. It’s under the Tools -&gt; Global Options -&gt; Code -&gt; at the bottom of the Display tab.\n\n\n\nA few examples\nIn the PPP data, all borrower names are in upper case, and all cities and addresses are in proper case. In addition, all of the punctuation has been removed from those two columns, such as periods, commas, quotes and apostrophes.\nWhen you use a condition :\n\nNumbers are without quotes, commas, dollar signs or anything other than digits and decimal points. (1000000.24)\nText is always in quotes.\nDates are in the form “2022-01-22”.\n\n\nA list of borrowers in Flagstaff and Sedona\nStart with the borrowers just in Flagstaff.\n\nppp_small |&gt; \n  filter ( borrower_city == \"Flagstaff\") \n\nNow look for either “Flagstaff” OR “Sedona”:\n\nppp_small |&gt; \n  filter ( borrower_city == \"Flagstaff\" | borrower_city == \"Sedona\") \n\nThis could get old if you’re looking for, say 10 or 20 cities. Instead, you can put them in a list and look for them all at once, like in this query where we look for Flagstaff, Sedona and Bisbee\n\nppp_small |&gt; \n  filter ( borrower_city %in% \n             c(\"Flagstaff\", \"Sedona\", \"Bisbee\")\n           )\n\n\n\nNarrowed to under $100,000\nTo narrow it just to loans under $100,000, you can use an “&” condition, or add another filter. In practice, I will often try to separate my “AND” conditions into a new filter command — it’s sometimes easier to read than a lot of (|&,) symbols.\n\nppp_small |&gt;\n  filter ( borrower_city %in% c(\"Flagstaff\", \"Sedona\")) |&gt;\n  filter ( amount &lt; 100000) |&gt;\n  arrange (date_approved)\n\nThis is how you’d do it without the extra filter. Not the parentheses aroud the OR conditions. If you don’t have them, you ’ll get a weird (and wrong) answer – R won’t know which to do first.\n\nppp_small |&gt;\n  filter ( \n           (borrower_city == \"Flagstaff\" | borrower_city == \"Sedona\") &\n            amount &lt; 10000\n          ) |&gt;\n  arrange (date_approved)\n\n(You might note how I did the indentations above – it helps in reading the code to make clear what comes before what.)\n\n\nProjects in Maricopa County that are NOT in Phoenix or Scottsdale\nHere’s how to negate a filter - everything NOT in two cities. The ! at the beginning of the condition means that it must evaluate to FALSE instead of TRUE to be kept in the data.\n\nppp_small |&gt;\n  filter ( project_county == \"MARICOPA\" &\n             (! borrower_city %in% c(\"Scottsdale\", \"Phoenix\")) \n  ) |&gt;\n  arrange (amount)\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful when using missing data or combining conditions when you negate. The answer is often very different than you intended. It often behaves opposite of what you intended.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#using-str_like-for-inexact-matches",
    "href": "r-verb-filter.html#using-str_like-for-inexact-matches",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "15.4 Using str_like() for inexact matches",
    "text": "15.4 Using str_like() for inexact matches\nThe operators we’ve examined so far are pretty limited. You have to know exactly what’s in a character column, which can get pretty annoying when you are trying to nail down a story based on data entered by humans into computers.\nBut you can also use the results of formulas, or expressions as a condition, as long as they result in a true or false answer. One of the most powerful is the function str_like(), which was added to the tidyverse in late 2022. It allows you to use wildcard characters to find words and phrases that begin with or contain some characters, and it’s not case-sensitive!\n\n\n\n\n\n\nTip\n\n\n\nThere is a whole set of these “string” functions available in the tidyverse through its stringr package. We’ll get to some of the more common ones later in the book, especially those related to a powerful pattern-matching method called “regular expressions” or “regex”.\n\n\nThe difference in using functions is that you’ll structure your command a little differently. Instead of\nborrower_city == \"Scottsdale\", you’ll use a formula that looks like\nstr_like (borrower_city, \"scottsdale\"), where the function name comes first, then, inside parentheses, are the column you want to check and the value you want to look for.\nstr_like() uses wildcards to stand in for characters you’re not sure about. They are:\n\n_ for one missing character. Use it when you aren’t sure of spelling. The condition summari_e would be true if you weren’t sure if the British “summarise” or the American “summarize” were used in a column.\n% for any number of unknown characters, or a missing character.\nFor example, N% SCOTTSDALE R% would match “North Scottsdale Road”, “N. Scottsdale Rd” or “North Scottsdale Rd.” but not “Scottsdale” or “N Scottsdale Ave”.\n\nSmart use of wildcards can often save you a lot of typing!\n\nExamples\nYou might want to use str_like whenever you filter for words simply because it doesn’t care about upper and lower case.\nThis example would find borrowers who had addresses that were “CENTRAL AVE”, “Central Ave” or “central ave”. It excludes anyone with a street address, a suite number, or anyone on North Central Ave. because there is no wildcard:\n\nppp_small |&gt; \n  filter ( str_like ( borrower_address, \"central ave\"))\n\nBut this one finds all of the borrowers on Central Avenue:\n\nppp_small |&gt; \n  filter ( str_like ( borrower_address, \"% central ave%\") )\n\nIf you examine some of the answers, you’ll see why having wildcards is so important — there’s really no way to guess how, exactly, a borrower might have entered their address. There’s no standardization on this or on many other columns in the data.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#a-note-on-na-or-missing-data",
    "href": "r-verb-filter.html#a-note-on-na-or-missing-data",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "15.5 A note on NA, or missing data",
    "text": "15.5 A note on NA, or missing data\n\n\n\n\n\n\nMissing values never match anything!\n\n\n\nA filter will never return any rows that are missing, or the special NA value, in the column you test – it is a void that is never equal or not equal to any other value.\n\n\nIn most datasets, there is at least a little missing data. In this case, the forgiveness amount never shows the value zero. Instead, it shows NA, or the R symbol for missing data.\nThis isn’t the same as the letters “NA”. It’s a special type of value that means “nothing”. It’s not zero, it’s not higher or lower than any other value, it doesn’t ever equal anything and it doesn’t ever NOT equal anything! It stands for a value that is truly unknown. In fact, an NA doesn’t even equal another NA, because we don’t know anything about either one.\nThis means you can’t filter for NA values directly and often have to take them into account.\nTo filter for NA values, you’ll use a special function — is.na() — designed to pick out these bad actors:\n\nppp_small |&gt; \n  filter ( is.na ( forgiveness_amount ))\n\nWe’ll come back to this later, but for now just remember that missing data is a scourge on your analysis, and it will almost always be a problem you have to solve.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-filter.html#footnotes",
    "href": "r-verb-filter.html#footnotes",
    "title": "15  Verbs in depth: Select, arrange, filter",
    "section": "",
    "text": "That’s why I’ve removed all of the punctuation from your the borrower names and addresses in your data file, and turned all of the names into upper case.↩︎",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Verbs in depth: Select, arrange, filter</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html",
    "href": "r-verb-mutate.html",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "",
    "text": "16.1 mutate to create new columns\nUse the verb mutate whenever you want to create or change existing columns in your data.\nExamples of this include:\nYou will often use a combination of filtering and mutating to create a new data frame using the &lt;- assignment to use in future code chunks. That’s because they can get complex, and you don’t want to repeat code that you might have to change over and over.\nmutate uses the syntax:\nThat’s pretty abstract, so we’ll do this using examples.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html#mutate-to-create-new-columns",
    "href": "r-verb-mutate.html#mutate-to-create-new-columns",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "",
    "text": "Computing difference between numbers in two columns\nReplacing NA values with “Unknown” or zero.\nCollapsing or creating categories for more meaningful analysis\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis chapter assumes you have gone through the filtering chapter. Many of the expressions and functions are the same, so if they don’t look familiar to you, consider reviewing that chapter (again) first.\n\n\n\n    mutate ( new_column_name = function (arguments))",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html#math-on-columns",
    "href": "r-verb-mutate.html#math-on-columns",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "16.2 Math on columns",
    "text": "16.2 Math on columns\nHere’s an example of computing the portion of the original amount that was forgiven.\nUse the single “=” sign to provide a name for the new column and create more than one new column using a comma between them:\n\nppp_orig |&gt;\n1    select ( borrower_name, borrower_city, amount, forgiveness_amount) |&gt;\n    mutate ( forgiven_pct = \n2               ( forgiveness_amount  /  amount )  * 100\n            )   |&gt;                   \n3  sample_n (50)\n\n\n1\n\nPick out a few columns to work with\n\n2\n\nCompute a new column using math\n\n3\n\nPrint out a random sample of 50 rows\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA value over 100 means that the government also waived fees and interest; a value under 100 means that less than all of the loan was forgiven.\n\n\n\n\n\n\n\n\n(I did some fancy formatting so you could read the results more clearly. We’ll come back to that in a later lesson.)\n\nConverting NA to 0\nIf you look through the pages, you’ll realize that there are a lot of rows with NA instead of a value. That’s because whenever you do anything with a missing value, the result is missing. Missing values infect everything they touch. You often have to deal with them before you can do anything else.\nWe would like to convert the forgiven amount from a missing value to zero, under the idea that if they have not filled it out, nothing has (yet) been forgiven. Of course, we’d have to check that with the SBA before publication.\nThere is a specific function used for that: replace_na(), with two arguments: The column you want to check for missing values, and the value you want to use instead. This isn’t limited to numbers – you can do the same thing with words, often replacing NA with “Unknown”.\n\nppp_forgiven_fixed &lt;- \n  ppp_orig |&gt;\n  mutate (amount_forgiven = replace_na(forgiveness_amount, 0))\n\n(Note that nothing came out in this code chunk because the result was saved into a new data frame)\n\n\nDetail and total with summary statistics\nYou can use mutate to put summary statistics next to your values so you can see whether or not they are similar to the average or median.1\n\nppp_forgiven_fixed |&gt; \n  mutate ( avg_forgiven = mean(amount_forgiven), \n           median_forgiven = median(amount_forgiven)) |&gt; \n  select (borrower_name, amount_forgiven, avg_forgiven, median_forgiven ) |&gt; \n  head()",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html#creating-categories-for-easier-analysis",
    "href": "r-verb-mutate.html#creating-categories-for-easier-analysis",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "16.3 Creating categories for easier analysis",
    "text": "16.3 Creating categories for easier analysis\nVery often, you’ll want to categorize entries in a database in order to make it simpler to count and sum the values in a meaningful way. For example, the business_type column has 24 different values, including NA . Here’s a list with the number of loans in each category:\n\n\n\n  \n\n\n\n\nYes-No categories using if_else()\nOne way to work with these is to create new columns with yes-no indicators for certain types of businesses like non-profits or individuals vs. companies.\nThe function to do this is if_else() , which tests a condition exactly the same way filter did, but then assigns a value based on whether it’s met or not. You list the condition first, then a comma , then what should happen if the condition is true, and then what should happen if it’s false.\nThe condition is done the same way you did it in a filter, but instead of picking out the rows that match, mutate acts on them separately from the rows that don’t match.\nThere’s no “maybe”, except for NA’s in the original value. Here’s the general form of what it looks like:\n\nnew_column_name = if_else ( test the old column for something as in a a filter,\n                         give it a value if it's true,\n                         give it another value if it's not true)\nSo here is a way to do this with the business_type using the same %in% operator you used in the filter lesson, saving it to new data frame in your Environment, then displaying the first 10 types of businesses using count()\n\nExample 1: Two categories using exact conditions\n\nppp_category_indiv &lt;- \n  ppp_forgiven_fixed |&gt;\n  mutate ( is_individual = \n              if_else ( business_type %in% \n                          c(\"Independent Contractors\", \n                            \"Sole Proprietorship\", \n                            \"Self-Employed Individuals\", \n                            \"Single Member LLC\"), \n                 \"Individual\", \n                 \"Organization\")\n  )  \n\nNow print off a sampling of a some of the rows to take a look:\n\nppp_category_indiv |&gt; \n  select ( is_individual, business_type) |&gt; \n  sample_n (50) \n\n\n  \n\n\n\n\n\nExample 2: Two categories using wildcards\n\nppp_category_nonprofit &lt;-\n  ppp_forgiven_fixed |&gt; \n  mutate ( is_nonprofit = \n             if_else ( str_like(business_type, \"%Non_Profit%\") , \n                       \"Is nonprofit\", \n                       \"Not nonprofit\"))  \n\nYou can check it using a new verb, count(), to check the number of items by category:\n\nppp_category_nonprofit |&gt; \n  filter ( is_nonprofit == \"Is nonprofit\") |&gt; \n  count ( is_nonprofit, business_type)\n\n\n  \n\n\n\n(The profit categorization is unclear for some of the original types, such as professional associations , tribal concerns and cooperatives.)\n\n\n\nMore than two categories using case_when()\nSometimes you will want more than one outcome, such as setting a value for “High”, “Medium” and “Low”. Instead of if_then, use the function case_when, which lets you string along conditions and their outcomes. The tilde (~) is used to show what happens if it’s true. At the end, you can add an argument .default=\"Other\" to say “everything else”.\n\noriginal data |&gt; \n  case_when ( first condition ~ what if it's true,\n              second condition ~ what if  it's true, \n              third condition  ~ what if it's true, \n              .default =  what to do with everything that's left\n              )",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html#putting-it-all-together",
    "href": "r-verb-mutate.html#putting-it-all-together",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "16.4 Putting it all together",
    "text": "16.4 Putting it all together\nHere is how you could set a column to with five types of borrowers instead of three. Once you put it together, you have a much more manageable list of five instead of 25 categories:\n\nppp_business_categories &lt;- \n  ppp_forgiven_fixed |&gt;\n  mutate (  new_business_type = \n              \n          case_when (  str_like(business_type, \"%non_profit%\") \n1                         ~ \"Non-profit\",\n                       \n                      business_type %in% \n                         c(\"Independent Contractors\", \n                            \"Sole Proprietorship\", \n                            \"Self-Employed Individuals\", \n                            \"Single Member LLC\")  \n2                        ~ \"Individual\",\n                      \n                       business_type == \"Tribal Concerns\" \n3                        ~ \"Tribal concerns\",\n                      \n                       str_detect (business_type, \n                                   \"LLC|Company|Corporation|Partnership\") \n4                         ~ \"Companies\",\n                      \n5                       .default = \"Other\"\n            )\n)\n\n\n1\n\nThe same code you used for non-profits, but with a ~ instead of a comma.\n\n2\n\nThe same code you used for individuals\n\n3\n\nA new type, for Tribal concerns,\n\n4\n\nand another new type for traditional companies, and\n\n5\n\nfinally, what you want it to say when none of the conditions are true.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis code chunk uses a function you haven’t seen yet, called str_detect(), which is a powerful way to streamline the conditions. Instead of using str_like(), with its limited wildcards, this is based on regular expressions, which we’ll look at later. But the way it’s used here is to separate words that might be in the description with “|”, for “OR”.\n\n\nTake a look at how the categories are distributed now:\n\nppp_business_categories |&gt;\n  count ( new_business_type)\n\n\n  \n\n\n\n\nSave it for use in another program\nSaving this for future use means you don’t have to worry anymore about some of the missing values, and you can filter and group by the simpler new business type instead of the original. This saves a data file called ppp_edited.RDS in your project folder.\n\nsaveRDS(ppp_business_categories, file=\"ppp_edited.RDS\")",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-verb-mutate.html#footnotes",
    "href": "r-verb-mutate.html#footnotes",
    "title": "16  Verbs in depth: New from old data with Mutate",
    "section": "",
    "text": "If you skipped converting the NA values above, the answers will always come out as NA. ↩︎",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Verbs in depth: New from old data with Mutate</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html",
    "href": "r-groupby-story.html",
    "title": "17  Summarizing detail into totals",
    "section": "",
    "text": "17.1 Summarizing with groups\nSummarizing a list of items in R is done using the verbs group_by and summarize. Think of grouping as answering the questions, “How many?” and “How much?”. They are particularly powerful when your question also has the words “the most” or the “the least” or “of each”. Some examples:",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html#summarizing-with-groups",
    "href": "r-groupby-story.html#summarizing-with-groups",
    "title": "17  Summarizing detail into totals",
    "section": "",
    "text": "Which Zip Code had the most crimes?\nWhat month had the least total rainfall?\nHow much did each candidate raise last quarter?\nIn playing cards, how many of each suit do I have in my hand?\nOn average, are Cronkite students taller or shorter than in other schools?\n\n\nConfusing grouping with sorting or arranging\nMany reporters confuse this summarization with “sorting”. One reason is that this is how we express the concept in plain language: “I want to sort Skittles by color”.\nBut in data analysis, sorting and and grouping are very different things. Sorting, which is done in R via the arrange() verb, involves shuffling a data frame’s rows into some order based on the values in a column. Grouping is a way to aggregate and compute summary statistics such as a count (the number of items), sum (how much they add up to), or average for category. It means “make piles and compute statistics for each one.”\n\n\nWhen to use filter vs. summary\nSomething that trips up beginners is a desire to see details and totals at the same time, which is more difficult than it sounds.\nA filter is used to display your selected items as a list. You’ll get to see all of the detail and every column. As a convenience, R shows you how many items are in that filtered list (usually). That’s great when you want to just look at them, or get more information about them. For instance, if you had a list of crimes by ZIP code, you might just want to see the list in your neighborhood – where, exactly, were they? When did they happen? Was it at night or the morning? What crimes happened on which blocks?\nAggregates are used when you just want to see summaries – does my ZIP code have more crime than others? Are robberies more common than car theft in my Zip code, and how does that compare to others?\nIn practice, you’ll go back and forth between summary and detail. They’re both important, just different.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html#motivational-exercise",
    "href": "r-groupby-story.html#motivational-exercise",
    "title": "17  Summarizing detail into totals",
    "section": "17.2 Motivational exercise",
    "text": "17.2 Motivational exercise\nThis exercise helps you internalize the differences between filtering and aggregating (or grouping). It uses a list of fatal police shootings logged by the Washington Post, downloaded in late 2022. The column names are pretty self-explanatory, and we’ll only be looking at the ethnicity column. (Scroll to the right if you can’t see it.)\n\n\nCreate a Quarto document in your project\nAdd the usual setup chunk to load the tidyverse, using the library command. Copy and paste the chunk from a previous lesson if you need to.\nCopy and paste this into a code chunk to read the data.\n\n\n\nwaposhootings &lt;- \n  readRDS ( \n        url ( \n           \"https://cronkitedata.s3.amazonaws.com/rdata/waposhootings.RDS\"\n           )\n        )\n\nExamine the data:\n\nwaposhootings |&gt; \n  sample_n (50)\n\n\n  \n\n\n\n\nFiltering method\nUsing a pen and paper, write out something that looks like the table below. The total number comes from the number of rows in the data frame.\n\n\n\nEthnicity\n# of victims\n% of total\n\n\n\n\nBlack, non-Hispanic\n\n\n\n\nWhite, non-Hispanic\n\n\n\n\nHispanic\n\n\n\n\nTOTAL\n5,945\n100%\n\n\n\n\nCopy and paste this code to list all fatal shootings of Black, non-Hispanic victims. Look at the number of rows returned and enter it into your handmade table.\n\n\nwaposhootings |&gt; \n  filter ( ethnicity == \"Black, non-Hispanic\")\n\n\nRun this code to get the number of White, non-Hispanic victims and write the answer down on your handmade table\n\n\nwaposhootings |&gt; \n  filter ( ethnicity == \"White, non-Hispanic\")\n\n\nRun this filter to get the number of Hispanic victims, and write the answer down.\n\n\nwaposhootings |&gt; \n  filter ( ethnicity == \"Hispanic\")\n\n\nCompute the percentages\n\n\nCompute the percent of total each group represents. Do this on a calculator (or just type the formula into Google). Remember, the percent of total formula is: group / total * 100\nWrite the answers into your handwritten chart.\nNow, rewrite your chart so that the highest percentage is on the first row, the second highest on the second row, and the third highest on the third row.\nRepeat your calculations, since you always have to do every hand calculation twice. They won’t add up to 100% because we left out people who had an unknown ethnicity or other ethnicity.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html#aggregating-for-an-easier-way",
    "href": "r-groupby-story.html#aggregating-for-an-easier-way",
    "title": "17  Summarizing detail into totals",
    "section": "17.3 Aggregating for an easier way",
    "text": "17.3 Aggregating for an easier way\nThat’s a lot of work. Imagine if you had to do this 50 times for 50 states, or even more for every Zip code? There has to be an easier way, and there is: Grouping and summarizing.\nWe’ll build the code one piece at a time so you can see what’s happening:\n\nCopy this code, one chunk at a time, into your Quarto document and run them to see what the answers are.\n\n\nGet the totals\n\n\nwaposhootings |&gt; \n  summarize ( num_shootings = n() ) \n\n\nAdd a “group” to summarize by ethnicity\n\n\nwaposhootings |&gt; \n  group_by (ethnicity) |&gt; \n  summarize (num_shootings = n() )\n\n\nArrange to get the highest number at the top\n\n\nwaposhootings |&gt; \n  group_by ( ethnicity) |&gt; \n  summarize (num_shootings = n() ) |&gt; \n  arrange ( desc (num_shootings ))\n\n\nCompute the percent\nNotice that the “mutate” line is exactly the same formula you typed into your calculator. The only difference is that we computed the total number of shootings instead of typing it. \nThis is one of the most common patterns you’ll use in data reporting, and we can get much more sophisticated with it. Don’t expect to understand it fully right away, but you should recognize the idea: If you find yourself filtering in order to count something, consider using grouping instead.\n\n\nwaposhootings |&gt; \n  group_by ( ethnicity) |&gt; \n  summarize ( num_shootings = n() ) |&gt; \n  mutate ( total = sum ( num_shootings ) , \n           percent = num_shootings /total * 100 ) |&gt; \n  arrange ( desc ( num_shootings))",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html#differences",
    "href": "r-groupby-story.html#differences",
    "title": "17  Summarizing detail into totals",
    "section": "17.4 Differences",
    "text": "17.4 Differences\nWhen you filtered, you could see the details of every row. That’s great if you want to explore it or find examples. But it’s not great for that sentence you need when you want to say which group is the largest or smallest, or compare the numbers across groups.\nWhen you grouped, you got statistics by ethnicity, but you couldn’t see the details.\nTypically, you’ll go back and forth between aggregating and filtering, using each method to get to different goals. Don’t fall in the trap of trying to force all of your questions into a filter, or all of your questions into aggregation. Instead, think about which method will give you the answers you want most efficiently.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-groupby-story.html#your-questions",
    "href": "r-groupby-story.html#your-questions",
    "title": "17  Summarizing detail into totals",
    "section": "17.5 Your questions:",
    "text": "17.5 Your questions:\n\nOn your Quarto document, try to write out, in plain English, what you just did. See if you can articulate what the difference between filtering and aggregating is, in your own words.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Summarizing detail into totals</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html",
    "href": "r-verb-groupby.html",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "",
    "text": "18.1 summarize for statistics\nsummarize computes summary statistics such as the number of rows in a data frame or the sum of dollar values. It removes the original columns completely, and only produces the summary statistics you compute within that statement. Using summarize alone produces a data frame with one row. It’s the equivalent of putting nothing in your pivot table in Excel other than the “Values” area.\nAnother way to think of summarize is that it collapses your list of items (loans, in our example) into a statistical report.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#summarize-for-statistics",
    "href": "r-verb-groupby.html#summarize-for-statistics",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "",
    "text": "The dreaded NA\nYou saw in the mutate section that missing values are always a problem. Because they’re unknown, they can’t match anything else, they can’t be considered 0, and they can warp any answers you get. But there’s usually nothing you can do about missing data, so you have to tell the program exactly what to do about them.\nThere are two choices:\n\nLet them infect everything they touch, turning everything into NA. In this scenario, a total of the dollar values in a column would be NA if any of the values in that column is missing:\n\n\nppp_orig |&gt; \n  summarize ( total = sum(forgiveness_amount))\n\n\n  \n\n\n\n\nIgnore them in a computation completely, effectively removing that value from your calculation.\n\nThere’s no right answer, and it depends on what you’re doing. In some cases, you know that they stand for the value 0, and in others you don’t. We will usually ignore them by adding an argument to every summary function that could be infected by them : na.rm = TRUE , which means, “remove NA’s before you do anything.”.\n\nppp_orig |&gt; \n  summarize ( total = sum (forgiveness_amount, na.rm=TRUE))\n\n\n  \n\n\n\n\n\nSummary functions\nSome of the common functions you’ll use to summarize are :\n\nmean (column_name, na.rm=T) – for an average : Numbers only\nsum (column_name, na.rm = T): Numbers only\nn() – for “how many”, or “count”. Anything - this counts rows, not values\nn_distinct ( column_name) : The number of unique entries in the column. Use it to see how many categories there are in a column.\nmedian (column_name, na.rm=T): Numbers only\nmin (column_name, na.rm=T): Dates and numbers\nmax (column_name , na.rm=T): Dates and numbers\n\nWhen used on the whole data frame, it’s customary to just glimpse the output, since there’s only one row:\n\nppp_orig |&gt; \n  summarize ( n(), \n              mean (amount, na.rm=T), \n              mean (forgiveness_amount, na.rm=T), \n              min (date_approved, na.rm=T), \n              max (date_approved, na.rm= T), \n              n_distinct ( business_type)\n  ) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 6\n$ `n()`                                 &lt;int&gt; 169259\n$ `mean(amount, na.rm = T)`             &lt;dbl&gt; 73206.66\n$ `mean(forgiveness_amount, na.rm = T)` &lt;dbl&gt; 77050.41\n$ `min(date_approved, na.rm = T)`       &lt;date&gt; 2020-04-03\n$ `max(date_approved, na.rm = T)`       &lt;date&gt; 2021-06-29\n$ `n_distinct(business_type)`           &lt;int&gt; 24\n\n\nThis produced a data frame with 1 row and 5 columns. The column names are the same as the formulas that created them, which is difficult to work with. Create new column names using the name (in back-ticks if it’s got spaces or special characters) and assign them the values of the summaries using the = sign:\n\nppp_orig |&gt; \n  summarize ( number_of_rows =  n(), \n              mean_amount = mean (amount, na.rm=T),    \n              median_amount = median (amount, na.rm=T),\n              mean_forgiven = mean (forgiveness_amount, na.rm=T),   \n              first_loan = min (date_approved, na.rm=T),   \n              last_loan = max (date_approved, na.rm= T), \n              business_type_count = n_distinct(business_type)\n  ) |&gt;\n  glimpse()\n\nRows: 1\nColumns: 7\n$ number_of_rows      &lt;int&gt; 169259\n$ mean_amount         &lt;dbl&gt; 73206.66\n$ median_amount       &lt;dbl&gt; 20800\n$ mean_forgiven       &lt;dbl&gt; 77050.41\n$ first_loan          &lt;date&gt; 2020-04-03\n$ last_loan           &lt;date&gt; 2021-06-29\n$ business_type_count &lt;int&gt; 24\n\n\nNote that the mean forgiven removes those with missing values for the forgiven amount, which is wrong! We need to turn them into zeroes first.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#grouping-for-lists",
    "href": "r-verb-groupby.html#grouping-for-lists",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "18.2 Grouping for lists",
    "text": "18.2 Grouping for lists\nNow that you know how to summarize the whole data frame, you’ll want to start getting totals by category. This is the same thing as a pivot table in spreadsheets: the column names that create the “groups” are the equivalent of the Rows area a spreadsheet pivot table:\n\n\nGrouping by one column\nIn the PPP data, the “draw” refers to which of the two programs was involved - the original one created in April 2020, or the one with stricter criteria passed by Congress that December.\nHere’s how we’d get some key statistics by draw:\n\nppp_orig |&gt;\n  group_by ( draw ) |&gt;\n  summarize ( first_loan = min ( date_approved ), \n              total_amount = sum (amount), \n              total_forgiven = sum (forgiveness_amount, na.rm=T), \n              `# of loans` = n() \n  )\n\n\n  \n\n\n\nHere are a couple of things to note about grouped output:\n\nThe only columns saved are the ones that are shown in either the group_by or summarize rows. All of the other original columns have been eliminated. You no longer have them to work with .\nThe names of the columns for the summary statistics are the ones defined before the “=” sign in the summarize statement.\nTRAP! Don’t ever name your summary columns the same thing as a group_by column. It will override those names, and your output will be unintelligible.\n\n\n\n\n\n\n\nNaming your columns\n\n\n\nNote that the name of the columns doesn’t always follow our standard. In this case, # of loans has a special character and spaces. In order to create or use it, you must enclose them in back-tics (`) or you’ll get an error.\n\n\n\n\nGrouping by more than one column\nIf you wanted to know the numbers outstanding and forgiven by draw, you could add another column to the group by:\n\nppp_orig |&gt;\n  group_by ( loan_status, draw ) |&gt;\n  summarize ( first_loan = min ( date_approved ), \n              total_amount = sum (amount), \n              total_forgiven = sum (forgiveness_amount, na.rm=T), \n              loan_ct = n() \n  )\n\n\n  \n\n\n\n\n\nA shortcut : count()\nIf all you want to do is count or add by group, you can use the count() function as a shortcut. It does the exact same thing as a combination of group_by() and summarize( n() ) and arrange()` to get the number of items in each category, sorted by the most frequent to least:\n\nppp_orig |&gt; \n  count ( loan_status, draw, \n          sort=TRUE,\n          name = \"loan_ct\")\n\n\n  \n\n\n\n\n\n\n\n\n\nNew ways to summarize\n\n\n\nA new version of the tidyverse, in 2023, has changed some of the options for the summarize() verb that make grouping unnecessary much of the time, but it’s confusing. I’m skipping it for now.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#using-and-converting-groups",
    "href": "r-verb-groupby.html#using-and-converting-groups",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "18.3 Using and converting groups",
    "text": "18.3 Using and converting groups\n\nConverting from long to wide data\nYou’ve looked at different ways to think about data, but when we talk about “granular” data, “database” thinking or “tidy” data, we generally mean we’re working with what some people call “long” rather than “wide” data. That is, there are many rows but few columns.\nBut your instinct is to want to look at a rectangle of wide data, with the values of one column down the side and another across the top. Helpfully, a function called pivot_wider() does just that – pivots your data from long to wide.1\nStart with a simple query with two grouping columns (note that I’ve called the number of loans loan_ct, so it’s easier to work with later on.\n\n\n\n\n\n\n\n  \n\n\n\n\nThis is really hard to read. Turn it on its head with pivot_wider():\n\nNormally, you’ll only want to have one summary statistic shown in a rectangle, with one column spread across the top and another column shown in rows. There are a lot of advanced options in pivot functions that let you show more than one statistic at a time, and tell R how to name them. There will be a chapter later on that addresses a lot of the problems you have in reading tables, so we’ll put that off for now.\nHere’s an explanation of what the command looks like.\n```{r}\n\npivot_wider ( id_cols =  column that you want to see as is down the side,\n              names_from =  column with the words you want to see across the top, \n              values_from = column with the numbers you want in the middle\n              ) \n```\n\n\n\n\n\n\nTip\n\n\n\nYour instinct will be to turn your data into one of these wider tables right from the start, but try to overcome it. The tidyverse expects to do most of its work in “long” format, saving the “wide” format just for printing.\n\n\n\n\nTotals and subtotals\nYou noticed that when you created the summaries, there was no option to create a “percent of total” such as the percent of loans in each draw, or the percent of money that had been forgiven.\nYou can use summary functions outside a summarize statement!\nThis means that you can compute the percent of total, the same way you used an option in pivot tables. This took me a long time to understand, so try to slow down, and just try it a few times! When you look carefully at your output, you’ll start to understand it better.\nThe trick is to summarize, then use mutate to add a column with the percentages made out of totals:\n\nppp_orig |&gt;\n  group_by ( draw) |&gt;\n  summarize ( loan_count = n() ) |&gt;\n  mutate ( all_loans = sum (loan_count), \n           pct_of_total = loan_count / all_loans * 100\n           )\n\n\n  \n\n\n\n\nWhat happens if you have more than one group?\nThis is where the idea of grouped data gets a little confusing. It depends on exactly how you did your summarize statement. But if you use the default mechanism, the “all_loans” is the subtotal. The default behavior is that the “groups” are kept for all but the last column listed in the group_by statement, meaning any summaries you do off of the data will refer to the subtotal.\n\nppp_orig |&gt;\n  group_by ( draw, loan_status) |&gt;\n  summarize ( loan_count = n() ) |&gt;\n  mutate ( loans_in_draw = sum (loan_count),\n           pct_of_draw = loan_count / loans_in_draw * 100)\n\n\n  \n\n\n\nHere’s a pretty typical way to do this: Create a subtotal, use it for your percentages, then pivot the percentages:\n\nppp_orig |&gt; \n  group_by ( draw, loan_status) |&gt; \n  summarize ( loan_count = n() ) |&gt; \n  mutate ( loans_in_draw = sum(loan_count), \n           pct_of_draw = loan_count / loans_in_draw * 100 ) |&gt; \n  pivot_wider ( \n    id_cols = c(draw, loans_in_draw), \n    names_from = loan_status, \n    values_from = pct_of_draw,\n    values_fill = 0)\n\n`summarise()` has grouped output by 'draw'. You can override using the\n`.groups` argument.\n\n\n\n  \n\n\n\nNow you can easily compare the outcome by draw, by reading across to reach 100% and reading down to compare them.\nWe’ll have a whole chapter / week on making good tables that are readable and understandable. For now, just remember that it’s always possible to turn a data frame on its head, and that you can compute much of what you need BEFORE you do that.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#practice",
    "href": "r-verb-groupby.html#practice",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "18.4 Practice",
    "text": "18.4 Practice\nPutting together the grouping and summarizing, along with the commands you learned last chapter to filter, arrange and display the head() and tail() of a dataset should equip you to write the code for these questions:\n\nWhich lenders provided the most loans?\nWhich lenders provided the most amount of money loaned?\nWhich borrowers got the least amount of money?\nShow the number of loans in each draw that went to the 24 (including NA) types of businesses. To see them all on one screen, add “, rows.print=25” to the heading of the code chunk like this: {r  , rows.print=25}\nTry to compute the percent of loans that went to projects in each county in Arizona. This will require first filtering, then grouping.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#understanding-grouped-data",
    "href": "r-verb-groupby.html#understanding-grouped-data",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "18.5 Postscript: Understanding grouped data",
    "text": "18.5 Postscript: Understanding grouped data\nYou may have noticed an odd warning after you run the code with multiple grouping columns, for example:\n\n\n`summarise()` has grouped output by 'draw'. You can override using the `.groups` argument.\" \n\n\nWhat does that mean?\nWhen you grouped by loan status and draw, R effectively split up your data frame into five independent and completely divorced piles - one for each combination of draw and loan status that it found. It processed them one by one to create the output data frame that was printed out.\nAfter it’s done summarizing your data, R doesn’t know what you want to do with the piles – keep them, or put everything back together again.\nBy default, after you group by more than one column, it maintains the separate piles for all but the last group in your list under group_by – in this case the loan_status. Here, everything you do after this will work on three piles separately.The message tells you what it did with the piles, and how to change that behavior.\nThe documentation of grouped data provides details of how each of the tidyverse’s verbs handle grouped data.\nHere’s what a “glimpse()” looks like for a data frame that has retained some groups:\n\nppp_orig |&gt; \n  select ( loan_status, date_approved:amount) |&gt; \n  group_by ( loan_status) |&gt; glimpse()\n\nRows: 169,259\nColumns: 11\nGroups: loan_status [3]\n$ loan_status      &lt;chr&gt; \"Paid in Full\", \"Paid in Full\", \"Paid in Full\", \"Paid…\n$ date_approved    &lt;date&gt; 2020-04-10, 2020-04-11, 2020-04-11, 2020-04-29, 2020…\n$ draw             &lt;chr&gt; \"First\", \"First\", \"First\", \"First\", \"First\", \"First\",…\n$ borrower_name    &lt;chr&gt; \"SFE HOLDINGS LLC\", \"NAVAJO TRIBAL UTILITY AUTHORITY\"…\n$ borrower_address &lt;chr&gt; \"9366 East Raintree Drive\", \"Po Box 170\", \"2999 N44th…\n$ borrower_city    &lt;chr&gt; \"Scottsdale\", \"Fort Defiance\", \"Phoenix\", \"Tucson\", \"…\n$ borrower_state   &lt;chr&gt; \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\",…\n$ borrower_zip     &lt;chr&gt; \"85260\", \"86504\", \"85018\", \"85711\", \"85250\", \"85012\",…\n$ franchise_name   &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ loan_status_date &lt;date&gt; 2021-08-17, 2022-02-05, 2021-09-25, 2021-08-21, 2021…\n$ amount           &lt;dbl&gt; 10000000, 10000000, 10000000, 10000000, 10000000, 100…\n\n\nNotice the “Groups” row at the top – that tells you it’s got three piles, defined by the loan_status column.\n\n\nGetting rid of the message\nYou can do two things to get rid of the message. I suggest the first of these, since it makes you explicitly decide what to do each time, depending on your goal:\n\nAdd a .groups=... argument that looks like this at the end of the summarize statement. This example tells R to do what it does by default, with no warning:\n\n\nppp_orig |&gt;\n  group_by ( loan_status, draw ) |&gt;\n  summarize ( `# of loans` = n() , \n              .groups = \"drop_last\"\n  )\n\n\n  \n\n\n\nThe other possibilities are : .groups=\"drop\" and \".groups=\"keep\" (Note the period before the word “groups”. I have no idea why, but sometimes options are indicated this way.)\n\nAdd a line to your setup chunk, changing the default behavior through the systemwide options:\noptions(dplyr.summarise.inform = FALSE)\n\n\n\nWhat does “tidy” data have to do with groupings?\nGrouped data effectively breaks out values of categories and treats them independently, which is the equivalent of temporarily treating them as their own data frame.\nIt’s somewhat difficult in the tidyverse to summarize across columns – it really wants to summarize rows. In a spreadsheet, it’s just as easy to write an =sum(B1:J1) as it is =sum(B1:B12). That’s not true in R journey so far. This is probably the first time your instinct would be to wreck a perfectly good dataset.\nWe’ll come back to all of that, but just remember that it’s possible to do all kinds of computations within a group that you’d normally think you want to do across columns. One example is, say, percent change over time. Instead of trying to compute them one by one, you can use groups and the lag() function to do math that depends on a previous row.\n\n\n\nstate\ncounty\nmonth\ncases\n\n\n\n\nAL\nAuburn\n2020-04-01\n24\n\n\nAL\nAuburn\n2020-05-01\n35\n\n\nAL\nAuburn\n2020-06-01\n200\n\n\n\ncovid_data |&gt;\n  group_by (state, county) |&gt; \n  arrange (month) |&gt; \n  mutate ( change = cases - lag(cases) , \n           pct_change = change / lag(cases) * 100 ) \nThis method will start over for each county, so it will be NA for the first month within each county.\nThis is just one example of how grouped data is quite powerful when used correctly. There are many others, such as extracting the most recent event in a court history by case. Try to think about how one group would be computed, and then don’t worry how the rest will work – R will do that thinking for you.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-verb-groupby.html#footnotes",
    "href": "r-verb-groupby.html#footnotes",
    "title": "18  Verbs in depth: Aggregating with groups",
    "section": "",
    "text": "Yes, there is something called pivot_longer(), which lets you turn rectangular data into the tidy form.↩︎",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Verbs in depth: Aggregating with groups</span>"
    ]
  },
  {
    "objectID": "r-groupby-skittles.html",
    "href": "r-groupby-skittles.html",
    "title": "19  Aggregating with Skittles",
    "section": "",
    "text": "19.1 Skittles by color\nThe following code chunk reads the list of Skittles from a location on the web:\nskittle_list &lt;- \n  readRDS( url (\"https://cronkitedata.s3.amazonaws.com/rdata/skittle_list.RDS\"))\nHere’s what that list looks like in a sortable, filterable table. (You’ll learn this soon):\nTry filtering the list above to see how many of each color was in this particular package. (Look at the number of rows at the bottom after filtering the list in place.)",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Aggregating with Skittles</span>"
    ]
  },
  {
    "objectID": "r-groupby-skittles.html#skittles-by-color",
    "href": "r-groupby-skittles.html#skittles-by-color",
    "title": "19  Aggregating with Skittles",
    "section": "",
    "text": "Compute how many Skittles of each color\nTo keep the computation simple, use this code chunk to create a new data frame of five rows and two columns: The color, and the number of Skittles.\n\nskittle_counts &lt;-\n  skittle_list |&gt; \n  group_by ( color) |&gt; \n  summarize ( skittles = n() )\n\nNow print the list:\n\nskittle_counts\n\n\n  \n\n\n\nTake a look at the new data frame carefully. It’s no longer a one-by-one list of each Skittle. Instead, it’s a summary of the number of Skittles by color. The original color column still exists because it was in the group_by statement, but the ID is gone. More importantly, there’s a new column called skittles, which is a count of the number of skittles for each row. There are only five rows, not 60.\nThere are an unequal number of Skittles in this package, but it’s hard to express them in numbers we understand well. To do that, you’ll want to compute a percent of total.\n\n\nCompute a percent of total\nTo compute a percent of total, you’ll use the formula category / total * 100. However, we have no total here. One way to do it divide each row by 60, since we know that’s how many rows were in the original. Try it:\n\nskittle_counts  |&gt; \n  mutate ( percent = skittles / 60 * 100)\n\n\n  \n\n\n\nBut a more efficient, and more reproducible way to do it would be to let R compute the total into a new column. This suggests you’ll need a mutate verb:\n\nskittle_counts |&gt; \n  mutate ( total_skittles = sum ( skittles )) |&gt; \n  mutate ( percent = skittles / total_skittles * 100 )",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Aggregating with Skittles</span>"
    ]
  },
  {
    "objectID": "r-groupby-skittles.html#add-a-column-who-got-each-skittle",
    "href": "r-groupby-skittles.html#add-a-column-who-got-each-skittle",
    "title": "19  Aggregating with Skittles",
    "section": "19.2 Add a column: Who got each Skittle?",
    "text": "19.2 Add a column: Who got each Skittle?\nLet’s make this a little more complex. I’ve randomly split up the Skittles between me an my husband. The new data frame that includes our names is loaded this way:\n\nskittle_name_list &lt;- \n  readRDS( url (\"https://cronkitedata.s3.amazonaws.com/rdata/skittle_name_list.RDS\"))\n\nAgain, you can filter it to see how many orange Skittles Sarah got, or how many purple ones Duke got. This could get old fast if you had to do this for both people and all five colors.\n\n\n\n\n\n\nBut grouping it can make short work of it. This example saves it into a new data frame with 10 rows and three columns. The “.groups=”drop”” part removes any default grouping left over by default.\n\nskittle_distribution &lt;- \n  skittle_name_list |&gt; \n  group_by ( color, whose) |&gt; \n  summarize ( skittles = n() , \n              .groups = \"drop\")\n\nLook at it:\n\nskittle_distribution\n\n\n  \n\n\n\n\nPivot to view the names side-by-side\nThat’s hard to read - let’s pivot it so that the names are across the top and the colors down the side:\n\nskittle_distribution |&gt; \n  pivot_wider ( id_cols = color, \n                names_from = whose, \n                values_from = skittles)\n\n\n  \n\n\n\nSarah never got any fewer Skittles than Duke – they were all higher, except for the tie in yellow ones.\n\n\nSkittle percentages by person\nBut was she more or less likely to get any given color? You need percents for that:\nStart with the total by person :\n\nskittle_distribution |&gt; \n  group_by ( whose ) |&gt; \n  mutate ( person_total = sum ( skittles ) ) \n\n\n  \n\n\n\nNotice that grouping by whose Skittles creates a total of 23 for Duke and 37 for Sarah. Now we can compute the percent by adding one line to the query:\n\nskittle_distribution |&gt; \n  group_by ( whose ) |&gt; \n  mutate ( person_total = sum ( skittles ) ) |&gt; \n  mutate ( percent = skittles / person_total * 100 )\n\n\n  \n\n\n\nTo say this in English, we can say that “Duke was less likely to get green Skittles, but more likely to get Yellow ones” based on a comparison of those percentages.\nThat’s really hard to read, though.\n\n\nPivot again for easier reading\nInstead, we want to turn the chart into something that has the colors down the side, and the names across the top. Each column would then add up to 100%.\n\nskittle_distribution |&gt; \n  group_by ( whose ) |&gt; \n  mutate ( person_total = sum ( skittles ) ) |&gt; \n  mutate ( percent = skittles / person_total * 100 ) |&gt; \n  pivot_wider ( id_cols = color, \n                names_from = whose, \n                values_from = percent)\n\n\n  \n\n\n\nReading left to right, you can see that Sarah was more likely to get green and red Skittles, but much less likely to get yellow, purple and orange ones, controlling for the larger of number of Skittles she got in the first place.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Aggregating with Skittles</span>"
    ]
  },
  {
    "objectID": "r-groupby-skittles.html#takeaway",
    "href": "r-groupby-skittles.html#takeaway",
    "title": "19  Aggregating with Skittles",
    "section": "19.3 Takeaway",
    "text": "19.3 Takeaway\nIf you could follow this Skittles example, imagine all of the stories that depend on exactly this construct: Are Black motorists more likely to be searched than white? Are you more likely to die if you got a COVID vaccine? Is a homicide in Baltimore more likely to be solved than in Chicago?\nThey all depend on two-way tables like this one, with the independent variable listed down the side, and the dependent variable listed across the top , with the “column percents” in the middle.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Aggregating with Skittles</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html",
    "href": "r-verb-join.html",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "",
    "text": "20.1 Join basics and relational databases\nHere is a great explainer on joins made by a previous MAIJ student, Andy Blye. Be sure to watch it:\njoining in computer programming matches columns in one table to another, where the values within one or more columns match exactly. Here’s an example from Jenny Bryan’s Stat 545 course textbook:",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#join-basics-and-relational-databases",
    "href": "r-verb-join.html#join-basics-and-relational-databases",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "",
    "text": "superheroes\n\n\n\n\nRelational databases\nMost large data systems created since the 1980s are called “relational databases”, which means that each unit (person, ticket, vehicle) is stored in a different table1, and common columns link them together. They also routinely use codes or shorthand to store data, and provide another table to “look up” the details.\nFor example, your student ID is stored in one place in the university system with your name, address, email address, etc. When you sign up for courses, the database looks up that information to attach to your schedule. You only have to change it in one place, and it is automatically sent out through all of the interactions you have with the university.\nSimilarly, the course number, section and term is all that needs to be stored in the system for any semester. Those data points then populate the name of the course, the students registered for it, and holidays.\nThe system is created this way because it’s more efficient and reliable. Important information is stored only once, and can then be applied to millions of rows.\nExamples of relational databases include:\n\nCampaign finance systems, where donors are stored in one table and candidates in another, linked through a candidate or political action committee id.\nInspection records, such as those for restaurants, hospitals, housing code violations and workplace safety, which typically have at least three tables: The establishment (like a restaurant or a workplace), an inspection (an event on a date), and a violation (something that they found). Each table has its own ID, which is used whenever they are linked together.\nA court docket data system, which usually has many types of rows: A master case table links to information on charges, defendants, lawyers, sentences and court hearings.\n\nThis is similar to, but stricter than, the tidy data principles of separating different kinds of information into separate data frames.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#join-syntax",
    "href": "r-verb-join.html#join-syntax",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.2 join syntax",
    "text": "20.2 join syntax\n\n\n\n\n\n\nTip\n\n\n\nThere are actually pretty good explanations of the concept of joining and the variations of it in R in the documentation.\n\n\nThere are several kinds of joins, but the syntax is similar across them.\n  old_table |&gt;\n     inner_join (new_table , \n     join_by = (name of old_table column == name of new_table column) )\n     \n\n\n\n\n\n\nTip\n\n\n\nTry to piece together what this is saying, similar to the filter conditions: If the two columns are equal (double-equal signs), put them together.\n\n\n\nTypes of joins\n\nAn inner_join means that the value(s) in the common columns must match in BOTH tables – it will eliminate any row without a match\nA left_join or right_join keeps everything from one table, and only the information that matches from the other. Those columns will contain NA wherever the match fails.\nA full_join puts together both tables no matter whether there is any match. It’s pretty rare to use this.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#matchmaking-with-joins",
    "href": "r-verb-join.html#matchmaking-with-joins",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.3 Matchmaking with joins",
    "text": "20.3 Matchmaking with joins\n\nAttaching characteristics to a dataset\nYou’ll often want to learn more about a geographic area’s demographics, voting habits or other characteristics, and match it to other data.\nSometimes it’s simple: Find the demographics (Census data frame) of counties that switched from Trump to Biden (voting results) as a way to isolate places you might want to visit.\nAnother example from voting might be to find the precinct that has the highest percentage of Latino citizens in the county, then match that precinct against the voter registration rolls to get a list of people you might want to interview on election day. In these instances, the join is used as a filter, but it comes from a different table. .\nThis is also common when you have data by zip code or some other geography, and you want to find clusters of interesting potential stories, such as PPP loans in minority neighborhoods.\n\n\nSummarize data against another dataset\nThe previous examples all result in lists of potential story people or places. If you use join on summarized data, you can characterize a broad range of activity across new columns. Simplified, this is how you can write that more PPP money went to predominantly white neighborhoods than those that were majority Black.\n\n\n“Enterprise” joins\nInvestigative reporters often use joins in ways unintended by the data creators. In the 1990s, they dubbed these “enterprise” joins, referring to the enterprising reporters who thought of them. In these instances, reporters combine datasets in ways that find needles in haystacks, such as:\n\nSchool bus drivers who have had tickets for driving while intoxicated.\nDay care center operators who are listed on the sex offender registry.\nDonors to a governor who got contracts from the state\n\nWhen you match these kinds of datasets, you will always have mistakes — some apparent matches are incorrect in the real world; some matches that should exist are ignored because of variations in names or other details. You always have to report out any suspected matches, so they are time consuming stories.\nIn the mid-2000s, when some politicians insisted that dead people were voting and proposed measures to restrict voting, almost every regional news organization sent reporters on futile hunts for the dead voters. They got lists of people on the voter rolls, then lists of people who had died through the Social Security Death Index or local death certificates. I never met anyone who found a single actual dead voter, but months of reporter-hours were spent tracking down each lead. Instead, they were people who had not yet been eliminated on the rolls but never voted. In others, they were people with the same names who had nothing to do with the dead person. In still others, they were the same people, but very much alive!\nIt’s very common for two people to have the same name in a city. In fact, it’s common to have two people at the same home with the same name – they’ve just left off “Jr.” and “Sr.” in the database. In this case, you’ll find matches that you shouldn’t. These are false positives, or Type I errors in statistics.\nWe rarely get dates of birth or Social Security Numbers in public records, so we have to join by name and sometimes location. If someone has moved, sometimes uses a nickname, or the government has recorded the spelling incorrectly, the join will fail – you’ll miss some of the possible matches. This is very common with company names, which can change with mergers and other changes in management, and can be listed in many different ways.\nThese are false negatives, or Type II errors in statistics.2\nIn different contexts, you’ll want to minimize different kinds of errors. For example, if you are looking for something extremely rare, and you want to examine every possible case – like a child sex offender working in a day care center – you might choose to make a “loose” match and get lots of false positives, which you can check. If you want to limit your reporting only to the most promising leads, you’ll be willing to live with missing some cases in order to be more sure of the joins you find.\nYou’ll see stories of this kind write around the lack of precision – they’ll often say, “we verified x cases of….” rather than pretend that they know of them all.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#using-lookup-tables-ppp-industry-codes",
    "href": "r-verb-join.html#using-lookup-tables-ppp-industry-codes",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.4 Using lookup tables: PPP industry codes",
    "text": "20.4 Using lookup tables: PPP industry codes\nA lookup table is a list of unique items that translates codes to words. One example is the industry code in the PPP data. The data itself only has the NAICS code – a standard government scheme that categorizes every business into one of about a thousand possible industries. But you don’t know what those codes mean. You need an index, or lookup table, to tell you that.\nWe’ll use a table that contains the list of industries and match it to the PPP data. (The lookup table was derived from the concordance package in R, but is fully explained at the Census website.)\n\nOnce you load these data frames, be sure to explore them a little to make sure you understand what they contain.\n\nThe following code chunk loads both the original PPP data and the code table:\n\nnaics_codes &lt;- readRDS( url ( \"https://cronkitedata.s3.amazonaws.com/rdata/naics_lookup.RDS\"))\nppp_orig &lt;- readRDS (url ( \"https://cronkitedata.s3.amazonaws.com/rdata/ppp_az_loans.RDS\"))\n\nglimpse(naics_codes)\n\nRows: 1,042\nColumns: 7\n$ naics_code     &lt;chr&gt; \"111110\", \"111120\", \"111130\", \"111140\", \"111150\", \"1111…\n$ sector_code    &lt;chr&gt; \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"11\", \"…\n$ subsector_code &lt;chr&gt; \"111\", \"111\", \"111\", \"111\", \"111\", \"111\", \"111\", \"111\",…\n$ sector_desc    &lt;chr&gt; \"Agriculture, Forestry, Fishing and Hunting\", \"Agricult…\n$ subsector_desc &lt;chr&gt; \"Crop Production\", \"Crop Production\", \"Crop Production\"…\n$ naics_desc     &lt;chr&gt; \"Soybean Farming\", \"Oilseed (except Soybean) Farming\", …\n$ naics_cronkite &lt;chr&gt; \"11 - Agriculture\", \"11 - Agriculture\", \"11 - Agricultu…\n\n\nNotice that the industry code is 7 characters long, and has a detailed description. There are also “sector” and “subsector” codes, which use only the beginning of the code, and link to more general descriptions.\n\nAttaching words to codes\nIn this example, we’ll take a small set of columns from the original table, and show how it links to the lookup table:\n\nppp_orig |&gt; \n  select ( borrower_name, amount, naics_code) |&gt; \n  inner_join ( naics_codes, \n               join_by (naics_code == naics_code) \n  ) |&gt; \n  glimpse()\n\nRows: 166,672\nColumns: 9\n$ borrower_name  &lt;chr&gt; \"SFE HOLDINGS LLC\", \"NAVAJO TRIBAL UTILITY AUTHORITY\", …\n$ amount         &lt;dbl&gt; 10000000, 10000000, 10000000, 10000000, 10000000, 10000…\n$ naics_code     &lt;chr&gt; \"722310\", \"221122\", \"621610\", \"621111\", \"517311\", \"6211…\n$ sector_code    &lt;chr&gt; \"72\", \"22\", \"62\", \"62\", \"51\", \"62\", \"56\", \"62\", \"48\", \"…\n$ subsector_code &lt;chr&gt; \"722\", \"221\", \"621\", \"621\", \"517\", \"621\", \"561\", \"621\",…\n$ sector_desc    &lt;chr&gt; \"Accommodation and Food Services\", \"Utilities\", \"Health…\n$ subsector_desc &lt;chr&gt; \"Food Services and Drinking Places\", \"Utilities\", \"Ambu…\n$ naics_desc     &lt;chr&gt; \"Food Service Contractors\", \"Electric Power Distributio…\n$ naics_cronkite &lt;chr&gt; \"722 - Restaurants and food service\", \"22 - Utilities\",…",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#joining-risks",
    "href": "r-verb-join.html#joining-risks",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.5 Joining risks",
    "text": "20.5 Joining risks\n\njoining tl;dr\nThere are lots of risks in joining tables that you created yourself, or that were created outside a big relational database system. Keep an eye on the number of rows returned every time that you join – you should know what to expect.\n\n\nDouble counting with joins\nWe won’t go into this in depth, but just be aware it’s easy to double-count rows when you join. Here’s a made-up example, in which a zip code is in two counties.\nSay you want to use some data on zip codes :\n\n\n\nzip code\ncounty\ninfo\n\n\n\n\n85232\nMaricopa\nsome data\n\n\n85232\nPinal\nsome more data\n\n\n\nand match it to a list of restaurants in a zip code:\n\n\n\nzip code\nrestaurant name\n\n\n\n\n85232\nMy favorite restaurant\n\n\n85232\nMy second-favorite restaurant\n\n\n\nWhen you match these, you’ll get 4 rows:\n\n\n\nzip code\ncounty\ninfo\nrestaurant name\n\n\n\n\n85232\nMaricopa\nsome data\nMy favorite restaurant\n\n\n85232\nPinal\nsome more data\nMy favorite restaurant\n\n\n85232\nMaricopa\nsome data\nMy second-favorite restaurant\n\n\n85232\nPinal\nsome more data\nMy second-favority restaurant\n\n\n\nNow, every time you try to count restaurants, these two will be double-counted.\nIn computing, this is called a “many-to-many” relationship – there are many rows of zip codes and many rows of restaurants. In journalism, we call it spaghetti. It’s usually an unintended mess.\n\n\nLosing rows with joins\nThe opposite can occur if you aren’t careful and there are items you want to keep that are missing in your reference table. If there were invalid NAICS codes in the original data, they would have been eliminated from the resulting joined table.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#congratulate-yourself",
    "href": "r-verb-join.html#congratulate-yourself",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.6 Congratulate yourself",
    "text": "20.6 Congratulate yourself\n\n\nCongratulations! This is the last key verb that you need to understand to address most stories. Your palette now has all of the primary colors!",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#resources",
    "href": "r-verb-join.html#resources",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "20.7 Resources",
    "text": "20.7 Resources\n\nThe “Relational data” chapter in the R for Data Science textbook has details on exactly how a complex dataset might fit together.\nAn example using a superheroes dataset, from Stat 545 at the University of British Columbia",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-verb-join.html#footnotes",
    "href": "r-verb-join.html#footnotes",
    "title": "20  Verbs in depth: Matchmaking with joins",
    "section": "",
    "text": "another name for a data frame↩︎\n I remember them by thinking of the boy who cried wolf. When the village came running and there was no wolf, it was a Type I error, or false positive ; when the village ignored the boy and there was a wolf, it was a Type II error, or false negative.↩︎",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Verbs in depth: Matchmaking with joins</span>"
    ]
  },
  {
    "objectID": "r-recipes.html",
    "href": "r-recipes.html",
    "title": "21  Recipes",
    "section": "",
    "text": "21.1 Starting a document\nErase everything it gives you, and use our usual setup. Copy it from a document that you like, set up a start-up document, or copy it from Sarah’s Github gist https://gist.github.com/sarahcnyt/e60ad2d7ccf65498fc88791f3bb683ae.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#starting-a-document",
    "href": "r-recipes.html#starting-a-document",
    "title": "21  Recipes",
    "section": "",
    "text": "Create or open a project - remember to name it lower-case with no spaces or quotes\nThen either move or download any needed files into that folder. (Rstudio , Finder or Explorer).\nCreate or open a document that you want to edit in File -&gt; New -&gt; Quarto Document.\n\n\n\nEdit the “setup” chunk to include any libraries you need for your document. Examples of other libraries include reactable for interactive tables, or sf for maps. You may need readxl if your data is coming from an Excel file rather than a csv or internet file.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#r-annoyances-and-errors",
    "href": "r-recipes.html#r-annoyances-and-errors",
    "title": "21  Recipes",
    "section": "21.2 R annoyances and errors",
    "text": "21.2 R annoyances and errors\nThere are several annoyances that aren’t consistent from one function in R to the next. These issues will fix many of the errors in your code, even when the error message isn’t clear. When you’re having trouble, look for\n\nSpelling - it’s amazing how many times you can look for a spelling error and not see it. If you get an error that says something like object not found, check your spelling.\nFailing to run previous code chunks: Every time you open R, or when you Render a document, R starts from scratch. Consider using the Run -&gt; Run All Chunks Above menu item (or the button just next to the Play button on a code chunk) to make sure you’re up to date. Be sure to close out of R completely every once in a while. Otherwise you might be depending on something you erased from your document.\nStray characters . These are REALLY hard to see. It might be an extra quote mark, an extra dash, a stray period. Unfortunately, there are very few useful error messages with these. Scour your code for them.\nQuoting issues - what kind of quotes, whether they’re needed, and if they’re matched open and closed.\nUnmatched or missing parentheses\nPutting words on the wrong side of equal signs. To create a new column name, put it on the left. To identify a column to be used as an argument, put it on the right. To filter a column, use two equals signs, with the column name on the left.\nAre you working with a list of items, such as a set of criteria in an %in% comparison? If so, you need to wrap them in the c() function, for “combine”\nCase-sensitivity in column names; back-ticks for more than one word in a column name rather than quotes.\nMissing or hanging pipes (|&gt; or %&gt;%, depending on which version of R you’re using.)\n\nYou often have to run your code chunk twice after fixing an error. (There is still a mistake in R’s innards that has to get flushed out.)\n\nGetting help\nSometimes you stare at a problem for a long time without seeing what’s wrong. Rather than do that, consider using AI to help you. One of the things it’s best at is to give you explanations or fixes. Just remember that we are working in the Tidyverse, and in the more modern version of it. That means the free version of Chat GPT will give you old, and possibly kind of weird, answers. I suggest https://perplexity.ai or the free Github Copilot that’s available in Microsoft Bing.\nHere are a couple of examples, based on real-world problems that students have had problems with.\n\nExample 1: Stray character\n\nmy_data &lt;--\n  shootings |&gt; \n  filter ( ethnicity == \"White\")\n\nError in FUN(left): invalid argument to unary operator\n\n\nGoing to ChatGPT, here is my prompt and its answer. While the answer isn’t entirely clear, it points you to the problem – an extra dash in the assignment of the result into my_data.\n\n\n\n\nExample 2: Misspelling a data colummn\n\nshootings |&gt; \n  filter ( ethnicty == \"White\")\n\nError in `filter()`:\nℹ In argument: `ethnicty == \"White\"`.\nCaused by error:\n! object 'ethnicty' not found\n\n\nIn this, case, the error is reasonably understandable But you could ask Github Copolit to explain it:",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#getting-data-into-your-environment",
    "href": "r-recipes.html#getting-data-into-your-environment",
    "title": "21  Recipes",
    "section": "21.3 Getting data into your environment",
    "text": "21.3 Getting data into your environment\n\nIf you are referring to a file on your computer or on the web, then it must be in quotes.\nUse the proper library and function to read the data.\nCheck the output for proper treatment of text, dates and numbers, especially with Excel and text imports\nread_csv() (with an underscore) reads a plain text comma-delimited file from your project or the internet. It has cousins, such as read_tsv() for tab-delimited or read_delim() for files that have odd characters separating the columns.\nread_excel() gets a file from an Excel file (.xlsx). You must add library(readxl) to your libraries at top for it to work, and it won’t work on files stored on the interet.\nreadRDS() and load() reads R-native files. RDS files contain one data frame, which you have to assign to a new object. Rda files have multiple objects that are already named. Use readRDS ( url ( ....) ) to read data stored on the internet instead of in your project, with the web address in quotes.\n\n\nImport options\nThere are options in most of the importing functions that tell you how to treat each column and let you rename at the same time. One approach is :\nTo keep everything as text, which you can convert later.\n\nwapo_orig &lt;- \n  read_csv( \"https://raw.githubusercontent.com/washingtonpost/data-police-shootings/master/v1/fatal-police-shootings-data.csv\", \n            col_types= c (.default=\"c\")\n            )\n\nCheck your data using glimpse() or in the Environment tab to make sure you know what TYPE of data each column represents, and how the entries are entered – upper case, lower case, etc.\n\n\nRe-using answers from a previous step\nAssign your code chunk to a new variable in the environment to use it again later. This is useful when:\n\nYou have a complex filter that you want to apply to future steps.\nYou are joining data frames with columns having the same names: select just the columns you need and re-name any of the ones that are the same, except for the join one.\nYou have fixed some column types or adjusted their values.\nYou don’t want to look at so many columns and don’t need them for anything.\n\nUse the select verb to pick out and rename your columns. Put it as the last thing in your code chunk.\nThe assignment operator is &lt;-, which means “pour the answer into this variable name” You can use the keyboard shortcut OPT/ALT - to insert it automatically.\n\nselect_wapo  &lt;- \n  wapo_orig |&gt; \n  select ( id, name, date, armed, gender, race, city, state, signs_of_mental_illness, flee)\n\n\n\nOperators and symbols\nOperators and comparison symbols are here https://cronkitedata.github.io/djtextbook/quickstart-program.html#operators-assignment and here https://cronkitedata.github.io/djtextbook/r-verb-filter.html#filter-rows . It’s skipped &lt;-, which is the “assignment operator” to create a new data frame in the Environment.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#filtering",
    "href": "r-recipes.html#filtering",
    "title": "21  Recipes",
    "section": "21.4 Filtering",
    "text": "21.4 Filtering\nUse the same strategies to create new columns from old conditionally using mutate()\n\nOne condition that’s a number :\n\n  filter ( amount &gt; 1000 )\n\nA number between two values\n\n  filter ( between ( amount, 0, 1000))\n\nAn exact phrase or word\n\n  filter ( project_county == \"MARICOPA\")\n\nOne of several possible entries in one column (exactly)\n\n    filter ( project_county %in% c(\"MARICOPA\", \"PIMA\", \"PINAL\" ))\n\nEverything except missing values\n\n    filter ( ! is.na (project_county) )\n\nBetween two dates . Be sure it’s really a date in the data by glimpsing your data frame. If not, turn it into a date first.\n\n    filter ( approval_date &gt;= \"2021-01-24\" & \n           approval_date &lt;= \"2021-01-31\")\n\nPhrases, words or letters at the beginning of a column\n\n    filter ( str_like (borrower_type , \"Non-Profit%\"))\n\nPhrases, letters or words at the end of a column\n\n    filter ( str_like  (borrower_type, \"Corporation%\"))\nAll of these examples can be used in a mutate statement to create flags or new values if the conditions are met.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#mutate-functions",
    "href": "r-recipes.html#mutate-functions",
    "title": "21  Recipes",
    "section": "21.5 Mutate functions",
    "text": "21.5 Mutate functions\n\nif_else() : Choose one of two options.\ncase_when() : More than two options.\nreplace_na() : Change all NA values to something else, a constant not a value from another column.\nDealing with dates:\n\nAlready in the form yyyy-mm-dd, but text. Use as.Date()\nNeed to parse it from US-style dates, use mdy() . This depends on the lubridate library.\nTake the first x number of characters from a text value: str_sub(column_name, 1, x)",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#aggregate-count-sum-rows-by-category",
    "href": "r-recipes.html#aggregate-count-sum-rows-by-category",
    "title": "21  Recipes",
    "section": "21.6 Aggregate (count, sum rows by category)",
    "text": "21.6 Aggregate (count, sum rows by category)\n\nCounting (How many?)\n\nTop 10 list:\n\nwapo_orig |&gt; \n    count (state,  name=\"shootings\") |&gt; \n    arrange ( desc ( shootings )) |&gt; \n    head (10)\n\n\n  \n\n\n\nMake sure you don’t name the new column containing the count the same thing as a group_by() column.\n\nCounting unique entries\n\nSometimes you want to know how many items of a type, not how many rows, are included in a category.\n\n        group_by ( state) |&gt;\n        summarize ( number_of_shootings = n(), \n                    number_of_cities = n_distinct ( city )\n                    )\n\n\nSumming (how much?)\n\n\n    group_by (project_county) |&gt;\n    summarize ( total_amount = sum (amount, na.rm=T))",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#recoding-categories",
    "href": "r-recipes.html#recoding-categories",
    "title": "21  Recipes",
    "section": "21.7 Recoding categories",
    "text": "21.7 Recoding categories\nYou’ll often want fewer categories, or numbers in categories, that you want to use instead of the original values. This is done in a mutate statement. Don’t forget to save the output to a new data frame (&lt;-), or you won’t have access to it later on.\n\nCreate yes-no categories . This is really “Yes”, “No” or NA, where there is an NA to begin with.\n\n\n    mutate ( corp_yn = if_else  \n                      (str_like (borrower_type, \n                                   \"%Corporation%\"), \n                       \"Yes\", \n                       \"No\")\n       )\n\n\nRecode into more than two categories using case_when()\n\nThis example introduces str_detect(), which uses regular expressions. We’ll go over that later in the book.\n\n  mutate (  new_business_type = \n                case_when (  str_like(business_type, \"%non_profit%\") ~ \"Non-profit\",    #1st category\n\n                             business_type %in% \n                               c(\"Independent Contractors\", \n                                \"Sole Proprietorship\", \n                                \"Self-Employed Individuals\", \n                                  \"Single Member LLC\")              ~ \"Individual\",    # 2nd category\n                             \n                             business_type == \"Tribal Concerns\"     ~ \"Tribal concerns\",    #3rd category\n                             \n                             str_detect (business_type, \"LLC|Company|Corporation|Partnership\") ~ \"Companies\",  #4th category\n                             \n                             TRUE ~ \"Other\")         #catchall 5th category\n            )\n\n\nRecode numbers into categories\n\nIt’s often useful to give them numeric codes in front so they sort properly:\n\n        mutate ( new_type = \n                 case_when  (\n                    amount &lt;= 1000 ~ \"00-Very low\", \n                    amount &lt;= 10000  ~ \"01-Low\", \n                    amount &lt;= 100000 ~ \"03-Medium\", \n                    amount &gt; 100000 ~ \"04-High\")\n              )\n\nThis works because the first one that it finds will be used, so a value of exactly 1,000 would be “Very low”, but a value of 1,001 would be “Low”.\n\nUsing a “lookup table” with a join to convert codes to words\n\nGet or create a lookup table\nIt usually has two columns - the original code, and the words you want to use. In this example, you can create it in-line using tibble() function:\n\nlkp_race &lt;- \n  tribble ( ~race_code, ~race_desc , \n            \"A\", \"Asian\",\n     \"W\", \"White\",\n     \"H\", \"Hispanic / Latino\",\n     \"B\", \"Black\", \n     \"O\", \"Other\",\n     \"N\", \"Unknown\")\n\nNow you can join it (using left-join in case of NA’s)\n\nwapo_with_desc &lt;- \n  select_wapo |&gt; \n  left_join ( lkp_race, \n              join_by ( race == race_code))",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#working-with-grouped-data-for-subtotals-changes-percents",
    "href": "r-recipes.html#working-with-grouped-data-for-subtotals-changes-percents",
    "title": "21  Recipes",
    "section": "21.8 Working with grouped data for subtotals, changes, percents",
    "text": "21.8 Working with grouped data for subtotals, changes, percents\n\nPercent of total by group\n\n\nwapo_with_desc |&gt; \n1  group_by ( state, race_desc ) |&gt;\n2  summarize ( shootings = n() , .groups=\"drop_last\") |&gt;\n3  mutate ( all_shootings = sum (shootings) ) |&gt;\n4  mutate ( pct_shootings = shootings / all_shootings * 100 )\n\n\n1\n\nGroup by two or more columns. The one that you want to subtotal goes last.\n\n2\n\nCompute “how many” for each “group” and save it to a column called “shootings”\n\n3\n\nAdd those up for a subtotal by state in this case.\n\n4\n\nCreate the percentage.\n\n\n\n\n\n  \n\n\n\n\nAlternate approach: count then sum\nThis is a little more intuitive because of when the grouping happens:\n\n   count (state, name_desc,  name=\"shootings\") |&gt; \n   group_by ( state ) |&gt; \n   mutate ( subtotal = sum (shootings ) , \n            pct = shootings  / subtotal * 100 ) |&gt; \n\nRun each step separately before adding the next step and examine the output to see what is happening.\n\nDisplay results as in spreadsheet form\n\nTo see the items across the top, use pivot_wider.\n  group_by ( state, race_desc) |&gt;\n  summarize ( shootings = n() ) |&gt;\n  pivot_wider ( names_from = race_desc, \n                id_cols = c( state), \n                values_from = shootings)\nYou can add an argument after values_from if you know that any missing values are zero, by using values_fill=0\nYou usually only choose one column to show down the side, one column to spread across the top, and one column to display the value.\n\nChange over time\n\nTo get the change over time, convert the data into years, then use the lag() function in a mutate within groups. You must sort (arrange) the rows in the proper order first.\n\nwapo_with_desc |&gt; \n  mutate ( year = year (as.Date(date))) |&gt; \n  count ( state, year , name=\"shootings\") |&gt; \n  arrange ( state, year ) |&gt; \n  group_by ( state ) |&gt; \n  mutate ( change = shootings - lag (shootings))\n\n\n  \n\n\n\n\nPick out the last item in a group, with all of its columns.\n\nNew verb introduced : slice_tail() .\nThis is particularly useful for chronological events, such as the last thing that happened in a court case, or the most recent complaint against a police officer. This example isn’t a great one, but it gets you the name and other details of the most recent shooting in each state:\n        arrange ( state, date) |&gt;\n        group_by ( state) |&gt;\n        slice_tail(n=1)\nNOTE: Don’t use slice_max(), because it will give you back multiple rows in the event of ties.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "r-recipes.html#quarto-document-tips",
    "href": "r-recipes.html#quarto-document-tips",
    "title": "21  Recipes",
    "section": "21.9 Quarto document tips",
    "text": "21.9 Quarto document tips\n\nYAML (front matter)\nThe YAML is very picky about exact casing, spacing and other details. If you try to render and you get a YAML error, it is likely at the top.\nTypical YAML front matter that goes within the three dashes beginning on the very first line:\n\ntitle: \"Name of document\"\nauthor: \"Your name\"\noutput: \n  html: \n    theme: cosmo\n    code-tools: true\n    embed-resources: true\n    toc: true\nexecute: \n   warning: true\n   error: true\n   eval: true\n\n\nCode chunk options\nThese must be the very first rows in your code chunk, and must start in the first position, with spacing as shown. Use #| label: setup in the first code chunk to run it automatically every time you start up R.\n\n\nNothing runs; everything is gray\nYou probably erased the last three back-ticks at the end of the code chunk. You may need to restart R to get it re-set.",
    "crumbs": [
      "Verbs of the tidyverse",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Recipes</span>"
    ]
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced topics",
    "section": "",
    "text": "More complicated data structures\nThe previous chapters all dealt with data that comes in data frames, or tibbles, much like a spreadsheet. You had a small introduction to the idea of vectors (lists) at the beginning, but we haven’t really focused on these.\nTechnically, a data frame is a list of lists – the rows make up the first list, and the columns the second. Why do you have to know this? Scraping data, in particular, relies on working with lists. They’re difficult – I have a really hard time dealing with them in the most efficient way.\nI’ll introduce you to some lists in the section on regular expressions, just to get you used to the idea that not everything comes in data frames. When you get to scraping and extracting data from APIs like the Census, it will become more important.",
    "crumbs": [
      "Advanced topics"
    ]
  },
  {
    "objectID": "advanced.html#practice-data",
    "href": "advanced.html#practice-data",
    "title": "Advanced topics",
    "section": "Practice data",
    "text": "Practice data\nOur PPP data isn’t great for some of the practice in these chapters. I’ll introduce some other data sets, but their content isn’t as important as it was in the last section – they’re only there to show you examples without trying to make specific claims about the data.",
    "crumbs": [
      "Advanced topics"
    ]
  },
  {
    "objectID": "advanced-regex.html",
    "href": "advanced-regex.html",
    "title": "22  Regular expressions Part I",
    "section": "",
    "text": "Sample data\nThe following sample data will be used in this lesson:",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#building-blocks-of-regular-expressions",
    "href": "advanced-regex.html#building-blocks-of-regular-expressions",
    "title": "22  Regular expressions Part I",
    "section": "22.1 Building blocks of regular expressions",
    "text": "22.1 Building blocks of regular expressions\n\nLiteral strings\nThese are just letters, words or precise phrases like “abc” or “Mary”. They are case-sensitive by default.\nSome characters are used as special instructions in regular expressions and have to be “escaped” by adding backslash in front of them. Three of the most common are:\n\nperiods (\\.)\nquestion marks (\\?),\nand parentheses \\(\n\nSo if you want to search for Patriot Front (USA), you’d need to type Patriot Front \\(USA\\).\nIn fact, these escape characters can get unweildy, so in the next chapter you’ll see a way to make it a little easier to read.\n\n\nStart or end with…\n^ = “Find only at the beginning of a line”  $ = “Find only at the end of a line”\n\n\nWild cards and quantifiers\nA wild card is a character you use to indicate the word “anything”. Here are some ways to use wild cards in regex.\n\n\n\n\n\n\n\nRegex\nWhat it means\n\n\n\n\n. (a period)\nAny single character of any type\n\n\n?\nMight or might not exist\n\n\n* (asterisk)\nAny number of times repeated, including none. One of the most common patterns is .*, which means “anything or nothing”, the equivalent of the “%” you used in str_like()\n\n\n+\nExists one or more times.\n\n\n{1,3}\nExists between 1 and 3 times (change the number to change the repetition)\n\n\n\nRegular expressions also have wild cards of specific types. These are called “character classes” and are often what makes regular expressions so powerful. Here are some common character classes. When you begin to learn a new language, you’ll look up what the character classes are in that implementation of regular expressions.1\n\n\n\n\n\n\n\nClass code\nWhat it means\n\n\n\n\n\\d\nAny digit\n\n\n\\w\nAny word character. This is upper or lowercase letters, numbers and an underscore\n\n\n\\s\nAny whitespace (tab, space, etc.)\n\n\n\\b\nAny word boundary (period, comma, space, etc.)\n\n\n\\n\nA new line – carriage return or line feed.\n\n\n\nThese can be negated by upper-casing them. For example ‘\\D’ means anything EXCEPT a digit.\nThere are other, more complicated codes for punctuation, special characters like emojis, and others.\nYou can define your own character classes by enclosing them in brackets: []. This can be particularly useful in searching for alternative spellings or abbreviations. For example, in R, the work “summarize” is spelled both using the American and British method. To look for either spelling, you could write summari[zs]e\n\n\nAlternation and precedence\n“Alternation” is a fancy way of saying “either or”, and “precedence” is a fancy way of saying “this before that”. They often go together in regular expressions.\n|= Alternation, or “OR”, the same character you use in filters.  () = precendence (and grouping) operators\nIt’s easy to use alternation when it’s a simple word or phrase. But usually, it’s a little more complex and you have to tell the regex what portion of the pattern is the either-or. An example is looking for a year. 19|20\\d\\d won’t work – it gets either 19, or 20xx. Instead, (19|20)\\d\\d will give you the full year.\nParentheses are also used for “capture groups”, which lets you re-use what you found later on, such as replacing part but not all of a string.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#testing-and-examining-patterns-with-regex101",
    "href": "advanced-regex.html#testing-and-examining-patterns-with-regex101",
    "title": "22  Regular expressions Part I",
    "section": "22.2 Testing and examining patterns with Regex101",
    "text": "22.2 Testing and examining patterns with Regex101\nRegex 101 is a website that lets you copy part of your data into a box, then test different patterns to see how they get applied. Regular expressions are very difficult to write and to read, but Regex101 lets you do it a little piece at a time.\n\nLooking for specific words or characters\nThe easiest regex is one that has just the characters you’re looking for when you know that they are the right case. They’re called literals because you are literally looking for those letters or characters.\nType “Mary had a little lamb” into the Test String box, then type “little” into the Regular Expression box. You should get something like this, showing the match that was found and and explanation of the results. The flags here are not very important right now.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#practice-1-extract-date-parts",
    "href": "advanced-regex.html#practice-1-extract-date-parts",
    "title": "22  Regular expressions Part I",
    "section": "22.3 Practice #1: Extract date parts",
    "text": "22.3 Practice #1: Extract date parts\nIf you think about it, figuring out the pieces of American-style dates can be difficult: The year is at the end, and there sometimes are no leading zeroes in front of the month and year.2 This part of the lesson will show you how to isolate pieces of dates using regular expressions.\nCopy and paste these dates into the regex 101 window:\n9/7/2017\n9/11/1998\n9/11/2017\n9/19/2018\n9/15/2017\n10/13/2019\n11/3/2017\n\nGet the year\n\nTry to think of three ways to isolate the year. They might include words begining with “19” or “20”, or the last four characters of the string, or four digits after a non-digit.\n\nPossible answers:\n\\d+$\n\\D\\d{4}\n[0-9]*$\n(19|20)\\d\\d\n\nExamine the output in the regex101 window to see how it found (or didn’t find!) your year.\n\n\nUsing capture groups to rearrange the dates\nUsing capture groups in parentheses, try to isolate each of the pieces of the date. Then click on the menu bar to open the substitution section. Here, you’ll see how to isolate pieces of the date, then rearrange it into year-month-day. (It’s still not great - you’d need to do some more munging to add a leading zero to the month and day, but for now this is good practice. )\nTry coming up with the rest of it on your own before you type in the answer:\n  (\\d+)\\W(\\d+)\\W(\\d+)\n\nNow each piece has its section, numbered 0 for the whole match, and then 1-3 for the pieces.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#practice-2-extract-pieces-of-phone-numbers",
    "href": "advanced-regex.html#practice-2-extract-pieces-of-phone-numbers",
    "title": "22  Regular expressions Part I",
    "section": "22.4 Practice #2: Extract pieces of phone numbers",
    "text": "22.4 Practice #2: Extract pieces of phone numbers\nHere are some phone numbers in different formats:\n623-374-1167\n760.352.5212\n831-676-3833\n(831)-676-3833\n623-374-1167 ext 203\n831-775-0370\n(602)-955-0222  x20\n928-627-8080\n831-784-1453\nThis is a little more complicated than it looks, so try piecing together what this one says:\n  (\\d{3})\\D+(\\d{3})\\D+(\\d{4})\nAnything within parentheses will be “captured” in a block.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#practice-3-extract-address-pieces",
    "href": "advanced-regex.html#practice-3-extract-address-pieces",
    "title": "22  Regular expressions Part I",
    "section": "22.5 Practice #3: Extract address pieces",
    "text": "22.5 Practice #3: Extract address pieces\nHere are a few lines of the data from Prof. McDonald’s tutorial, which you can copy and paste to go his exercise. (He uses the Javascript version of regular expressions, but for our purposes in this exercise, it doesn’t matter which one you use. If you choose Python, you’ll have one extra step, of putting a slash () before the quotes. The colors work a little better if you leave it on the default PHP method.)\n\"10111 N LAMAR BLVD\nAUSTIN, TX 78753\n(30.370945933000485, -97.6925542359997)\"\n\"3636 N FM 620 RD\nAUSTIN, TX 78734\n(30.377873241000486, -97.9523496219997)\"\n\"9919 SERVICE AVE\nAUSTIN, TX 78743\n(30.205028616000448, -97.6625588019997)\"\n\"10601 N LAMAR BLVD\nAUSTIN, TX 78753\n(30.37476574700048, -97.6903937089997)\"\n\"801 E WILLIAM CANNON DR Unit 205\nAUSTIN, TX 78745\n(30.190914575000477, -97.77193838799968)\"\n\"4408 LONG CHAMP DR\nAUSTIN, TX 78746\n(30.340981111000474, -97.7983147919997)\"\n\"625 W BEN WHITE BLVD EB\nAUSTIN, TX 78745\n(30.206884239000487, -97.7956469989997)\"\n\"3914 N LAMAR BLVD\nAUSTIN, TX 78756\n(30.307477098000447, -97.74169675199965)\"\n\"15201 FALCON HEAD BLVD\nBEE CAVE, TX 78738\n(30.32068282700044, -97.96890311999965)\"\n\"11905 FM 2244 RD Unit 100\nBEE CAVE, TX 78738\n(30.308363203000454, -97.92393357799966)\"\n\"3801 JUNIPER TRCE\nBEE CAVE, TX 78738\n(30.308247975000484, -97.93511531999968)\"\n\"12800 GALLERIA CIR Unit 101\nBEE CAVE, TX 78738\n(30.307996778000472, -97.94065088199966)\"\n\"12400 W SH 71 Unit 510\nBEE CAVE, TX 78733\n(30.330682136000462, -97.86979886299969)\"\n\"716 W 6TH ST\nAUSTIN, TX 78701\n(30.27019732500048, -97.75036306299967)\"\n\"3003 BEE CAVES RD\nROLLINGWOOD, TX 78746\n(30.271592738000436, -97.79583786499967)\"\nExamine each of the lines to see what you want to extract. In our case, the relevant things to notice are:\n\nEach of the addresses starts and ends with a quotation mark. This makes it easy to find the beginning of each address.\nThe city names are in upper case, followed by a comma, a space, and the state abbreviation.\nThe zip code comes at the end of a line, is all numerals, and is preceded by a space.\n\nThis process of identifying the consistency and inconsistency in data is useful not only in building regular expressions, but in data cleaning in general. We’ll see how that works when we get to OpenRefine later on.\n\n\nFind the Zip Code for each line.\nFind the city names as a list\nFind the latitude (the first large number in the coordinates)\n\nSee if you can make substitutions so that you get a list of addresses in a single line, without the latitude and longitude, such as “12400 W SH 71 Unit 510 BEE CAVE, TX 78733”\n\nThis process is not as outlandish as it seems. It’s quite frequent that you get lists of addresses on separate rows, but you need to import them into a data frame as a single (or “parsed” set of rows.\nI often do the initial process of using regular expressions to fix problems like this in OpenRefine, which is designed for data cleanup. A future lesson will introduce you to that free software.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex.html#footnotes",
    "href": "advanced-regex.html#footnotes",
    "title": "22  Regular expressions Part I",
    "section": "",
    "text": "There are several “flavors” or regex, with slightly different sets of characters and options↩︎\nIn R, this is why we always use the lubridate library. It understands a lot of the the variations.↩︎",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Regular expressions Part I</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html",
    "href": "advanced-regex-r.html",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "",
    "text": "23.1 Resouces for using regular expressions in R",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#resouces-for-using-regular-expressions-in-r",
    "href": "advanced-regex-r.html#resouces-for-using-regular-expressions-in-r",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "",
    "text": "The tidyverse cheat sheet on the package stringr (which is party of tidyverse) has the outlines of each function that uses these regular expression. It’s a little hard to read, but has all of the shortcuts on the second page. Look at the part that says, “type this”, which translates to “to mean this” as in Regex101.\nNathan Collins, a student in the 2022 MAIJ cohort, [did an explainer on using str_detect() on the ACLED])(https://npcdata.github.io/projects/collins_explainer.html) data that many of you are using for your projects\n\n\n\n\n\n\n\nImportant\n\n\n\nIn R, regular expressions are a little different than in the Python and other languages listed in regex101 .\n\nThe expressions are still case sensitive! In real life, it’s often easier to use a combination of functions, first to turn the column into lower case, and then to use the regular expression on that. It’s much easier than trying to match which case it is.\nInstead of one \\ backslash (\\), there are two (\\\\) to “escape” special characters like parentheses or to use a special type of character such as a digit.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#sample-data",
    "href": "advanced-regex-r.html#sample-data",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.2 Sample data",
    "text": "23.2 Sample data\nFor this example, we’ll use a small set of Tweets from Donald Trump, representing the last few months he was on on the platform. It runs from Nov. 1, 2020 to Jan. 8, 2021, when Twitter banned him. Retweets were removed, but there are a number of tweets that are just links to other content.\nHere is a random sample of the 1,112 tweets:\n\n\n\n\n\n\nUsing tweets as a source for data is a little dated, but it’s a good example of trying to pick out text from a set of short items.\n\nString functions for regular expressions\n\nstr_detect( col_name, expression) looks for the presence of the expression, and returns TRUE or FALSE.\nstr_extract(col_name, expression) does the same thing, except it returns whatever the first thing that matched your expression. It includes wild cards, so it could just return the whole phrase.\nstr_match(col_name, expression) is more complicated - it lets you use pieces of the expression, but it returns a list, which is hard to work with. We’ll skip that for now.\n\nSome other useful string functions are:\n\nTo convert the case, use str_to_lower(colname), str_to_upper(colname), or str_to_title()\n\nTo smush together phrases, use str_c() or paste() or paste0.\n\n\n\nConvert everything to lowercase\nOne strategy for using regular expressions is to convert everything to lower case before trying to match text. In R, the regular expression is, by default, case-sensitive and it’s difficult to predict how it might show up in free text like this. 1\n\ntrump_tweets &lt;-\n  trump_tweets |&gt; \n  mutate ( tweet_lowercase = str_to_lower(text))",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#using-str_detect",
    "href": "advanced-regex-r.html#using-str_detect",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.3 Using str_detect",
    "text": "23.3 Using str_detect\nThe function str_detect() results in either TRUE or FALSE. It’s very useful in two situations:\n\nAs a filter for rows in your data.\nAs a way to assign values in an if_else() or case_when() expression, when creating a new column using mutate().\n\n\nExample 1: Filter for “biden”\n\ntrump_tweets |&gt; \n  select ( tweet_date, tweet_lowercase) |&gt; \n  filter (  str_detect ( tweet_lowercase, \"biden\"))\n\n\n  \n\n\n\nBut that leaves out any time he’s called “sleepy joe”. Instead, look for either “sleepy joe” or “joe biden”:\n\ntrump_tweets |&gt; \n  select ( tweet_lowercase) |&gt; \n  filter (  str_detect ( tweet_lowercase, \"(biden|sleepy joe)\")) \n\n\n  \n\n\n\nNow, use either the reactable() or the DT() library to make a table that will show you the entire text:\n\ntrump_tweets |&gt; \n  select (  tweet_lowercase) |&gt; \n  filter (  str_detect ( tweet_lowercase, \"(biden|sleepy joe)\")) |&gt;  \n  reactable( striped=TRUE, \n             searchable=TRUE) \n\n\n\n\n\n(When you have multi-word phrases, be sure to put the whole thing in parentheses. Otherwise, it doesn’t know how to interpret it: It could be (biden|sleepy) joe, which wouldn’t match much! )\n\n\nKeywords\nAnother way to look at these tweets is to look for signs of populism – words like “globalist” or “marxist”. Here’s one way to find a lot of them:\nThere are two or or three parts to a regular expression:\n\nThe pattern you are trying to find.\nAn optional replacement, which could use part of what you’ve found in the original seeking phase.\nOptions - the big one in R is negate=TRUE, which means to match everything EXCEPT what is found.\n\nIn practice, you’ll usually save the pieces of each pattern you’ve found into a variable, then put them back together differently.\nHere are two other good tutorials on regular expressions:\n\nFrom Justin Meyer at a recent IRE conference that can serve as a guide.\n\nFrom Prof. Christian McDonald of UT-Austin\nIf you’re using R, you can use a regex using the stringr package (part of the tidyverse) using the functions str_detect , str_extract and their cousins. They look like this:\n\n    str_detect(var_name, regex(\"pattern\")) \n\nThe vignette on the stringr package in the Tidyverse is reasonably helpful.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#types-of-regular-expression-patterns",
    "href": "advanced-regex-r.html#types-of-regular-expression-patterns",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.4 Types of regular expression patterns",
    "text": "23.4 Types of regular expression patterns\n\nLiteral strings\nThese are just letters, like “abc” or “Mary”. They are case-sensitive and no different than using text in a filter.\nYou can tell the regex that you want to find your pattern at the beginning or end of a line:\n  ^   = \"Find only at the beginning of a line\"\n  $   = \"Find only at the end of a line\"\n\n\nWild cards\nA wild card is a character you use to indicate the word “anything”. Here are some ways to use wild cards in regex:\n    .      = \"any single character of any type\"\n    .?     = \"a possible single character of any type (but it might not exist)\"\n    .*     = \"anything or nothing of any length\"\n    .+     = \"anything one or more times\"\n    .{1,3} = \"anything running between 1 and 3 characters long\"\nRegular expressions also have wild cards of specific types. In R, they are “escaped” using two backslashes. In other languages and in the example https://regex101.com site they only use one backslash:\n      \\\\d   = \"Any digit\"\n      \\\\w   = \"Any word character\"\n      \\\\s   = \"Any whitespace (tab, space, etc.)\"\n      \\\\b   = \"Any word boundary\" (period, comma, space, etc.)\nWhen you upper-case them, it’s the opposite:\n      \\\\D = \"Anything but a digit\"\n\n\nCharacter classes\nSometimes you want to tell the regex what characters it is allowed to accept. For example, say you don’t know whether there is an alternative spelling for a name – you can tell the regex to either ignore a character, or take one of several.\nIn R, we saw that there were alternative spellings for words like “summarize” – the British and the American spellings. You could, for example, use this pattern to pick up either spelling:\n        summari[sz]e\nThe bracket tells the regex that it’s allowed to take either one of those characters. You can also use ranges:\n  [a-zA-Z0-9]\nmeans that any lower case, upper case or numeric character is allowed.\n\n\nEscaping\nBecause they’re already being used for special purposes, some characters have to be “escaped” before you can search for them. Notably, they are parentheses (), periods, backslashes, dollar signs, question marks, dashes and carets.\nThis means that to find a period or question mark, you have to use the pattern\n    \\\\. or\n    \\\\?\nIn the Regex101, this is the biggest difference among the flavors of regex – Python generally requires the least amount of escaping.\n\n\nMatch groups\nUse parentheses within a pattern to pick out pieces, which you can then use over again. The end of this chapter shows how to do this in R, which is a little complicated because we haven’t done much with lists.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#sample-data-1",
    "href": "advanced-regex-r.html#sample-data-1",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.5 Sample data",
    "text": "23.5 Sample data\nHere are three small text files that you can copy and paste from your browser into the regex101.com site. It’s a site that lets you test out regular expressions, while explaining to you what’s happening with them.\n\nA list of phone numbers in different formats\nA list of dates that you need to convert into a different form.\nA list of addresses that are in multiple lines, and you need to pull out the pieces. (Courtesy of IRE)\nA small chunk of the H2B visa applications from Arizona companies or worksites that has been kind of messed up for this demonstration, in tab-delimited format.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#testing-and-examining-patterns-with-regex101",
    "href": "advanced-regex-r.html#testing-and-examining-patterns-with-regex101",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.6 Testing and examining patterns with Regex101",
    "text": "23.6 Testing and examining patterns with Regex101\nRegex 101 is a website that lets you copy part of your data into a box, then test different patterns to see how they get applied. Regular expressions are very difficult to write and to read, but Regex101 lets you do it a little piece at a time. Just remember that every time you use ‘' in regex101, you will need’\\` in R.\n\nLooking for specific words or characters\nThe easiest regex is one that has just the characters you’re looking for when you know that they are the right case. They’re called literals because you are literally looking for those letters or characters.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#practice-1-extract-date-parts",
    "href": "advanced-regex-r.html#practice-1-extract-date-parts",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.7 Practice #1: Extract date parts",
    "text": "23.7 Practice #1: Extract date parts\nIn Regex 101, change the “Flavor” to “Python” – otherwise, you have to escape more of the characters.2\nWe want to turn dates that look like this:\n  1/24/2018\ninto something that looks like this:\n 2008-1-24\nCopy and paste these numbers into the regex 101 window:\n9/7/2017\n9/11/1998\n9/11/2017\n9/19/2018\n9/15/2017\n10/13/2019\n11/3/2017\nFirst, you can use any digit using the pattern “. Try to do it in pieces. First, see if you can find one or two digits at the beginning of the line.\n  ^\\d{1,2}\nTry coming up with the rest of it on your own before you type in the answer:\n  ^\\d{1,2}.\\d{1,2}.\\d{4}\n(This works because regular expressions normally are “greedy”. That is, if you tell it “one or two digits”, it will always take two if they exist.)\nPut parentheses around any pieces that you want to use for later:\n\n\n\n\n\n\n\n\n\nNow each piece has its section, numbered 0 for the whole match, and then 1-3 for the pieces.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#practice-2-extract-pieces-of-phone-numbers",
    "href": "advanced-regex-r.html#practice-2-extract-pieces-of-phone-numbers",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.8 Practice #2: Extract pieces of phone numbers",
    "text": "23.8 Practice #2: Extract pieces of phone numbers\nHere are some phone numbers in different formats:\n    623-374-1167\n    760.352.5212\n    831-676-3833\n    (831)-676-3833\n    623-374-1167 ext 203\n    831-775-0370\n    (602)-955-0222  x20\n    928-627-8080\n    831-784-1453\nThis is a little more complicated than it looks, so try piecing together what this one says:\n      (\\d{3})[-.\\)]+(\\d{3})[-.]+(\\d{4})\n(This won’t work in the “substitute” area – it would be easier to create a new variable with the results than to replace the originals.)\nAnything within parentheses will be “captured” in a block.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#practice-3-extract-address-pieces",
    "href": "advanced-regex-r.html#practice-3-extract-address-pieces",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.9 Practice #3: Extract address pieces",
    "text": "23.9 Practice #3: Extract address pieces\nHere are a few lines of the data from Prof. McDonald’s tutorial, which you can copy and paste to go his exercise. (He uses the Javascript version of regular expressions, but for our purposes in this exercise, it doesn’t matter which one you use. If you choose Python, you’ll have one extra step, of putting a slash () before the quotes. The colors work a little better if you leave it on the default PHP method.)\n    \"10111 N LAMAR BLVD\n    AUSTIN, TX 78753\n    (30.370945933000485, -97.6925542359997)\"\n    \"3636 N FM 620 RD\n    AUSTIN, TX 78734\n    (30.377873241000486, -97.9523496219997)\"\n    \"9919 SERVICE AVE\n    AUSTIN, TX 78743\n    (30.205028616000448, -97.6625588019997)\"\n    \"10601 N LAMAR BLVD\n    AUSTIN, TX 78753\n    (30.37476574700048, -97.6903937089997)\"\n    \"801 E WILLIAM CANNON DR Unit 205\n    AUSTIN, TX 78745\n    (30.190914575000477, -97.77193838799968)\"\n    \"4408 LONG CHAMP DR\n    AUSTIN, TX 78746\n    (30.340981111000474, -97.7983147919997)\"\n    \"625 W BEN WHITE BLVD EB\n    AUSTIN, TX 78745\n    (30.206884239000487, -97.7956469989997)\"\n    \"3914 N LAMAR BLVD\n    AUSTIN, TX 78756\n    (30.307477098000447, -97.74169675199965)\"\n    \"15201 FALCON HEAD BLVD\n    BEE CAVE, TX 78738\n    (30.32068282700044, -97.96890311999965)\"\n    \"11905 FM 2244 RD Unit 100\n    BEE CAVE, TX 78738\n    (30.308363203000454, -97.92393357799966)\"\n    \"3801 JUNIPER TRCE\n    BEE CAVE, TX 78738\n    (30.308247975000484, -97.93511531999968)\"\n    \"12800 GALLERIA CIR Unit 101\n    BEE CAVE, TX 78738\n    (30.307996778000472, -97.94065088199966)\"\n    \"12400 W SH 71 Unit 510\n    BEE CAVE, TX 78733\n    (30.330682136000462, -97.86979886299969)\"\n    \"716 W 6TH ST\n    AUSTIN, TX 78701\n    (30.27019732500048, -97.75036306299967)\"\n    \"3003 BEE CAVES RD\n    ROLLINGWOOD, TX 78746\n    (30.271592738000436, -97.79583786499967)\"",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#on-your-own",
    "href": "advanced-regex-r.html#on-your-own",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "23.10 On your own",
    "text": "23.10 On your own\nThis is a small list of H2A visa applications, which are requests for agricultural and seasonal workers, from companies or worksites in Arizona. Try importing it into Excel, then copying some of the cells to practice your regular expression skills.\nYou might try:\n\nFinding all of the LLC’s in the list (limited liability companies) of names. (You should turn on the case-insensitive flag in Regex 101 or set that flag in your program if you do.)\nSee how far you can get in standardizing the addresses.\nSplit the city, state and zip code of the worksite.\nFind all of the jobs related to field crops such as lettuce or celery.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-regex-r.html#footnotes",
    "href": "advanced-regex-r.html#footnotes",
    "title": "23  Inexact matching and regular expressions in R",
    "section": "",
    "text": "Yes, we might care about the case, isolating places where Trump uses all-caps. But for now let’s deal with the substance of the tweets.↩︎\nEach language implements regular expressions slightly differently – when you begin to learn more languages, this will be one of the first things you’ll need to look up.↩︎",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Inexact matching and regular expressions in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html",
    "href": "advanced-scrape1.html",
    "title": "24  Scraping without programming",
    "section": "",
    "text": "24.1 Where reporters get data\nReporters can get data from people, using FOIA, asking nicely or by finding a whistleblower to leak it. But we often also get data from publicly published sources, usually on the web.\nThere are three ways to get data from the web:",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html#where-reporters-get-data",
    "href": "advanced-scrape1.html#where-reporters-get-data",
    "title": "24  Scraping without programming",
    "section": "",
    "text": "Download it, or use an API1. In these cases, the makers of the data have specifically offered it up for your use in a useful format. We’ll cover API’s later, but don’t forget to study the site for a download link or option. If there isn’t one on a government site, you might call the agency and ask that they add one. They might just do it. We’ve been using downloadable data throughout this book.\nFind it on your browser. Often the person making the website delivers structured data to your browser as a convenience. It’s easier for them to make interactive items on their page by using data they’ve already delivered in visualizations and tables. It also reduces the loads on their servers. These are usually in JSON format. You might be able to find it right on your computer. It’s a miracle!\nScrape it. This set of chapters goes over how to scrape content that is delivered in HTML form – a web page. There would be other methods to scrape PDFs, which can be easy or hard.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html#a-json-miracle-walkthrough",
    "href": "advanced-scrape1.html#a-json-miracle-walkthrough",
    "title": "24  Scraping without programming",
    "section": "24.2 A json miracle walkthrough",
    "text": "24.2 A json miracle walkthrough\nThis walkthrough shows you how to find some json in your browser. Use the Chrome browser for this - Firefox and Safari also have similar features, but they look different.\nHere is a simple page that will show you what the json looks like and how to extract it. This is what a human sees:\n\nThe table of presidents is actually produced using a small javascript program inside the HTML that walks through each item and lists it as a row.\n\nOpen the page in Chrome, then right-click anywhere on the page and choose “Inspect”. It may appear as just two items - the “head” and the “body”. But notice the little arrows - they show you that there is more content underneath. For now, we’ll ignore this, but it will be important later.\nChoose the Network tab, then re-load the page.\n\n\n\n\ninspect network\n\n\nThis shows you everything that the browser is attempting to load into your browser. (You may not see the “favicon” item. I have no idea why it’s showing up on mine - it’s not been requested!)\nYou can ignore most of this. Importantly, the “simple-page.html” is the actual page, and the “simple.json” is the data! Click on the simple.json row, then choose the “Response” tab:\n\n\n\nsimple json\n\n\nThat’s what json looks like - a list of rows within an item called “presidents”, each identified by the name of the column they’ll become.2\n\nRight-click on the simple.json file name, and you’ll see a lot of options. Choose the one that says Copy-&gt;Copy link address.\nGo to a new browser window and search for “json to csv”. This one is one that I often see first.\nPaste your copied link in the tab that says, “Enter URL” and press “Load URL”. You’ll see an option to copy the result as a csv file!\n\n\n\n\njson to csv\n\n\n\nA harder example\nThat was easy! But it’s also trivial. However, this method can often save you from having to page through results of a page. One example is the Maricopa County nightly list of mugshots, which may have several hundred new entries each day. Here’s what today’s looked like on a desktop browser (it looks different on a smaller screen).3:\n\n\n\nmcso list\n\n\nIt looks like you’d have to go through each of the five pages to get all of the names of people who were booked into jail that night, but often a json file contains all of them – they’re just showing you one page at a time.\n\nThis page won’t let you right-click to get the inspector. Instead, on a Mac, press Opt-Cmd-C to open the inspector window. (I think it’s Shft-Ctl-C on Windows, but I’m not sure.)\n\nThis looks like a mess! Don’t worry. Switch to the Network tab, and re-load the page. This time, there are dozens of different things that get loaded on your page, and none of them are obviously json. You have a few strategies to find it.\n\nPress the “Fetch/XHR” tab to see if it shows up there. Use the “Preview” tab to look at what each of them is, and, miracle of miracles, it’s the third one on the list! Even better, it has all 425 entries! (It looks like they’re split into groups of 100, but they really aren’t.)\n\n\n\n\nmiracle 1\n\n\n\nRight-click on the name of the file, and choose Copy-&gt;Copy link, and repeat the process above to convert it to a CSV file.\n\n\n\n\n\n\n\nNote\n\n\n\nIn 2022, the county began limiting the number of results to some random list of 300. It appears that searching for an inmate only checks those first 300 results. (It could have been a coincidence that there were exactly 300 inmate on Jan 3, 2023, but I doubt it.)\n\n\n\n\nAn even harder example\nThe New York Times maintains a map with the vaccination rates for various demographic groups by county on its website. At first, the Times didn’t provide a Github repo for the data. How can we extract the data from this map?\nThe easiest way would be to see if it contains a json miracle!\n\nOpen your inspector panel\nCopy and paste the link to the map page, and open it in your Chrome browser with the inpsectors showing.\nSwitch to the network tab. (If you opened the map before opening the inspector, reload it now. )\n\nYikes! The “Fetch /XHR” button doesn’t help us much here. There are too many different json files to check. We could look one by one and see if they’re right, but sometimes that’s just too hard. Instead,\n\nOpen the “Search” button on your inspector (it’s different from the Filter), and type in a county name (this one is “Maricopa”). You should see only a few of them. The most promising is the “doses_county.json”, so try that one first:\n\n\n\n\nnyt example\n\n\nThis time, it’s hard to find the item in the list of files in the browser. Instead, right-click in the “Preview” area, and copy the object. You can paste that into the box in the JSON to csv converter instead of entering a URL.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html#no-json-no-problem-maybe",
    "href": "advanced-scrape1.html#no-json-no-problem-maybe",
    "title": "24  Scraping without programming",
    "section": "24.3 No json? No problem (maybe)",
    "text": "24.3 No json? No problem (maybe)\nYou may not be able to find a json file – either it’s too hard to find, or it’s not useful, or it doesn’t exist. For a simple page, there’s no problem getting the data in Google Sheets. (This is one area where Excel lags behind Google Sheets.) We’ll go into HTML tags in more depth in the next chapter, but if your data is held in a table or structured list, you can import it directly into Google sheets.\nNote that this trick is really only useful if your page doesn’t change a lot, or if you just want a one-time snapshot. It doesn’t automatically update, and I don’t know how to capture changes – it would involve a Google scripting program, which I don’t know how to do. I’ve never learned because I usually only gather data for my own use, and it’s easier to program it than to finagle Google Sheets.\nThe trick to using Google Sheets is to find a table tag (&lt;table&gt;) or a list tag (&lt;ul&gt; or &lt;ol&gt;) that contains the data you want.\nHere’s an example, taken from a previous year’s MAIJ cohort: Reporters wanted to know whether Scottsdale was relatively unique in its city council structure, which has no districts. All members are at-large. Some research suggests that this disempowers non-white or less wealthy areas, because more privileged residents are often more active in local politics.\nThe reporters knew it was rare, but one question nagged at them: Was Scottsdale the largest city in the nation with a purely at-large council? That would make a nice tidbit for the story, but it wasn’t worth a major data collection endeavor.\nBallotpedia, a crowdsourced website with information on local governments, had collected a page of city council officials in the 100 largest cities in the US. Extracting this information into a structured table, then using regular expressions, could help make that a relatively simple job. This could even be done in Google Sheets, which also has a regular expression implementation. Because it’s so rare, just getting a list of cities that had no district or ward membership would give them a place to start looking up populations.\nThis information is stored in an HTML table, identified by the “\n” element.\nRight-click on the page, and open your inspector. It looks like a mess, but you can search for tables using a simple “Find” using Cmd (or Ctl) -F.\nYou may notice it says you can find by string, selector or XPath.\nIn the box, enter “&lt;table” (with the opening “&lt;”, but no closing one.). You should see “1 of 12” in the result box. As you go through the list, the currently selected table will be highlighted. When you hit “3 of 12”, you’ll notice that the browser has selected the table you want. That’s the information we need.\nOpen a Google Sheet, and copy the page URL to cell A1. This just makes it easier to construct the formula to extract the table.\nIn cell A3, enter the following formula:\n      \n      =ImportHTML(A1, \"table\", 3)That means, “Go to the web address listed in cell A1, look for”table” tags, then return whatever is in the third one.”\n\n\ngooglesheet\n\nWhen you hit “enter” the whole table will populate on your Google Sheet. Unfortunately, you can’t get the link to the city from this method, which means you don’t have a good way to extract a city name. We’ll come back to this when we go to scraping in R. (There is a way to get this in Google Sheets, but it’s not very reliable – it will choke as soon as it encounters a missing URL.)But if you just need the text of a table or list in a spreadsheet, this is an easy way to get it. (To get a list from an “ol” or “ul” (ordered and unordered lists) tag, use “list” instead of “table”.)\n24.4 Recap\nSometimes – especially on modern websites that create interactive elements on the fly – the data you need is already sitting on your computer. In fact, it’s quite hard to scrape those in other ways because the HTML is created when it’s loaded into your browser.\nBut when it’s not, there may be another simple way to get the content.\nThe problem is that getting the content without programming can leave you unsatisfied because you can only get the text, not any of the underlying information. The next chapter shows you one method of getting more information from a web page using CSS selectors.\nI sometimes use a Chrome extension called “Chrome Scraper” to get slightly more complex information out of a website, which uses a language called XPath to parse a web page. It’s harder than the CSS selector method, though, so I’m skipping it for now.\n\n\n\n\n\n“application programming interface”↩︎\nIn R, our style was to name columns in lower case with words separated by underscores. In Javascript, the custom is usually called “camel case”, with words smushed together and the first letter of each upper cased. It’s just a custom, not a rule.↩︎\nI’m hiding names of people to the extent possible, and won’t list them in text here - they’ll only be in the images. Instead, I’ll show pictures of how to find the json when a name is necessary. Although this book is probably not indexed by Google, it’s possible that it could be some day, and I don’t want their names to show up in a Google search.↩︎\n\n\n  \n      \n         23  Inexact matching and regular expressions in R\n                \n  \n  \n      \n        25  Introduction to scraping in R \n      \n  \n\n  \n    \nData reporting for investigative journalism, Spring 2024, written by Sarah Cohen\n   \n    \n       \n    \n    \nProduced with Quarto",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html#recap",
    "href": "advanced-scrape1.html#recap",
    "title": "24  Scraping without programming",
    "section": "24.4 Recap",
    "text": "24.4 Recap\nSometimes – especially on modern websites that create interactive elements on the fly – the data you need is already sitting on your computer. In fact, it’s quite hard to scrape those in other ways because the HTML is created when it’s loaded into your browser.\nBut when it’s not, there may be another simple way to get the content.\nThe problem is that getting the content without programming can leave you unsatisfied because you can only get the text, not any of the underlying information. The next chapter shows you one method of getting more information from a web page using CSS selectors.\nI sometimes use a Chrome extension called “Chrome Scraper” to get slightly more complex information out of a website, which uses a language called XPath to parse a web page. It’s harder than the CSS selector method, though, so I’m skipping it for now.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape1.html#footnotes",
    "href": "advanced-scrape1.html#footnotes",
    "title": "24  Scraping without programming",
    "section": "",
    "text": "“application programming interface”↩︎\nIn R, our style was to name columns in lower case with words separated by underscores. In Javascript, the custom is usually called “camel case”, with words smushed together and the first letter of each upper cased. It’s just a custom, not a rule.↩︎\nI’m hiding names of people to the extent possible, and won’t list them in text here - they’ll only be in the images. Instead, I’ll show pictures of how to find the json when a name is necessary. Although this book is probably not indexed by Google, it’s possible that it could be some day, and I don’t want their names to show up in a Google search.↩︎",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Scraping without programming</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html",
    "href": "advanced-scrape2.html",
    "title": "25  Introduction to scraping in R",
    "section": "",
    "text": "25.1 Understanding a web page and its structure\nWeb pages are written in HTML, even if they don’t have “html” at the end of the file name.\nHTML is like an upside-down tree. It has a trunk, which is an &lt;html&gt; tag, then two main branches: &lt;head&gt; and &lt;body&gt;. The content of the page branches out from the body tag:\nAll of HTML is just text. The tags tell your browser how to render each element, while attributes give them extra information, like the URL of a link, or a formatting class.\nWe can navigate the tree using RVest.\nThe library rvest splits up the tree into its distinct elements, retaining the structure of the tree. The read_html() function takes a chunk of text, a page saved on your computer, or a page on the internet and parses it into its pieces.\nWe’re going to parse the page using “css selectors”, which tells the program how to navigate the page. The css selector can use the tag , an attribute, or both to find elements on the page. In this case, there is only one table, so we can just find one element using the “table” tag.\nThis method of scraping doesn’t work if the page was created on the fly by executing a Javascript program on your browser, the way that the simple page in the last chapter did. Those pages usually have a json dataset that you can grab more easily.\nHere’s what the page looks like when rendered, with the full tree shown on the right.\nThis code parses the simple page at the address shown into its pieces, and save the result as my_htmlYou may notice that I’ve broken up the code to do one thing at a time. First, it saves the address in a variable called “url”. Then it uses the same piping we used in data work.1\n# label: readpres\n# echo: true\n# eval: false\n\nurl &lt;- \"https://cronkitedata.s3.amazonaws.com/docs/presidents.html\"\n\nmy_html &lt;-\n  url |&gt;  \n  read_html()\n\n\nprint(my_html)\n\n{html_document}\n&lt;html&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n   &lt;h1&gt;A heading&lt;/h1&gt;\\n   &lt;p class=\"intro\"&gt; This is  a paragraph  ...\nThis is a complex object, and you’re only seeing the beginning of it – the two elements that are at the top of the tree. You’ll notice that there is a new object that is a data type called a list rather than a data frame in you environment. Lists are used to store complicated structures that don’t fit neatly into rectangle.\nTo find any element, like the table, use its tag in an html_node() . To find all of the elements of a type, make it plural, like html_nodes().2\nUse the tag name or CSS selector to get just a piece of the page.\nmy_html |&gt;\n html_element(\"body\") \n\n{html_node}\n&lt;body&gt;\n[1] &lt;h1&gt;A heading&lt;/h1&gt;\n[2] &lt;p class=\"intro\"&gt; This is  a paragraph with italic&lt;/p&gt;\n[3] &lt;div&gt;\\n     &lt;p&gt; This is a simple table&lt;/p&gt;\\n     &lt;table&gt;\\n&lt;thead&gt;&lt;tr&gt;\\n&lt;t ...\nTo get all of the paragraphs, make the command plural. Note how you now get the HTML of the selected elements in their entirety.\nmy_html |&gt;\n  html_elements (\"p\")\n\n{xml_nodeset (2)}\n[1] &lt;p class=\"intro\"&gt; This is  a paragraph with italic&lt;/p&gt;\n[2] &lt;p&gt; This is a simple table&lt;/p&gt;\nAnd to get everything with a class of “intro”, use a period to indicate a class, and convert it to text using the html_text function, asking R to remove extra whitespace with the “trim” argument.\nmy_html |&gt;\n  html_elements (\".intro\") |&gt;\n  html_text (trim=T)\n\n[1] \"This is  a paragraph with italic\"",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html#understanding-a-web-page-and-its-structure",
    "href": "advanced-scrape2.html#understanding-a-web-page-and-its-structure",
    "title": "25  Introduction to scraping in R",
    "section": "",
    "text": "Datacamp html tree\n\n\n\n\n\n\nOpen or create a project in RStudio, and create a new document. You can do this as a new markdown document, or as an R Script.\nThe code to load two libraries:\n\n\n#| label: setup-scrape2\n#| message: false \n#| warning: false\n#| echo: true\n\nlibrary(tidyverse)\nlibrary(rvest) \n\n\nAnd run your setup chunk.\n\n\n\n\n\n\n\n\n\nfull page\n\n\n\n\n\n\n\n\n\n\n\n\n\nA special type: table\nTables are so commonly scraped that rvest has special way to extract the values, just as we did in Google Sheets, which puts it right into a data frame:\n\nmy_html |&gt;\n  html_element (\"table\")  |&gt;\n  html_table ()\n\n\n  \n\n\n\nThe singular version of html_element() picks out the first piece that matches the selector. The plural version would result in a list of all of them, from which you can select the number you want using the odd syntax .[[n]], where “n” is the table number.\n\n\nA harder example: Ballotpedia\nHere’s an example using the ballotpedia page we used in the last section:\n\nurl &lt;-\"https://ballotpedia.org/List_of_current_city_council_officials_of_the_top_100_cities_in_the_United_States\"\n\nballotpedia &lt;- \n  url |&gt;\n  read_html() |&gt;\n  html_elements(\"table\")\n\nprint (ballotpedia)\n\n{xml_nodeset (12)}\n [1] &lt;table class=\"infobox\" style=\"text-align: center; width:200px\"&gt;&lt;tbody&gt;\\n ...\n [2] &lt;table class=\"bptable sortable collapsible collapsed\" style=\"background: ...\n [3] &lt;table class=\"bptable gray sortable\" id=\"officeholder-table\" style=\"widt ...\n [4] &lt;table class=\"wikitable;\" style=\"width=100%\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th colspan= ...\n [5] &lt;table class=\"navbox\" cellspacing=\"0\" style=\";\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=\"pa ...\n [6] &lt;table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n [7] &lt;table class=\"navbox\" cellspacing=\"0\" style=\";\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=\"pa ...\n [8] &lt;table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n [9] &lt;table class=\"navbox\" cellspacing=\"0\" style=\";\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=\"pa ...\n[10] &lt;table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n[11] &lt;table class=\"navbox\" cellspacing=\"0\" style=\";\"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=\"pa ...\n[12] &lt;table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n\n\nLooking at this, we have several ways to get at the proper table. We can pick the third element that we just got:\n\nballotpedia[[3]]\n\n{html_node}\n&lt;table class=\"bptable gray sortable\" id=\"officeholder-table\" style=\"width:auto; border-bottom:1px solid #bcbcbc;\"&gt;\n[1] &lt;thead&gt;&lt;tr colspan=\"5\" style=\"background:#4c4c4c!important;color:#fff!imp ...\n[2] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td style=\"padding-left:10px;\"&gt;&lt;a href=\"https://ballotpedi ...\n\n\nor, you might notice that it has an “id” attribute called ‘officeholder-table’.\n\noffice_holders &lt;-\n  url |&gt;\n  read_html() |&gt;\n  html_node(\"#officeholder-table &gt; tbody\")\n\nIf you print it, you’ll see something like this:\n\n\n\noffice image\n\n\n\n\nGetting the link\nBefore, in Google Sheets, we had no way to pick up the list of links that would tell us what city each member was in. That’s also true when we use the html_table() function to turn it into a data frame:\n\noffice_table &lt;- \n  url |&gt;\n  read_html() |&gt;\n  html_node(\"#officeholder-table\") |&gt;  # keep the whole table to get headings \n  html_table()\n\nBut now we can get a list of the names of cities by extracting an “attribute” from the tag. (This is a little harder than I’d intended because not every row has a link, meaning we have to rejigger the formula to create empty rows when the link doesn’t exist.)\n\ncity_links &lt;-\n  ballotpedia[[3]] |&gt;  # the third table in our list\n  html_nodes (\"tbody &gt; tr\") |&gt;  # all rows\n  html_node (\"td &gt; a\") |&gt;  # justlink tag in the first column \n  html_attr(\"href\")  # the URL\n  \n\ntail(city_links)\n\n[1] \"https://ballotpedia.org/Barbara_Hanes_Burke\"         \n[2] \"https://ballotpedia.org/Jeff_MacIntosh\"              \n[3] \"https://ballotpedia.org/John_Larson_(North_Carolina)\"\n[4] \"https://ballotpedia.org/James_Taylor,_Jr.\"           \n[5] \"https://ballotpedia.org/Kevin_Mundy\"                 \n[6] \"https://ballotpedia.org/Robert_C._Clark\"             \n\n\nWe’ve never done this before, but we can add this list as a column to the data frame using the tidyverse add_column() function of the tidyverse . At the same time, you can put your regular expression muscles to work by “extracting” rather than “detecting” a pattern3:\n\noffice_table |&gt;\n  add_column (city=city_links, .before=\"Office\") |&gt;\n  mutate (city_extracted = str_extract(city, \"ballotpedia.org\\\\/(.*)$\", group=1) , .after=\"city\") |&gt;\n  sample_n(20)\n\n\n  \n\n\n\n(The str_extract () function matches everything after the last slash , because we put parentheses around the last part, which is a a capture group. This capability used to be very difficut in R – now it’s a bit easier. )\nIt doesn’t work perfectly, but it generally gets us a bit closer to a city name.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html#your-turn",
    "href": "advanced-scrape2.html#your-turn",
    "title": "25  Introduction to scraping in R",
    "section": "25.2 Your turn",
    "text": "25.2 Your turn\nA lot of people use IMDB pages as practice for scraping because its HTML is a little primitive. Try extracting the name, year, rating, and rank of each item in this list https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250\nHints:\n\nIf you right-click on the table, you’ll see the table has an attribute of class=\"chart full-width\" . That means you can use the class selector .chart &gt; table . If you use plural, it will be a list with one item in it. If you use singular, it will be the table itself.\nTo get three columns in a data frame of text, html_table(trim=T)\nThe year is held in a span element with a class of “secondaryInfo” in the first column. See if you can figure out how to get at it. 4\nTo extract title and its link, use the a tag\nTo get the full information from the rating column, including the number of votes that it’s based on, use the strong tag then the title attribute. See if you can figure that one out.\nTo put them all together, use the add_column() verb",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html#cheat-sheet",
    "href": "advanced-scrape2.html#cheat-sheet",
    "title": "25  Introduction to scraping in R",
    "section": "25.3 Cheat sheet",
    "text": "25.3 Cheat sheet\n\nHTML tags / elements\nEach tag has an opening and closing it. This is what a “p” open and close tag looks like, with the text inside it shown on the page:\n&lt;p&gt; This is a paragraph &lt;/p&gt;\n\nHere is a list of common tags you’ll use in scraping:\n\n\n\n\n\n\n\ntag\ndescription\n\n\n\n\np\nor a paragraph\n\n\ndiv\na block of text , or division of the page.\n\n\nspan\nAn area within a div or p element that is treated specially without it breaking into a new line.\n\n\na\na link. It should always have an attribute of href, which is the URL to the link.\n\n\nh1 through h6\nwhich are headline levels. “h1” is the headline, “h2” is a sub-head, and so on.\n\n\n\nModern websites might have these sections, which are used instead of the “div” tag:\n\n\n\ntag\ndescription\n\n\n\n\nnav\na navigational menu\n\n\nmain\nthe main block of the page with the content\n\n\naside\na sidebar\n\n\nfooter\nthe stuff at the bottom.\n\n\n\nTables are structured like this:\n\n\n\n\n\n\n\ntag\ndescription\n\n\n\n\ntable\nthe main container.\n\n\nthead and tbody\nA heading area and the body area. These are always just below the table element\n\n\nth\nthe row that contains the headings . These are the first row within the body\n\n\ntr\nall of the content rows. These are subsequent rows within tbody\n\n\ntd\nall of the cells (columns). These are always WITHIN a tr or th element.\n\n\n\nAny of these can be nested within any others. Typically, a page starts with an “h1” tag, then has “div” tags for different sections, such as the sidebar or the main content. An “a” tag is typically nested within others.\nStandalone tags:\nA few tags don’t have opening and closing versions - they just stand alone:\nimg - an image to show. It would have a src attribute for the link, and an alt attribute for text to show for accessibility. example: &lt;img src=\"path-to-my-image\" alt=\"This is a picture of...\"&gt;\nbr - A hard line break.\n\n\nCommon attributes for tags\n\n\n\n\n\n\n\ntag\ndescription\n\n\n\n\nhref\nthe URL of a link within an a tag.\n\n\nsrc\nthe path to an image, within an img tag.\n\n\nclass\na reference to a CSS class. More than one class can be identified, separated with a space.\n\n\nid\na unique name for this element using CSS\n\n\n\nPeople can also make up their own attributes - they’re arbitrary.\n\n\nRvest functions\n\n\n\n\n\n\n\nfunction\ndescription\n\n\n\n\nread_html()\nto parse the page. Start with a file name or URL.\n\n\nhtml_elements\nto get ALL elements that match your query. It always gives back a list of objects, even if it’s empty. To pluck one by number, use [[n]]. You might see it as html_nodes(), which is from an older version of the rvest library\n\n\nhtml_element\nthe FIRST element that matches your query. Always returns a single object. (You might see html_node())\n\n\nhtml_table()\nconvert a table to a data frame with just its text\n\n\nhtml_text(trim=T)\nget the text within an element.\n\n\nhtml_attr (attr_name)\nget the value of an attribute. Commonly, this is used as httr_attr(\"href\") to get the link inside an a link element.\n\n\nadd_column\nappend columns to the end of a dataframe from lists/vectors. They must be in the same order, and there have to be the same number of items as there are rows.\n\n\nadd_row\nto append rows at the bottom. These can be by name or position. There can’t be any columns in the row you want to add that aren’t in the one you’re adding to.\n\n\n\n(You’ll often see these operations as cbind and rbind - they’re similar. Our way is just the tidyverse way.)\n\n\nExamples of common CSS selectors\nThis uses an example assuming the tag “p” , class “myclass” and id “myid” are used. You substitute the tags, classes, and id’s you want. See https://www.scraperapi.com/blog/css-selectors-cheat-sheet/ for a more in-depth cheat sheet.\n\n\n\n\n\n\n\nselector\ndescription\n\n\n\n\np\na “p” element. Replace with the element you want to capture.\n\n\n.myclass\nany element with class=“myclass”\n\n\np .myclass\n“p” element with a class of “myclass”.\n\n\np &gt; .myclass\nevery child element of p with a class of ‘myclass’ regardless of the tag. Must be a direct child.\n\n\n#myid\nAny element with “id=‘myid’”\n\n\nbody &gt; div &gt; table .content-table &gt; tbody &gt; tr\nA row within a table classed “content-table” within a div.\n\n\n\nYou have to go through the whole path to an element if you need it, which is why you have to look in the inspector section of your browser or use the CSS Selector Gadget (a chrome extension that I’ve never been able to work properly!)",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html#going-further",
    "href": "advanced-scrape2.html#going-further",
    "title": "25  Introduction to scraping in R",
    "section": "25.4 Going further",
    "text": "25.4 Going further\nThe CSS selectors shown in this chapter are a little limiting – you’ll find times when the information you’re seeking isn’t defined using those selectors. That’s when the XPATH selectors we saw in the last chapter are used. It’s beyond the scope of this tutorial, but ask for some help or try to find XPATH examples if you can’t figure out how to get to a part of the page with your css selections – it’s pretty common for this to happen.",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "advanced-scrape2.html#footnotes",
    "href": "advanced-scrape2.html#footnotes",
    "title": "25  Introduction to scraping in R",
    "section": "",
    "text": "We glossed over this before, but any time you use a pipe, whatever comes above a command is used as the first argument of the current command. So this code is the same as read_html(url). Sometimes you need it as something other than the first argument, in which case you reference it using a period.↩︎\nA newer version of rvest prefers the use of html_element() instead of node. For us, they mean the same thing and both work. The newer syntax throws a warning in your RStudio environment that I can’t troubleshoot, so I’m waiting for an update to the rvest package to switch.↩︎\nstr_extract changed in December 2022. Make sure your packages are up to date.↩︎\n html_elements (\"tbody &gt; tr &gt; .titleColumn &gt; .secondaryInfo\") |&gt; html_text(trim=T)↩︎",
    "crumbs": [
      "Advanced topics",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Introduction to scraping in R</span>"
    ]
  },
  {
    "objectID": "vis.html",
    "href": "vis.html",
    "title": "Visualization",
    "section": "",
    "text": "In this section\nThe following chapters review ways of looking at your data while reporting. We’re leaving publication quality graphic alone – those often depend on using sites like Datawrapper or D3 in Javascript to get high-performance visualizations, while static graphics often depend on using Adobe products to make typography and palettes acceptable to your publication.\nThis chapter is done in R, mainly using the package ggplot2, which is part of the tidyverse. If you want to follow along with with interactive aspects, you’ll want to install the plotly package. There may be others noted in individual sections.",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "vis.html#package-installations",
    "href": "vis.html#package-installations",
    "title": "Visualization",
    "section": "Package installations",
    "text": "Package installations\nThere are some packages we’ll use that are not standard for the rest of the book. They are shown in the relevant chapters, but here is a full list:\n\nreacatable and DT for good-looking interactive reports\ngt and gtsummary for static tables\nsf and several others for geographic analysis\nleaflet and plotly for interactive graphics and maps.",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "vis.html#visualization-tools",
    "href": "vis.html#visualization-tools",
    "title": "Visualization",
    "section": "Visualization tools",
    "text": "Visualization tools\nYou can use a lot of different types of online and free applications to play around with graphics instead of R. They’re less reproducible, and they often come and go as free options, but they’re sometimes easier:\n\nDatawrapper.de , which is used in a lot of newsrooms. If you want to publish a visualization, it can link directly from R. It’s also easy to use on its own. If you want to eventually publish your visualizations, this is probably the one that is most compatible with newsrooms.\nFlourish (recently purchased by Canva, so who knows? But you can now create private visualizations without paying. )\nRAWGraphs, made for designers to sketch their work before digging deep into Adobe Illustrator. It’s a little hard to use unless your data is in just the right form.\nTableau Public, made mainly for business intelligence, but a free version is available. I haven’t used it for a while for a few reasons: Newsrooms don’t use it much because it doesn’t scale well to mobile; it’s hard to save drafts in the free version; it’s a little hard to get used to the interface for a quick visualization.\nThe underlying D3 language of graphics, which is written in Javascript, is what powers a lot of these products. It’s what the professional visual storyteller use. (Aside: javascript is probably the second language you’d want to learn. It helps with scraping and is used extensively newsrooms because it’s the language of the web and mobile, so there are people to help you. But it’s not easy.)",
    "crumbs": [
      "Visualization"
    ]
  },
  {
    "objectID": "vis-reporting.html",
    "href": "vis-reporting.html",
    "title": "26  Visualization as a reporting tool",
    "section": "",
    "text": "26.1 Examples",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Visualization as a reporting tool</span>"
    ]
  },
  {
    "objectID": "vis-reporting.html#examples",
    "href": "vis-reporting.html#examples",
    "title": "26  Visualization as a reporting tool",
    "section": "",
    "text": "Size and growth at once\nThis graphic was used for less than five minutes, but helped us see that there was one company that would be a good candidate for further reporting. The details are unimportant, but it showed that this company began quite small and grew quite quickly — the characteristic we were seeking. The others were not as strong candidates.\n\n\n\n\n\n\n\n\nReviewing two data points in a long list\n\n\n\nThis graphic shows the difference between the percent of businesses that got PPP loans in majority Latino zip codes compared to majority white non-Hispanic, based on four different measures. We used it at Reveal to identify places that might be the right place for a story – those where, across a series of measures, the difference was noticable, and the number of loans was large enough to warrant investigating. The team decided to center the story in Los Angeles, though it also reported on Dallas.\n\n\n\nText with graphics\nLooking at a table of numbers is mind-numbing. It’s hard to pick out what’s important. Some of the most imaginative visualizations come in the form of tables. In fact, there’s even a contest among R users to build the best tables. 1\nOne simple example is this analysis of federal prosecutions of gun charges over three administrations. We were curious: which statutes are the ones that Joe Biden is prioritizing compared with previous administrations? And was there a major downturn in prosecutions during the pandemic? These “sparklines” on the data table help show the trends, even when the numbers themselves are very different:\n\n\n\nNetworks of companies, people or programs\nThis is an example of a network analysis of Medicare charges among nephrologists – kidney doctors. We had heard that one consultant was showing doctors how to do procedures in their offices that were traditionally done in hospitals. The procedures had unusually high billing rates in doctors’ offices, but very low ones in hospital settings. These networks looked at the similarity across doctors’ practices by comparing the proportions of billing codes, then clustering them when they matched about 75 percent of the time. The area of red, large circles on the right turned out to be the same consulting practice. Sadly, the reporter who worked on this tip left the paper before we had a chance to publish.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Visualization as a reporting tool</span>"
    ]
  },
  {
    "objectID": "vis-reporting.html#tips-for-exploratory-visualizations",
    "href": "vis-reporting.html#tips-for-exploratory-visualizations",
    "title": "26  Visualization as a reporting tool",
    "section": "26.2 Tips for exploratory visualizations",
    "text": "26.2 Tips for exploratory visualizations\n\nUse small multiples to quickly orient yourself in a large dataset\n“Small multiples” are repeated images, usually over time or place. This example shows where each round of agricultural disaster payments went, which helped us decide on where to focus our reporting – Kansas and North Dakota.\n\n\n\nLook at your data from all directions\nWhen you’re trying to understand a story or a dataset, there’s no wrong way to look at it; try it every way you can think of, and you’ll get a different perspective. If you’re reporting on crime, you might look at one set of charts with change in violent crimes in a year; another might be the percent change; the other might be a comparison to other cities; and another might be a change over time. Use raw numbers, percentages and indexes.\nLook at them on different scales. Try following the rule that the x-axis must be zero. Then break that rule and see if you learn more. Try out logarithms and square roots for data with odd distributions.\nKeep in mind the research done on visual perception. William Cleveland’s experiments showed that the eye sees change in an image when the average slope is about 45 degrees. This suggests you ignore the admonitions to always start at zero and instead work toward the most insightful graphic. Other research in epidemiology has suggested you find a target level as a boundary for your chart. Each of these ways helps you see the data in different ways. When they’ve stopped telling you anything new, you know you’re done.\n\n\nDon’t assume\nNow that you’ve looked at your data a variety of ways, you’ve probably found records that don’t seem right – you may not understand what they meant in the first place, or there are some outliers that seem like they are typos, or there are trends that seem backwards.\nIf you want to publish anything based on your early exploration or in a published visualization, you have to resolve these questions and you can’t make assumptions. They’re either interesting stories or mistakes; interesting challenges to common wisdom or misunderstanding.\nIt’s not unusual for local governments to provide spreadsheets filled with errors, and it’s also easy to misunderstand government jargon in a dataset.\nFirst, walk back your own work. Have you read the documentation, its caveats and does the problem exist in the original version of the data? If everything on your end seems right, then it’s time to pick up the phone. You’re going to have to get it resolved if you plan to use it, so you might as well get started now.\nThat said, not every mistake is important. In campaign finance records, it’s common to have several hundred postal codes that don’t exist in a database of 100,000 records. As long as they’re not all in the same city or within a candidate, the occasional bad data record just doesn’t matter.\nAsk yourself: if I were to use this, would readers have a fundamentally accurate view of what the data says?\n\n\nAvoid obsessiong over precision and presentation details\nThe flip side of not asking enough questions is obsessing over precision before it matters. Your exploratory graphics should be generally correct, but don’t worry if you have various levels of rounding, if they don’t add up to exactly 100 percent or if you are missing one or two years’ data out of 20. This is part of the exploration process. You’ll still see the big trends and know what you have to collect before it’s time for publication.\nIn fact, you might consider taking away labeling and scale markers, much like the charts above, to even better get an overall sense of the data.\n\n\nCreate chronologies of cases and events\nAt the start of any complex story, begin building chronologies of key events and cases. These are easily done in a spreadsheet or a data collection system like Airtable. You will eventually need one.\n\n\nMeet with designers and graphics editors early and often\nBrainstorm about possible graphics with the artists and designers in your newsroom. They will have good ways to look at your data, suggestions of how it might work interactively, and know how to connect data and stories. It will make your reporting much easier if you know what you have to collect early on, or if you can alert your team that a graphic isn’t possible when you can’t collect it.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Visualization as a reporting tool</span>"
    ]
  },
  {
    "objectID": "vis-reporting.html#footnotes",
    "href": "vis-reporting.html#footnotes",
    "title": "26  Visualization as a reporting tool",
    "section": "",
    "text": "These are painstakingly built and are meant for publication, but you can see how much they can convey using color, size, shape and other elements within a table of numbers.↩︎",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Visualization as a reporting tool</span>"
    ]
  },
  {
    "objectID": "vis-tables.html",
    "href": "vis-tables.html",
    "title": "27  Report-making in R",
    "section": "",
    "text": "27.1 What we want from tables and reports",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Report-making in R</span>"
    ]
  },
  {
    "objectID": "vis-tables.html#what-we-want-from-tables-and-reports",
    "href": "vis-tables.html#what-we-want-from-tables-and-reports",
    "title": "27  Report-making in R",
    "section": "",
    "text": "Numbers with formatting, such as percent, dollar signs and commas.\nAutomatic wrapping of cells so that you don’t have to scroll to find columns.\nTotals and sub-totals shown on the same table as the detail.\nPrintable tables that summarize your data succinctly so that you can write from it, or….\nInteractive tables with filtering, searching and sorting so you can explore it.\n\n\nLibraries used\nBe sure to install the following packages before attempting to follow this chapter.\n\nreactable , for interactive tables that can incorporate small graphic elements.\nDT, for interactive tables with a few different features, including selectable columns.\ngt for static tables with formatting, labelxling and totals. (It has a cousin, gtsummary that we won’t be using that you might want to explore on your own.)\n\nIf you followed along int the first chapter setting up R and RStudio, you already have these.\n\n\n\n\n\n\nNumber formatting\n\n\n\nThere are a lot of ways to turn a big number into something readable in R. The problem is that most of these turn them into character, or text, columns. That’s ok if you don’t want to change the order in which the rows appear. But it wrecks the ability to sort (arrange) – “$4” is seen as larger than “$10” as text because 4 comes after 1 in the alphabet. The formatting options in the tables below are difficult to implement, but maintain sortability. The same thing happens with dates.\n\n\nMost of these packages will be demonstrated with a random sample of the PPP data we’ve worked with so far.\n\n```{r}\n#| label: setup\n#| warning: false\n#| message: false\n\nlibrary(\"tidyverse\")\nlibrary(\"lubridate\")\nlibrary(\"janitor\")\nlibrary(\"DT\")\nlibrary(\"reactable\")\nlibrary(\"gt\")\n```\n\nHere is a sample of the data from PPP – we have to use it because it has the column types that give us problems: Dollar values and dates.\n\nppp &lt;- readRDS ( \n               url ( \n                 \"https://cronkitedata.s3.amazonaws.com/rdata/ppp_sample.RDS\"\n                 )\n               ) \n\n\nppp_sample &lt;- \n  readRDS( url (\"https://cronkitedata.s3.amazonaws.com/rdata/ppp_sample.RDS\") )",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Report-making in R</span>"
    ]
  },
  {
    "objectID": "vis-tables.html#interactive-tables-with-reactable",
    "href": "vis-tables.html#interactive-tables-with-reactable",
    "title": "27  Report-making in R",
    "section": "27.2 Interactive tables with reactable",
    "text": "27.2 Interactive tables with reactable\n`reactable() is a version of the Javascript library used in many websites that you visit. It’s been adapted to R in the package you loaded above. It’s highly customizable, but it can involve a lot of typing to get a good table. The documentation is excellent and there are great examples in the wild.\nThe table in this document were made with reactable() in 2022.\n\nLet’s start by examining the data:\n\nglimpse (ppp)\n\nRows: 1,000\nColumns: 12\n$ date_approved      &lt;date&gt; 2021-05-22, 2021-03-20, 2020-04-27, 2021-04-08, 20…\n$ borrower_name      &lt;chr&gt; \"SANDRA GRAHAM\", \"SIBI LLC\", \"IDENTIFY YOUR SPACE L…\n$ borrower_address   &lt;chr&gt; \"2901 W Mariposa St\", \"1630 W Guadalupe Rd 104\", \"4…\n$ borrower_city      &lt;chr&gt; \"Phoenix\", \"Gilbert\", \"Chandler\", \"Tucson\", \"Page\",…\n$ borrower_state     &lt;chr&gt; \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ…\n$ borrower_zip       &lt;chr&gt; \"85017\", \"85233\", \"85225\", \"85705\", \"86040\", \"85281…\n$ loan_status        &lt;chr&gt; \"Paid in Full\", \"Paid in Full\", \"Paid in Full\", \"Pa…\n$ loan_status_date   &lt;date&gt; 2021-09-25, 2022-01-20, 2021-02-24, 2021-12-21, 20…\n$ amount             &lt;dbl&gt; 12395.00, 694100.00, 44722.00, 107500.00, 13181.00,…\n$ forgiveness_amount &lt;dbl&gt; 12417.07, 698740.01, 45026.36, 108109.66, 13296.33,…\n$ naics_code         &lt;chr&gt; \"611511\", \"518210\", \"541410\", \"711310\", \"561990\", \"…\n$ forgiven           &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Y…\n\n\nIf you want to show all of the rows and all of the columns, you just have to add reactable() to the end of your code chunk:\n\nppp |&gt; \n  reactable()\n\n\n\n\n\nThat’s ok, but it’s not a lot better than the original – the numbers are still kind of inscrutible, and we can’t do much with it.\n\nAdd filters and sorts\nHere’s a simple fix that will make every column sortable and searchable:\n\nppp |&gt; \n  reactable( sortable=TRUE, \n             filterable =  TRUE)\n\n\n\n\n\nBut the numbers are still inscrutible. We want to see them as dollar values. (I’m adding the “compact” and “defaultPageSize” arguments to save a little room here:\n\nppp |&gt; \n  reactable ( \n    compact = TRUE, \n    sortable = TRUE, \n    filterable= TRUE,\n    defaultPageSize = 5,\n    striped = TRUE,\n    defaultColDef = colDef ( format= colFormat (currency=\"USD\", separators = TRUE) )\n    )\n\n\n\n\n\nNotice that the dollar avmounts are shown as currency, but the still sort correctly! This is NOT true with most other solutions you see to this problem. The reason that the Zip code and the NAICS codes are NOT shown as dollars is because we defined them as text (or character) values – see it in the glimpse() above.\nYou might decide to stop here! It’s a much easier way to explore your data – and let your readers do the same – than to have to filter all the time. Just be aware that your browser and computer will only accept so many rows – don’t try it on something more than about 5,000 rows. And be sure to keep it paginated, or it will take forever to render.\nNotably, you can create a date column in a form you like to see, and it will still sort properly.\nSometimes, you want to treat specific columns differently, or change the names. There are two approaches to this: 1) Use a select verb first to rename and choose your columns in the order you want to see them, or 2) set each column separately. Here, we’ll do a little of both:\n\nppp |&gt; \n  select ( `Date` = date_approved, \n           `Name` = borrower_name, \n           `City` = borrower_city, \n           `Zip Code` = borrower_zip, \n           `Original Amount` = amount, \n           `Forgiven Amount` = forgiveness_amount, \n           `Was it forgiven?` = forgiven) |&gt; \n  # now just copy from above \n reactable ( \n    compact = TRUE, \n    sortable = TRUE, \n    filterable= TRUE,\n    defaultPageSize = 15,\n    striped = TRUE,\n    defaultColDef = colDef ( format= colFormat (currency=\"USD\", separators = TRUE) )\n    )\n\n\n\n\n\nThat works! Now we want to format the date column in the way we like to see it – in American style.\n\nppp |&gt; \n  select ( `Date` = date_approved, \n           `Name` = borrower_name, \n           `City` = borrower_city, \n           `Zip Code` = borrower_zip, \n           `Original Amount` = amount, \n           `Forgiven Amount` = forgiveness_amount, \n           `Was it forgiven?` = forgiven) |&gt; \n  # now just copy from above \n reactable ( \n    compact = TRUE, \n    sortable = TRUE, \n    filterable= TRUE,\n    defaultPageSize = 15,\n    striped = TRUE,\n    defaultColDef = colDef ( format= colFormat (currency=\"USD\", separators = TRUE) ), \n    columns = \n      list ( `Date` = colDef (format =  colFormat(date = TRUE))\n           )\n )\n\n\n\n\n\nThere are ways to further customize the date, but this will work using the default date format for your country. Again, unlike other methods of doing this, the dates sort correctly.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Report-making in R</span>"
    ]
  },
  {
    "objectID": "vis-tables.html#static-tables-with-gt",
    "href": "vis-tables.html#static-tables-with-gt",
    "title": "27  Report-making in R",
    "section": "27.3 Static tables with gt",
    "text": "27.3 Static tables with gt\nFor this example, we’ll use the Washington Post shootings data, which has something to count and create percentages for. This code reads the data, then creates a new column that breaks the original “armed” column into three possiblities – those who were armed with a gun (and nothing else), or “other or unknown”.\n\nshootings &lt;- readRDS(url(\"https://cronkitedata.s3.amazonaws.com/rdata/waposhootings.RDS\")) |&gt;\n  mutate ( is_armed = case_when ( armed == \"gun\" ~ \"Gun\",\n                                  armed == \"unarmed\" ~ \"Unarmed\", \n                                  .default = \"Other or unknown\") \n  )\n\nTake a look at it:\n\nglimpse(shootings)\n\nRows: 5,945\nColumns: 6\n$ name      &lt;chr&gt; \"Tim Elliot\", \"Lewis Lee Lembke\", \"John Paul Quintero\", \"Mat…\n$ year      &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, …\n$ armed     &lt;chr&gt; \"gun\", \"gun\", \"unarmed\", \"toy weapon\", \"nail gun\", \"gun\", \"g…\n$ ethnicity &lt;chr&gt; \"Asian\", \"White, non-Hispanic\", \"Hispanic\", \"White, non-Hisp…\n$ flee      &lt;chr&gt; \"Not fleeing\", \"Not fleeing\", \"Not fleeing\", \"Not fleeing\", …\n$ is_armed  &lt;chr&gt; \"Gun\", \"Gun\", \"Unarmed\", \"Other or unknown\", \"Other or unkno…\n\n\nUnlike reactable(), gt() is used when you have a very small table that you just want people to be able to read, with percentages and numbers properly formatted. Let’s try it using the number and percentage of armed victims. Let’s start with a simple table. (In this case, let’s not multiply the percentage by 100 – gt will do that later. )\n\nshootings |&gt; \n  count ( is_armed , name=\"Shootings\") |&gt; \n  mutate ( pct = Shootings / sum (Shootings))  |&gt; \n  gt()\n\n\n\n\n\n\n\nis_armed\nShootings\npct\n\n\n\n\nGun\n3406\n0.57291842\n\n\nOther or unknown\n2136\n0.35929352\n\n\nUnarmed\n403\n0.06778806\n\n\n\n\n\n\n\n\nshootings |&gt; \n  count ( is_armed , name=\"Shootings\") |&gt; \n  mutate ( pct = Shootings / sum (Shootings))  |&gt; \n  gt( ) |&gt; \n    cols_label ( is_armed = \"Armed?\", \n                 pct = \"Percent\")\n\n\n\n\n\n\n\nArmed?\nShootings\nPercent\n\n\n\n\nGun\n3406\n0.57291842\n\n\nOther or unknown\n2136\n0.35929352\n\n\nUnarmed\n403\n0.06778806\n\n\n\n\n\n\n\nNow we can format the “shootings” column:\n\nshootings |&gt; \n  count ( is_armed , name=\"Shootings\") |&gt; \n  mutate ( pct = Shootings / sum (Shootings))  |&gt; \n  gt() |&gt; \n    cols_label ( is_armed = \"Armed?\", \n                 pct = \"Percent\") |&gt; \n    fmt_number ( columns = c( Shootings), \n                 decimals = 0) \n\n\n\n\n\n\n\nArmed?\nShootings\nPercent\n\n\n\n\nGun\n3,406\n0.57291842\n\n\nOther or unknown\n2,136\n0.35929352\n\n\nUnarmed\n403\n0.06778806\n\n\n\n\n\n\n\nThat’s better!\nlet’s format the percentages:\n\nshootings |&gt; \n  count ( is_armed , name=\"Shootings\") |&gt; \n  mutate ( pct = Shootings / sum (Shootings))  |&gt; \n  gt() |&gt; \n    cols_label ( is_armed = \"Armed?\", \n                 pct = \"Percent\") |&gt; \n    fmt_number ( columns = c( Shootings), \n                 decimals = 0)  |&gt; \n    fmt_percent ( columns = c(pct), \n                  decimals = 1)\n\n\n\n\n\n\n\nArmed?\nShootings\nPercent\n\n\n\n\nGun\n3,406\n57.3%\n\n\nOther or unknown\n2,136\n35.9%\n\n\nUnarmed\n403\n6.8%\n\n\n\n\n\n\n\nMuch better. Now let’s add a total at the bottom:\n\nshootings |&gt; \n  count ( is_armed , name=\"Shootings\") |&gt; \n  mutate ( pct = Shootings / sum (Shootings))  |&gt; \n  gt() |&gt; \n    grand_summary_rows (\n      columns = c (Shootings, pct) ,\n      fns = list ( Total = \"sum\") \n    )  |&gt; \n    cols_label ( is_armed = \"Armed?\", \n                 pct = \"Percent\") |&gt; \n    fmt_number ( columns = c( Shootings), \n                 decimals = 0)  |&gt; \n    fmt_percent ( columns = c(pct), \n                  decimals = 1) \n\n\n\n\n\n\n\n\nArmed?\nShootings\nPercent\n\n\n\n\n\nGun\n3,406\n57.3%\n\n\n\nOther or unknown\n2,136\n35.9%\n\n\n\nUnarmed\n403\n6.8%\n\n\nsum\n—\n5945\n1\n\n\n\n\n\n\n\nFormatting the summary rows has to be done separately, which is a pain. It’s only really useful when you want to send it to publication.\n\nPivoting and summarizing\nThe GT package is really useful when you want to present the results of one of your analyses that has percentages that you’ve pivoted. Here’s an example: Our question is:\n\nAre Black victims more or less likely to have been armed than White victims?\n\n\nCompute the percentage of of “armed” answers within each ethnicity. This is a shortcut way to do that, with n being the default name of the count() answer.\n\n\nshootings |&gt; \n  filter ( ethnicity %in% c(\"White, non-Hispanic\", \"Black, non-Hispanic\"))  |&gt; \n  count ( is_armed, ethnicity) |&gt; \n  group_by ( ethnicity ) |&gt; \n  mutate  ( pct = n / sum (n)) \n\n\n  \n\n\n\n\nPivot it to show the percentages within the two races:\n\n\nshootings |&gt; \n  filter ( ethnicity %in% c(\"White, non-Hispanic\", \"Black, non-Hispanic\"))  |&gt; \n  count ( is_armed, ethnicity) |&gt; \n  group_by ( ethnicity ) |&gt; \n  mutate  ( pct = n / sum (n)) |&gt; \n  pivot_wider ( names_from = ethnicity,  \n                values_from = pct, \n                id_cols = is_armed )\n\n\n  \n\n\n\nNow create a formatted table:\n\nshootings |&gt; \n  filter ( ethnicity %in% c(\"White, non-Hispanic\", \"Black, non-Hispanic\"))  |&gt; \n  count ( is_armed, ethnicity) |&gt; \n  group_by ( ethnicity ) |&gt; \n  mutate  ( pct = n / sum (n)) |&gt; \n  pivot_wider ( names_from = ethnicity,  \n                values_from = pct, \n                id_cols = is_armed ) |&gt; \n  gt ( ) |&gt; \n   cols_label ( is_armed = \"Armed?\", \n                `Black, non-Hispanic` = \"Black\", \n                `White, non-Hispanic` = \"White\") |&gt; \n   fmt_percent ( \n                 decimals = 0)  \n\n\n\n\n\n\n\nArmed?\nBlack\nWhite\n\n\n\n\nGun\n60%\n59%\n\n\nOther or unknown\n31%\n35%\n\n\nUnarmed\n9%\n6%",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Report-making in R</span>"
    ]
  },
  {
    "objectID": "vis-maps-begin.html",
    "href": "vis-maps-begin.html",
    "title": "28  Mapping: An introduction",
    "section": "",
    "text": "28.1 The power of maps\nThe most famous use of mapping may be the John Snow map, made in 1854 during a cholera outbreak in London. Officials didn’t know the cause, but Dr. Snow mapped the deaths and quickly saw the common element: The Broad Street water pump.\nIn data journalism, we use maps to see patterns that would be impossible any other way. Sometimes, they are used to write paragraphs rather than display data.\nIn 2015, we did a story about the tycoons funding the upcoming presidential election. Mapping their locations let us write this paragraph:\nMapping seems magical and inconceivable until you do it. But it does require a little bit of vocabulary and a few basic concepts before you can accurately and confidently work with maps.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Mapping: An introduction</span>"
    ]
  },
  {
    "objectID": "vis-maps-begin.html#the-power-of-maps",
    "href": "vis-maps-begin.html#the-power-of-maps",
    "title": "28  Mapping: An introduction",
    "section": "",
    "text": "cholera map\n\n\n\n\n\nNearly all the neighborhoods where they live would fit within the city limits of New Orleans. But minorities make up less than one-fifth of those neighborhoods’ collective population, and virtually no one is Black. Their residents make four and a half times the salary of the average American, and are twice as likely to be college educated.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Mapping: An introduction</span>"
    ]
  },
  {
    "objectID": "vis-maps-begin.html#mapping-faq",
    "href": "vis-maps-begin.html#mapping-faq",
    "title": "28  Mapping: An introduction",
    "section": "28.2 Mapping FAQ",
    "text": "28.2 Mapping FAQ\n\nWhat is GIS?\nGraphical Information Systems are programs and apps built to work with data that have a spatial, or geographic, element to them. In them, data is meant to be worked with in maps or on a spatial plane or globe rather than in columns and rows. Examples of GIS systems are ESRI’s ArcMap, which is used primarily by researchers and government agencies who create the underlying maps we’ll use in journalism. They are relatively hard on a computer, and require more memory and disk space than we’ve used.\nA popular free and open source GIS system is QGIS, which will work on either Windows or Mac machines. There are excellent tutorials available for QGIS in journalism, including a Knight Center course (but the handout links are dead), and Alexandra Kanik’s prerequisite tutorials for a master class in mapping done for NICAR in 2022.\n\n\nWhat kinds of maps do we use?\n\n“Reference” maps are simply features on a screen or on paper. Think of Google Maps as reference maps. Cartographers take great care in making reference maps readable and useful, from the shape of each line or point to the color of kind of area like a mountain range. Font selection and line types are other elements that cartographers care about, so that it is possible to use the map for navigation or to picture a location.\n“Thematic maps” are the kind that we usually make in journalism: They are designed to illustrate data such as wealth, topography, or crime.\n\nGenerally, your project will begin by finding the right reference map for your work, then layer on top of it thematic elements from your data. Reference maps can come from many places, such as the Census TIGER program, Google Maps ($$$), OpenStreetMap, MapBox, Leaflet or other providers. You don’t have to make them yourself. Turn them into thematic maps by joining data to them or layering data on top.\n\n\nWhat are longitude and latitude?\nThese are the coordinates that define a spot on the globe. In GIS applications, you will usually get these as “decimal degrees” rather than the degree, minute, second form that you learned in junior high.\n\n\n\nSource: mapschool.io\n\n\nUsually, for local data, you will want the numbers to have at least three numbers after the decimal point. However, agencies often mask the actual locations in data they provide by lopping off decimals. Most coding services provide “rooftop” level positions, which have six numbers after the decimal point. For example, longitude (or X) -73.990593 and latitude (Y) 40.740121 define the position of the front of White House.\nIn the United States, longitude (X) is always negative (except Guam), because it’s in the Western Hemisphere. Latitude (Y) is always positive, because there is no place in the United States in the Southern Hemisphere.\n\n\nWhat are features?\nEach layer on your map displays a “feature”, such as roads, schools, or Census tracts. At its simplest, a feature layer is one of the following three types:\n\n\n\nSource: mapschool.io\n\n\n\nPoints: The simplest of the types, a point is a place, defined by its longitude and latitude, or its XY position. You can get that information from the data you’re requesting or obtaining from the government, or you can try to figure it out yourself using a process called “geocoding”. We’ll come back to that.\nLines are strings of points smushed together — they don’t have to be straight, but they do not have the concept of an interior or exterior. You can find points along a line, or distances to a line. While there are many line features in reference maps, I have hardly ever used them in my work. People who work with traffic or water flows, however, use them a lot. You can use them to determine patterns on either side of a border, or a distance to a road.\nPolygons: These lines smushed together to form an enclosed area, such as a state, a Census tract or a Zip Code. They often touch one another, and you can always find out which points fall within each polygon using a “spatial join” – one of the most common tasks that we do. You can turn polygons into points by finding their center point, or centroid.\n\n\n\nRaster data and imagery : For simplification, each of the above types are considered “vector” data – they are defined by positions by connecting a whole bunch of points in space. But raster data, like satellite imagery, are just pixels of different colors. You can technically look at them in any program, but when they are imported to GIS systems, they become “georeferenced”, in that their edges are referenced to position on the globe. For now, we are not going to look at raster data.\n\n\n\n\nSource: mapschool.io\n\n\n“Attributes” are the data elements attached to features – columns in a database! Thematic maps use attributes such as income, population, or temperature to determine the size, color or shape of features on a maps.\n\n\nWhere do I get map data?\nUnlike other parts of the government, sharing map data is very common in agencies. They are so hard to make, and so useful, that most cities and states have GIS departments whose job it is to organize and document the map data.\nMap data comes in various forms – usually a bundle of files called “shapefiles”, which were originally created by ESRI. More modern systems often have “geojson” files, which are easier to use. Google Earth uses “KML” or “KMZ” data, which is also easy to use. JSON files are less efficient than shapefiles, so they are often much larger.\n\nLocal and state governments: In Arizona, the AZGEO hub (accessible with a free user account) coordinates the map data for many agencies of the state. At ASU, the library has a GIS lab with access to much of the base layer information needed, such as county voting districts. They also have contacts in local government who can help you find the underlying map data you need.\nFederal government: At the federal level, the US Geological Service can sometimes serve as a clearinghouse. The Weather Service, USDA and other agencies often have GIS data for free. The National Map also has many layers that can be used.\nThe Census Bureau has mapped every address and block in the US, and creates TIGER files for counties, voting districts, school districts and other political boundaries.\n\nFor very detailed maps, such as the footprints of property parcels in a county , you may have to buy the information. If you get it directly from the county, it shouldn’t be a lot – maybe $100 – but if you buy it privately, it can cost thousands of dollars.\n\nGetting map data from ESRI / ArcMap\nMany government agencies have turned their GIS applications over to the private company, ESRI, and make them public through that system. It’s often quite confusing to download data from ESRI, but it is often possible. If you don’t see a way to use the data yourself, then call the agency responsible for it and ask – you will usually be able to get them to send it to you or give you instructions on where to find it. We’ll look at an example of getting data from an ESRI map in the next chapter.\n\n\n\nWhat if my data doesn’t have longitude and latitude?\nThe process of turning addresses into points on a map is called “geocoding”, and it is one of the most expensive and time-consuming processes in data journalism. The reason is that there are so many ways to represent an address, and that addresses change all the time. It’s not unusual to have 95 percent of your addresses map perfectly, but then you have to look at the remaining 5 percent to see whether there are consistent errors. When I used to do a lot of mapping in Washington, DC, there were three types of addresses that routinely failed – anything on North or South Capitol Street; anything on Martin Luther King, Jr. Avenue, and anything on Nannie Helen Burroughs Avenue. That meant that any map I made, if I didn’t deal with these problems, would be misleading because they would be missing those locations. Geocoding New York City addresses in Queens can also be difficult.\nWe’re not the only ones who struggle with geocoding. In Los Angeles, reporters once found that the crime data supplied to the public was wrong – every time its geocoding failed, it put the crime at 1st and Spring Streets, which was the center of the city map. (Data used inside the police department was correct, because it came from a different source – the GPS data sent by police when they made a stop.)\nThis means that whenever you ask the government for data, you should ALWAYS ask for geographic coordinates and ask if they have GIS files (often using shapefiles). They may not give them to you, but it will save days of work and be much more accurate than trying to put them on a map yourself.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Mapping: An introduction</span>"
    ]
  },
  {
    "objectID": "vis-maps-begin.html#a-note-on-projections",
    "href": "vis-maps-begin.html#a-note-on-projections",
    "title": "28  Mapping: An introduction",
    "section": "28.3 A note on projections",
    "text": "28.3 A note on projections\n\nProjections are what we call the mathematical equations that do the trick of turning the world into some flat shape that fits on a printout or a computer screen. It’s a messy task to do, this transformation - there’s no way to smoosh the world onto a screen without distorting it in some way. You either lose direction, or relative size, or come out with something very weird looking.\n-- mapschool.io\n\nThis clip from The West Wing shows how projections can change the way you see the world.\n\nYou can play around with how the standard map distorts size at https://thetruesize.com\nBecause every map has a defined projection, you can’t do anything with one without the maker of the map telling you how it’s been flattened.\n\nGeographic coordinate systems\nGeorgraphic coordinate systems are often used for point features, because they describe posiitons on a globe. They are also often used for reference, or base, maps because they’re easy to use and transform.\nWGS84 with the code ESPG:4326, for World Geodetic System (1984 version) is the standard coordinate system for international data and GPS systems.\nNAD83 or EPSG:4269, is the standard used by the Census Bureau for North American data.\n\n\nProjected coordinate systems\nWhere geographic coordinate systems are measured in degrees, projected coordinate systems use units of measure that we understand – usually feet or meters. They are used more frequently to define direction, distance or shape. Where you would need a lot of geometry to compute the distance between two points on a globe, the distance between two points in a projected system are just the Pythagorean theorem you used in 6th grade: x2 + y2 = distance2\nIf a map is projected, it will usually have the information built into the data file. This is one of the files that makes up the bundle for shapefiles, and the other types have the projection built in. R and other mapping systems will usually recognize it.\nHowever, state and local agencies often leave this part out. In those cases, look for a standard projection used by the state. For Maricopa County, the standard is North American Datum 1983, Arizona State Plane, Central Zone, HARN, in feet (code 2868). For US data, many people use the U.S. national atlas equal area projection, or crs 9311 (especially if you’re only showing the lower 48) or ESRI:102009.\nThere’s a lot of complicated math that goes into flattening the earth into two dimensions for viewing on a screen or a piece of paper. Each of these projections has a number of characteristics, so instead of listing all of them whenever you want to use them, they have codes. You can look up codes using https://spatialreference.org/.\nNow that you have some working knowledge of GIS, we can move onto making maps.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Mapping: An introduction</span>"
    ]
  },
  {
    "objectID": "vis-maps-firstmap.html",
    "href": "vis-maps-firstmap.html",
    "title": "29  The first map",
    "section": "",
    "text": "29.1 Using tmap vs. ggplot and leaflet\nThere are several approaches to getting maps in R. In general, the heavy lifting is done the same way – through the sf (simple features) library. But displaying the maps can be done many different ways.\nThe tmap library does a little more automatically than using the visualization library ggplot to make maps, so that’s the approach this tutorial will take. Neither is better - tmap is a little easier and requires a little less typing, but is not quite as easy to marry with other Javascript interactive maps or to customize using ggplot.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The first map</span>"
    ]
  },
  {
    "objectID": "vis-maps-firstmap.html#setup",
    "href": "vis-maps-firstmap.html#setup",
    "title": "29  The first map",
    "section": "29.2 Setup",
    "text": "29.2 Setup\nYou’ll want to change your typical setup chunk to this when you start using maps:\n\n```{r}\n#| label: setup\n\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(sf)\nlibrary(tmap)\nlibrary(gt)\n\noptions(tigris_class=\"sf\",\n        tigris_use_cache = TRUE)\n```\n\nThe options make it easier to work with the Census bureau’s maps.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The first map</span>"
    ]
  },
  {
    "objectID": "vis-maps-firstmap.html#the-first-project",
    "href": "vis-maps-firstmap.html#the-first-project",
    "title": "29  The first map",
    "section": "29.3 The first project",
    "text": "29.3 The first project\nOur first project involves getting points on a map of Maricopa County, then displaying it based on Census demographics. Here, we’ll use voting locations as the points and Census tract level data for income.\n\nPoints\nWe have a list of voting locations that were geocoded by the Arizona Republic last year, courtesy of Caitlin McGlade. The locations aren’t exact – some of the digits were chopped off, and some weren’t found precisely. But they are generally in the right area.\nImport that file using the usual read_csv() method.\n\nlocations &lt;- read_csv(\"https://cronkitedata.s3.amazonaws.com/rdata/maricopa_voting_places.csv\")\n\nglimpse(locations)\n\nRows: 223\nColumns: 7\n$ site_name &lt;chr&gt; \"ASU DOWNTOWN CAMPUS\", \"ASU WEST CAMPUS\", \"AVONDALE CITY HAL…\n$ address   &lt;chr&gt; \"522 N CENTRAL AVE\", \"4701 W THUNDERBIRD RD\", \"11465 W CIVIC…\n$ city      &lt;chr&gt; \"PHOENIX\", \"GLENDALE\", \"AVONDALE\", \"BUCKEYE\", \"QUEEN CREEK\",…\n$ state     &lt;chr&gt; \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", \"AZ\", …\n$ zip       &lt;dbl&gt; 85004, 85306, 85323, 85326, 85142, 85335, 85255, 85234, 8525…\n$ longitude &lt;dbl&gt; -112.0740, -112.1599, -112.3031, -112.5839, -111.6361, -112.…\n$ latitude  &lt;dbl&gt; 33.45377, 33.60733, 33.44517, 33.37083, 33.25029, 33.57322, …\n\n\nThe Republic used a geocoding service that provides the data in the geographic reference WGS84, which is code 4326. There are names, addresses, and coordinates in this dataset.\nHere is how to turn the simple data into a geographic dataset:\n\nlocations_map &lt;- \n    locations |&gt; \n    st_as_sf ( \n               coords = c(\"longitude\", \"latitude\"), \n               crs = 4326)\n\nNow take a look at its structure, using the “str” rather than “glimpse” command, which provides a little more detail:\n\nstr(locations_map)\n\nsf [223 × 6] (S3: sf/tbl_df/tbl/data.frame)\n $ site_name: chr [1:223] \"ASU DOWNTOWN CAMPUS\" \"ASU WEST CAMPUS\" \"AVONDALE CITY HALL\" \"BUCKEYE CITY HALL\" ...\n $ address  : chr [1:223] \"522 N CENTRAL AVE\" \"4701 W THUNDERBIRD RD\" \"11465 W CIVIC CENTER DR 200\" \"530 E MONROE AVE\" ...\n $ city     : chr [1:223] \"PHOENIX\" \"GLENDALE\" \"AVONDALE\" \"BUCKEYE\" ...\n $ state    : chr [1:223] \"AZ\" \"AZ\" \"AZ\" \"AZ\" ...\n $ zip      : num [1:223] 85004 85306 85323 85326 85142 ...\n $ geometry :sfc_POINT of length 223; first list element:  'XY' num [1:2] -112.1 33.5\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:5] \"site_name\" \"address\" \"city\" \"state\" ...\n\n\nThis is hard to read, but take a careful look: Its type of data includes “sf”, along with data frame and table. The longitude and latitude columns are gone, replaced by a column called geometry.\nWe can check to see what projection R thinks it is in using the st_crs() function:\n\nst_crs( locations_map)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\nUsing the tmap library to display the map\n\ntm_shape ( locations_map) +\n  tm_dots ( )\n\n\n\n\n\n\n\n\nFor a static map, there are a lot of extra packages that have to be installed and there are some dependencies that we don’t want to deal with. Instead, we’ll just view it as a dynamic map:\n\ntmap_mode(mode=\"view\")\ntm_shape ( locations_map) +\n  tm_dots ( )\n\n\n\n\n# turn it back to plot\ntmap_mode(mode=\"plot\")\n\n\n\nAdding census tracts\nThe tigris package gets us access to all of the geographic data from the Census Bureau. Among them are Census tracts.\n\nmaricopa_geo &lt;- \n  tigris::tracts(state = \"04\", county=\"013\", year=\"2020\", \n                 cb=TRUE)\n\nTake a look at the projection that this file is in, since it ought to match when putting two maps together:\n\nst_crs(maricopa_geo)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nThis is a different coordinate system.\nIt’s ok, since we really want them both to be in a system designed for this part of the world.\nWe will present our map in the Arizona State Plane projection, 2868:\n\ntm_shape ( maricopa_geo, projection=2868) +\n  tm_polygons () +\ntm_shape ( locations_map, projection = 2868) +\n  tm_dots ( )\n\n\n\n\n\n\n\n\n\n\nAdding Census data\nI downloaded some data from the 2018-2021 American Community Survey. You can load it into your environment using this code chunk:\n\nmaricopa_demo &lt;- readRDS(\n                       url (\"https://cronkitedata.s3.amazonaws.com/rdata/maricopa_tract.Rds\")\n)\n\nNow, just like any other data, we can join these two datasets.\n\nmaricopa_demo_map &lt;- \n  maricopa_geo |&gt; \n  left_join ( maricopa_demo, join_by ( GEOID==geoid)) |&gt; \n  select (GEOID, tot_pop:pct_minority )\n  \n\nglimpse (maricopa_demo_map)\n\nRows: 1,009\nColumns: 6\n$ GEOID         &lt;chr&gt; \"04013422102\", \"04013422308\", \"04013103304\", \"0401342070…\n$ tot_pop       &lt;dbl&gt; 4558, 5491, 4164, 5022, 3441, 6215, 4353, 3800, 3990, 58…\n$ nonhisp_white &lt;dbl&gt; 1523, 3237, 1152, 4123, 2540, 3730, 3983, 2371, 3039, 55…\n$ median_inc    &lt;dbl&gt; 34486, 93581, 41853, 110208, 96406, 60598, 141107, 65033…\n$ pct_minority  &lt;dbl&gt; 67, 41, 72, 18, 26, 40, 8, 38, 24, 91, 17, 48, 17, 16, 5…\n$ geometry      &lt;POLYGON [°]&gt; POLYGON ((-111.8746 33.4049..., POLYGON ((-111.8…\n\n\n(You don’t need to select the “geometry” column – it has to be there if it’s a map.)",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The first map</span>"
    ]
  },
  {
    "objectID": "vis-maps-firstmap.html#a-chloropleth-map",
    "href": "vis-maps-firstmap.html#a-chloropleth-map",
    "title": "29  The first map",
    "section": "29.4 A chloropleth map",
    "text": "29.4 A chloropleth map\nNow we can use colors to show the percent of each tract that identified as a racial or ethnic minority in the American Community Surveys.\n\ntm_shape ( maricopa_demo_map, \n           projection = 2868) +\n  tm_polygons ( col = \"pct_minority\", \n                style = \"jenks\", \n                palette = \"YlGnBu\", \n                border.col = NULL)\n\n\n\n\n\n\n\n\nThere are a lot of options for drawing the maps, but these are the most common. Using “jenks” natural breaks to find the groupings for colors. It tries to find a place where big changes in color won’t happen for minor differences, but it sometimes needs to be tweaked.\n\nColor palettes\nYou may notice I changed the color palette so that the states with a lot of water are in blue and those with little are in yellow! Here are the common palettes used in R, which come from the Color Brewer: color advice for cartography:\n\nRColorBrewer::display.brewer.all()",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The first map</span>"
    ]
  },
  {
    "objectID": "vis-maps-firstmap.html#adding-the-voting-places-and-making-it-interactive",
    "href": "vis-maps-firstmap.html#adding-the-voting-places-and-making-it-interactive",
    "title": "29  The first map",
    "section": "29.5 Adding the voting places and making it interactive",
    "text": "29.5 Adding the voting places and making it interactive\n\ntmap_mode(\"view\")\n\ntm_shape ( maricopa_demo_map, \n           projection = 2868) +\n  tm_polygons ( col = \"pct_minority\", \n                style = \"jenks\", \n                palette = \"YlGnBu\", \n                border.col = NULL, \n                alpha = .3) +\ntm_shape ( locations_map) +\n  tm_dots (col = \"red\")\n\n\n\n\n\nYou have now made an interactive map of the Census Tracts and voter locations in Maricopa County, from start to finish! It’s not good enough to publish, but it’s good enough for you to explore on your own.\nThis method of making maps doesn’t have very many options, but it’s the simplest way to get started.\nYou can also use the ggplot library, with additional options added with other packages, to create a publishable static or interactive map.",
    "crumbs": [
      "Visualization",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>The first map</span>"
    ]
  },
  {
    "objectID": "appendix-math.html",
    "href": "appendix-math.html",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "",
    "text": "A.1 The PERS: Fractions, rates, percents and per capita\nYou can usually simplify your story if you can re-jigger your numbers into a rate, a ratio or a percentage. “One out of four” is a fraction, or a rate. “Forty percent” is another ratio or rate. And 235 deaths per 100,000 people is another.\nPercents and fractions are used to scale of very large or very small numbers while putting them into perspective.\nRates are also used to level the playing field – they compare two items that have a different base.\nWhen you see a lot of numbers in copy, examine them to see if a simple rate – “one of four” or 25 percent – would simplify your story.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-math.html#the-pers-fractions-rates-percents-and-per-capita",
    "href": "appendix-math.html#the-pers-fractions-rates-percents-and-per-capita",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "",
    "text": "Fractions and percents\nRepeat this: “Percents are fractions. Fractions are percents.” Remembering this all the time will keep you focused on the key element of percentages: They’re ratios, or rates, expressed as a fraction of 100.\n\nFiguring a percent:\nStep 1: Know your base. Think of the words “out of.” It’s the total of all the groups.\nStep 2: Divide the category you care about by the base.\nRemember that a fraction sign (/) means “divided by” (÷).\nStep 3: Move the decimal point two places to the right (or multiply by 100) to get the rate per hundred, or percent.\nStep 4: Round the answer to no more than one decimal place. Better\nyet, look for an easier fraction your readers will understand.\n\nFormula\nStep 1: Total = The base\nStep 2: (Category / Total) = Proportion\nStep 3: Proportion x 100 = Percent\nStep 4: Round and simplify.\n\nExample\nIf 58 people say they will vote in an upcoming election and 92 say they won’t, this is how to compute the percent of people who claim they will vote:\nStep 1: Base = number of people asked = 92 + 58 = 150\nStep 2: Rate = 58 out of 150 = 58/150 = .386666..\nStep 3: Percent = .38666… x 100 = 38.666666….\nStep 4: Round and simplify: = nearly 40 percent\n\n\n\nFrom fractions to percents and back\n\n\n\nYou probably know that 1 out of 4 is one-quarter, and that it’s also 25 percent. But you may not know how to get from one to another.\nFrom fractions to percents:\n1/4 = 1 ÷ 4 = 0.25. Move the decimal place over two places, or multiply by 100, to get 25%\nFrom percents to fractions:\n\nWrite your percent as a fraction: 25/100\nTry to find a “least common denominator:” 25 in this case goes into both the top and the bottom. You might want to round off either number to come out to a simple denominator.\nSimplify: (25 / 25) / (100 / 25) = 1 / 4\n\nTo get “One out of “ numbers:\n\nExpress your percentage as a proportion by dividing by 100, so 25% is 0.25.\nNow divide one by that number: 1 / .25 = 4, so your answer is one-fourth.\n\nTip for spreadsheet users: Excel allows you to format a number as a fraction or a percent. Play around with formats to see how the number is most easily described.\n\n\n\n\n\nRates and per capita\nAs with percentages, per person or per capita rates are used to level the playing field.\nThey’re often used when you need to compare two dissimilar places or events: Crimes in cities with different populations, deaths from various diseases or Gross Domestic Product across countries.\nRates also are often used with very big or very small numbers to change them into something we can understand.\nSometimes, though, a rate makes things more complicated, especially when events are rare and there is a consensus that they shouldn’t ever happen. Some examples include the 32 crashes attributed to GM’s faulty ignition switch, or the 64 deaths that the Centers for Disease Control associated with pharmacy compounding errors in 2012.\nOne rule of thumb is to use raw numbers when they are under 100, and revert to some kind of fraction or rate when they grow bigger.\n\nRates for large numbers\nA raw per-person figure is an average and should usually be used with very big numbers.\nA Gross Domestic Product of $17 trillion is hard to digest. So we reduce it to a number we can understand. If we divide it by 317 million, we get about $54,000 for every man, woman and child in the country. It doesn’t mean that each person earned $54,000 – in fact, almost half of all families earned less than that altogether at this writing. Instead, it includes all of the income that is generated by companies as well as people.\nBut the device turns an incomprehensible number into something we can picture. It also helps if we want to compare countries – it levels the playing field by adjusting for the size of the country.\n\n\nRates for small numbers - crime, death, and other rare events\nRates such as 23 per 1,000 people or something like it – are the same as percentages, but you multiply by something bigger than 100 or move the decimal place further to the right. Use these for very small numbers.\nIf 2.5 million people die in this country every year, then the percentage of people who die is a really small number: 0.789 per 100, or percent.\nA number that little is hard to digest. So experts up the ante and express the figure as 789 deaths per 100,000 people.\n\n\n\n\n\n\nSmall numbers warning\n\n\n\nBe careful about rates based on very small numbers. One example is the number of police shootings per 100,000 people. Most police departments in the country are very small and are more likely to serve only about 5,000 people. This means that just one shooting in the department can lift them from one of the lowest rates in the nation to one of the highest. Expect rates based on very small numbers to be unstable and potentially misleading.1\n\n\n\nFiguring a rate\nStep 1: Choose your base. This is often difficult. In reporting on fatalities by make of car, should you use the number of cars on the road, the number sold, or the total miles driven each year? You’ll have to decide.\nStep 2: Divide the number you care about by the base. Choosing the numerator can also be tricky. Going back to the automobile fatality example, would you use the total number of deaths or the number of driver deaths? Take a hint using other reports you see on the topic. Experts have often come to an informal agreement about what the most telling number is.\nStep 3: Multiply by a nice round number, such as 1,000, 100,000 or 1 million.\nStep 4: Round the answer and simplify.\n\nFormula\nStep 1: Choose the base, or “total”\nStep 2: (Category / Total) = Proportion or Rate\nStep 3: Proportion x 1,000 = Rate per thousand\nStep 4: Round to zero decimal places\n\nExample\nAccording to the FBI Crime in the United States for 2012, there were 13,000 violent and property crimes in Pittsburgh out of a population of 312,000. There were 8,870 crimes in Tucson out of a population of 531,000. Figuring a rate per thousand residents lets you compare the two cities:\n\n\n\n\n\n\n\nPittsburgh\nTucson\n\n\n\n\nStep 1: Base= 312,000 people\nStep 1: Base = 531,000 people\n\n\nStep 2: 13,000 crimes / 312,000 people = 0.041\nStep 2: 8,870 / 531,000 people = 0.017\n\n\nStep 3: 0.041 x 1,000 = 41 crimes per thousand\nStep 3: .017 * 1,000 = 17 crimes per thousand\n\n\n\nSo the crime rate for Pittsburgh was nearly 2 1/2 times that of Tucson that year , or 47/17 = 2.4\n\n\n\nSelecting your multiplier\n\n\n\nSome people feel that changing their multiplier from 100 to something bigger is cheating.\nAfter all, a 0.2 percent rate becomes a big number – 200 – when you change the base from 100 to 100,000!\nIn practice, though, there’s nothing magical about using a base of 100 (or percent). Instead, use the number that makes sense for the comparison you’re making.\n\nChoose a round number – 1,000, 1 million or 100,000.\nChoose the same number that the experts use: Crimes per 1,000 people, deaths per 100,000, or crashes per million miles driven, for example.\nChoose a base that will give you an easy way to express it to your readers. This is one that results in a number generally between 1 and 1,000 or so.\nTry to avoid using an outrageously large base. For instance, avoid expressing a local number in terms of 1 million people. Only a handful of cities have more than a million people.\nKeep the same base throughout your story. Don’t shift from 100,000 to 1,000 in crime statistics, for instance, when you move from murders to total crime rates.\n\nYou will often have to balance these rules of thumb against each other to come up with a compromise that allows you to write gracefully while keeping the sense of scale appropriate for the comparisons you’re making.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-math.html#measuring-change",
    "href": "appendix-math.html#measuring-change",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "A.2 Measuring change",
    "text": "A.2 Measuring change\nWe often write about change or difference, usually as a difference between place or time.\n\nSimple differences\nA simple difference is just the result of subtracting one number from another. If you are measuring differences in time, it’s the newer number minus the older number.\nOne time to use a simple difference is when the number is understandable without any calculations. Prices of common household goods, salaries and home prices are examples of numbers that needn’t always be put into perspective using percentage changes.\nIn the end, we work in news. That means that sometimes you’ll use a raw number when it’s more newsworthy. This doesn’t necessarily mean the number is more alarming – just more meaningful.\n\nFiguring a difference:\nSubtract the older number from the newer number.\nThis is not the same as subtracting the little number from the big number.\nIf a number has fallen you get a negative number. If a number has risen you get a positive number.\nFormula\nNew – Old.\nExample An executive made $2.4 million last year. She made $2.9 million this year.\nHer raise was: $2.9 – $2.4 = 0.5 million, or $500,000, or half a million dollars.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-math.html#percent-change-percent-difference",
    "href": "appendix-math.html#percent-change-percent-difference",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "A.3 Percent change / Percent difference",
    "text": "A.3 Percent change / Percent difference\nThe most butchered form of newsroom math is the percent difference, or the percent change.\nPart of the problem is that some folks have found five or six different ways to compute them. Unfortunately, only two of them work every time. I’ll show you both because sometimes – especially when you want to compare rates – one is easier than the other.\nNote that these methods work whether or not the number is going up or going down. If the number has fallen, you’ll get a negative answer. If the number has risen, you’ll get a positive one. And it still comes out right if the increase is bigger than 100 percent.2\nIn practice, I use Method 1 when I’m working in spreadsheets because I can look at the simple difference in one column and then use it in the formula for the percentage difference. I use Method 2 when I want to compare to percent changes to one another or when working with annual rates.\n\n\n\n\n Method 1: Subtract then divide\n\n\n\n Method 2: Divide then subtract\n\n\n\n\n\n\n\nFiguring a percent change\n\n\n\n\n Step 1: Get the simple difference between the numbers by subtracting the older number from the newer number. It doesn’t matter which one is bigger!\nStep 2: Divide the answer by the older number.\nStep 3: Multiply by 100, or move the decimal point two places to the right.\nStep 4: Round off and simplify.\n\n\n\n Step 1: Get the proportion of the new number compared to the old number. This is the same as the percent of total above, except the old number is the base.\nStep 2: Subtract 1 from that ratio\nStep 3: Multiply by 100, or move the decimal point two places to the right.\nStep 4: Round off and simplify.\n\n\n\n\n\n\n\n\n\nFormula\n\n\n\n\n Step 1: New – Old = Difference\nStep 2: Difference / Old = Decimal answer\nStep 3: Decimal x 100 = percentage difference\nStep 4: Round off.\n\n\n\n Step 1: New / Old = Ratio\nStep 2: Ratio - 1 = Decimal answer\nStep 3: Decimal x 100 = percentage difference\nStep 4: Round off.\n\n\n\n\n\n\n\n\nExample\n\nAn executive made $2.4 million last year. They made $2.9 million in this year.\n\n\nStep 1: Difference = 2.9 – 2.4 = 0.5\nStep 2: Difference / Original number =  0.5 / 2.4 = .208\nStep 3: Move the decimal point = 20.8%\nStep 4: Round off and simplify:  21% = 21 / 100 = about 20 / 100 = or about one-fifth.\n\n\n\nStep 1: Ratio = 2.9/2.4 = 1.208\nStep 2: Decimal answer = 1.208 - 1 = .208\nStep 3: Move the decimal point = 20.8%\nStep 4: Round off and simplify:  21% = 21 / 100 = about 20 / 100 = or about one-fifth.\n\n\nSo the executive got a raise equivalent to one-fifth of their original salary.\n\n\n\n\n\n\n\n\nReversing or predicting a percent change\n\n\n\nRemember that a number can grow many times, but it can only fall 100 percent to zero. This is a rough concept until you think it through. If you double a price of $20, increasing it by 100%, it’s $40. If you triple it, it’s $60. But if you reduce the $40 back to $20, it’s a 50 percent drop, to one-half the level, not a 100 percent decrease. In other words, percent changes can’t be reversed.\nThis means that the ads claiming you’ll use three times less detergent or a food contains three times less salt are wrong and impossible. What they probably mean is that it would be three times as much if you used the other brand or ate the other food, or the brand is one third as much. Here are two ways this makes a difference:\nYou need two of three numbers to reverse or predict a percent change:\n\nWhere the number started\nWhere it ended\nWhat the percent change would (or will) be\n\nAny two of those will give you what you need. It’s easiest if we use Method 2 above to get there. The example above assumes you know where it started and where it ended. Here’s how to do it if you you have either of the other two:\nWhere it starts and the percent change\nExample: You started with $100 and it grew by a total of 12%. Or, you started with $100 and it fell by a total of 12% (the percent change was -12%)\nStep 1: Convert the percent change to a ratio by moving the decimal place back : .12 (up) or -.12 (down)\nStep 2: Add 1, resulting in 1.12 (up) or .88 (down)\nStep 3: Multiply the beginning number by that amount : $100 x 1.12 = $112 (up), or $100 x .88 = $88 (down)\nWhere it ends and the total percent change\nFor example, say your house is worth $330,000, and it had appreciated by a total of 15% over the past few years. Here’s how to figure out where it started:\nStep 1: Convert the percent change to a decimal, as above: 0.15\nStep 2: Divide the current value by that amount = $330 / .15 = $287\nThis isn’t intuitive, but it differs because you’re starting from a bigger base. 15 percent of 330 isn’t the same as 15 percent of 287.\n\n\n\n\n\n\n\nGoing further with percents and rates\nThere are three common problems in changes and rates you will probably encounter that aren’t part of this guide. You should get help or look it up when these situations come up: 3\n\nRelative risk: That’s the technical term for dividing two percentages. If the mortgage denial rate for Black homeowners was 10 percent, and the denial rate of white homeowners was 5 percent, it means that Black homeowners are twice as likely to be denied a loan. This can be used with both rates and with changes.\nAnnual rates: When you know that something has grown, say, 2 percent a year for 10 years, it’s not the same thing as 20 percent. You have to annualize it.\nAdjusting for inflation: Comparing values across two points in time – especially today – means putting them on the same footing. Generally, you want to convert old values to their buying power today. For example, it’s hard to compare salaries for teachers today with those 50 years ago, because our money isn’t worth as much today.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-math.html#average-and-typical-values",
    "href": "appendix-math.html#average-and-typical-values",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "A.4 Average and typical values",
    "text": "A.4 Average and typical values\nAverages4 are just summaries. If a quote sums up an event, or an anecdote sums up a person using their actions instead of words, an average sums up a human condition of some kind – money, congestion, death or disease – in a single number.\nChoosing your average carefully or deciding there may be another number or method to sum up a situation can mean the difference between accurately and inaccurately describing your story.\nUnderstanding different kinds of “measures of central tendency” – what they tell us and what they don’t – is the first thing you learn in basic statistics classes. If an it doesn’t describe your data well, it’s not very productive to move forward into many other kinds of analysis.\nTrying to compare populations over time is particularly tricky using averages because of giant demographic shifts. Between the Baby Boom and the Millennials came what some people call the Baby Bust. Getting average spending on education, for example, across these generations is really misleading – it will boom, then bust, them boom again and no one number will describe that pattern.5\nTwo types of averages are reviewed here. Consult an introductory statistics book if your work depends on an average.\n\nThe average or mean\nA “mean” is what people mean when they say the word “average”.\nIt’s most descriptive when it summarizes numbers that don’t vary too much at either the top or bottom ends. These averages will often be misleading when they refer to items measured in dollar amount like incomes, housing costs and the like.\n\nFiguring a simple average or mean\nStep 1: Add up a list of numbers.\nStep 2: Divide the answer by the number of numbers you’ve added up.\nFormula\nStep 1: Sum of numbers\nStep 2: Sum / Count of numbers\nFor spreadsheet users: =AVERAGE(list of numbers)\nExample\nHere are six home prices on a block:\n$275,000          $1,200,000\n$275,000            $500,000\n$200,000            $395,000\n\nStep 1: 275 + 275 + 200 + 1,200 + 500 + 395 = 2,845 or $2,845,000\nStep 2: $2,845,000 / 5 = $569,000.\nSo the average home price is more than all but one on the list.\n\n\n\nThe median\nMedians are often used to summarize the value of things measured in dollars, especially home prices and incomes. They are not sensitive to one or two unusually high or low values the way the average in the previous example is.\nBut it’s harder to get a median because you need a list of all values. For example, if you know the total income of a metropolitan area and the number of people in that area, you can compute the average – or per capita income – but not the median.\nOne way to express the median is to call it the “typical” value. Another way is to say that it’s the “middle” value.\n\nFiguring a median:\nStep 1: List all of your numbers in order, beginning with the lowest and ending with the highest.\nStep 2: Count how many numbers you have and divide by two.\nStep 3: Add 0.5. If that comes out to a whole number (like 13), count up the list that many values.\nIf it’s not (like 12.5), take the average of the two numbers surrounding the number. 6\nIn other words, this is the closest you can get to the middle of the list. This is a sorting and counting job, not a calculator job.\nIn a spreadsheet, use the =MEDIAN() function.\nExample:\nStep 1:\nThe same list, but listed from lowest to highest, with an extra expensive home\n1.    $200,000\n2.    $275,000      \n3.    $275,000      \n4.    $395,000\n5.    $500,000\n6.  $1,200,000\nStep 2: 6/2 = 3\nStep 3: 3 + .5 = 3.5\nStep 4: Average the 3rd and 4th items on the list: (275 + 395) / 2 = $335,000\n\nAs a rule of thumb, the median will be more telling than the average when they’re very different as in this example. But the word “median” sounds very technical to some readers and the average encompasses all of the values in a list, so we use it when they’re not too different.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-math.html#footnotes",
    "href": "appendix-math.html#footnotes",
    "title": "Appendix A — Newsroom numbers cheat sheet",
    "section": "",
    "text": "Note the disclaimer in this story on police shootings by The Washington Post, in which changes in the rates of police shootings may just be random.↩︎\nIt’s impossible for a number to fall more than 100 percent. That would mean it went below zero and then no formula works. There’s no good way to show a percent change when a figure like annual company earnings goes from profit to loss.↩︎\nThey are part of the “Numbers in the Newsroom” book from which this guide is derived.↩︎\nI’m using the term “average” freely here. Technically, a simple average and median are measures of central tendency, but I’ll treat them as different types of averages for simplicity sake.↩︎\nThis is sort of an example of “Simpson’s paradox” in that an average hides meaningful trends among sub-populations.↩︎\nIn statistical programs like R, there are various ways to specify how to deal with medians when there are ties like this. This is the most common way, but it may not be the way your program handles it.↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Newsroom numbers cheat sheet</span>"
    ]
  },
  {
    "objectID": "appendix-filetypes.html",
    "href": "appendix-filetypes.html",
    "title": "Appendix B — File types in the wild",
    "section": "",
    "text": "Getting data into R (or any other program) is often among the more difficult parts of your project. This postscript goes over the common data formats you’re likely to run across in your work with R. However, R is not limited to these. You’ll often find arcane and specialized data file formats if you work with statisticians or experts in geographic analysis that can also be read in R. There is almost always a package available that will read it.\n\nTabular text data\nEvery computer can read and write plain text. Those are the characters you can type on a typewriter, with no fancy formatting or other features that require special software to ingest it. Normally, you’ll work with text data that already comes as columns and rows. They usually come in two flavors:\n\nCSV data is “comma-separated values” data, which means that a new column will be created whenever a comma is encountered. If there is a chance that there might be a comma inside a column, it will be enclosed by quote marks. This usually works OK, but there are some times when you have to be careful because there could be commas AND quotes inside a column. (A good example is a column of peoples’ names – they may be something like Smith, Johnny \"The Rat\") To overcome this, some people use:\nTSV , or tab-separated data. In this case, the tab key determines the distinction between columns, which is much rarer to find in plain text files.\n\nHere’s what a CSV might look like listing the last few presidents:\n  name, position, start_date, age_at_start_date\n  \"Biden, Joe\", President, 2021-01-20, 78\n  \"Trump, Donald\", President, 2017-01-20, 70\n  \"Obama, Barack\", President, 2009-01-20, 47\nIt looks like a mess to you, but it’s a thing of beauty to a computer.\nSome government agencies just make up a delimiter instead of a comma or tab - I’ve seen them with vertical bars (|) and tildes ~.\n\n\nNon-tabular text data\nAnother common format you’ll see passed around from computer to computer is called JSON. This stands for Javascript Object Notation, and is commonly used to pass data over the web , often to your phone or your browser.\nIt looks even worse, but it’s also a thing of beauty to a computer. The same data would look like this in JSON:\n {\"presidents\": [\n    {\"name\": \"Biden, Joe\", \"position\": \"President\", \"start_date\": \"2021-01-20\", \n       \"age_at start_date\": \"78\"},\n    {\"name\": \"Trump, Donald\", \"position\": \"President\", \"start_date\": \"2017-01-20\", \n       \"age_at start_date\": \"70\"},\n    {\"name\": \"Obama, Barack\", \"position\": \"President\", \"start_date\": \"2009-01-20\", \n       \"age_at start_date\": \"47\"}\n   ]\n  } \n    \n\n\nEvil data - PDF\nData supplied in a PDF file isn’t data at all – it’s effectively pixels placed on a page, and is intended for printing and viewing, not analyzing. Government agencies often print Excel files into PDF’s – I have no idea why, but it’s common, and it’s often difficult to convince them to do anything else regardless of the local public records law. For now, just remember that this is one file format you want to avoid if at all possible – it’s the most error-prone and difficult data to manage, even if it looks pretty.\n\n\nProprietary data formats\nExcel is one proprietary data format; Google Sheets is another. You may run into many different kinds, ranging from maps to statistical systems like SAS. R has two of its own propriety formats: .RData and .Rda, which we’ll use later. R has a package that will read almost any data – once you find it, it shouldn’t be a problem.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>File types in the wild</span>"
    ]
  },
  {
    "objectID": "appendix-ppp.html",
    "href": "appendix-ppp.html",
    "title": "Appendix C — Documentation for PPP data chapters",
    "section": "",
    "text": "C.1 Sources\nAll data used in this book was downloaded as of December 2022 from the SBA data site at https://data.sba.gov/dataset/ppp-foia , last updated in October.\nThe data dictionary is distributed at that same site at https://data.sba.gov/dataset/ppp-foia/resource/aab8e9f9-36d1-42e1-b3ba-e59c79f1d7f0 in an Excel spreadsheet.\nHere is some other background information on the program: https://www.sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program\nThe data was downloaded using a program to concatenate all of the files into one, large data frame. Only minimal standardization and cleaning was done:\nPPP loans have two geographic locations included: The location of the borrower, and the city, county, state and congressional district of the project that is being funded. This is especially important for construction and similar trades that have work done on sites.\nUnder the rules, most businesses had to apply for forgiveness within about a year of getting the loans. There were two choices: Apply for forgiveness, or begin paying it back after about 10 months. As of October 2022, 98 percent of the loans given in 2020 had been forgiven, and about 85 percent of the 2021 loans had been forgiven. In SBA parlance, this means they have been “remitted” or “disbursed” to the original lending institution. The Wikipedia page on the program provides a pretty detailed discussion of the details.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Documentation for PPP data chapters</span>"
    ]
  },
  {
    "objectID": "appendix-ppp.html#sources",
    "href": "appendix-ppp.html#sources",
    "title": "Appendix C — Documentation for PPP data chapters",
    "section": "",
    "text": "All recipient names were converted to upper case and punctuation were removed.\nAll city names are in proper case. (Casa Grande, not CASA GRANDE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Documentation for PPP data chapters</span>"
    ]
  },
  {
    "objectID": "appendix-ppp.html#data-used-in-the-tutorials",
    "href": "appendix-ppp.html#data-used-in-the-tutorials",
    "title": "Appendix C — Documentation for PPP data chapters",
    "section": "C.2 Data used in the tutorials",
    "text": "C.2 Data used in the tutorials\nThe data extracted from this textbook contains loans in Arizona if either the borrower or the project was in the state. It contains 169,259 loans.\nThe columns were renamed and then a selection of those columns were used in the tutorials.\nHere is the record layout as used in these tutorials:\n\n\n\n\n\n\n\n\n\nColumn name\nType\nDescription\n\n\n\n\nloan_id\nnumeric\nThe original, unique loan number that was provided by SBA\n\n\ndate_approved\ndate\n\n\n\ndraw\nchr\n“First” draw was April 2020-May 2020. “Second” draw was Jan 2021 to May 2021.\n\n\nborrower_name\nchr\nAll upper-case name of the borrower, with punctuation removed.\n\n\nborrower_address\nchr\nAll proper-case, punctuation removed - may include suite or apartment numbers.\n\n\nborrower_city\nchr\nAll proper-case, punctuation removed but not standardized, so there are a lot of variations of city names.\n\n\nborrower_state\nchr\n2-character upper case postal code\n\n\nborrower_zip\nchr\nfive-digit zip code of the borrower\n\n\nfranchise_name\nchr\nA “franchise” is a licensed outlet of a larger corporation, such as a McDonald’s store. This was not standardized and can be upper, lower or mixed case with punctuation..\n\n\nloan_status\nchr\n“Paid in Full”, “Active Un-Disbursed” , “Charged Off”, or “Exemption 4”. . Exemption 4 means that it is still active and has time to apply for forgiveness or payback. So far, there are no “Charged off” loans, but there are those with “Paid in Full” with no forgiveness\n\n\nloan_status_date\ndate\nthe last time the loan status was updated\n\n\namount\nnumeric\nThe most recent amount approved by the SBA for this loan\n\n\nforgiveness_amount\nnumeric\nThe amount forgiven in the loan (paid by taxpayers, not the business). This is NA if it has not been forgiven.\n\n\nforgiveness_date\ndate\nthe date the loan was forgiven. NA if it has not been forgiven.\n\n\nlender\nchr\nThe name of the original lender. It might have been transferred to another company for further servicing.\n\n\nrural_urban\nchr\n“U” = Urban, “R” = “Rural” The federal governemnt prioritizes loans to rural areas.\n\n\nlow_income_area\nchr\n“Y” or “N”. These are areas that are considered low-to-moderate income communities that are priorities for the federal government to help finance.\n\n\nproject_county\nchr\nupper-case name of the county that the project is in.\n\n\nproject_state\nchr\n2-character postal abbreviation for the project city\n\n\nproject_cong_dist\nchr\na 5-character congressional district indicator, such as “AZ-04”. These are districts as of early 2020, and do not reflect redistricting done after that.\n\n\nemployees\nnum\nThe number of employees used to compute the loan.\n\n\nbusiness_type\nchr\nOne of 24 categories of business, such as “501(c)3 - Non Profit” or “Corporation” or “Sole Proprietership”\n\n\nnaics_code\nchr\nA 6-digit code indicating the North American Industry Classification code of the recipient or project. This will be translated into words using another dataset.\n\n\n\n\nThere were other columns in the original dataset, but many of them were almost never filled in, or were shown to be inaccurate estimates.\nFor example, out of nearly 170,000 loans, 120,000 of them had no information on race or ethnicity of the business owner. None were marked as veteran-owned or had an owner’s gender filled out. Some other columns, such as the current lender (as opposed to the one that made the loan) added complexity without adding much for our purposes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Documentation for PPP data chapters</span>"
    ]
  },
  {
    "objectID": "appendix-ppp.html#other-data-for-use-in-with-ppp-data",
    "href": "appendix-ppp.html#other-data-for-use-in-with-ppp-data",
    "title": "Appendix C — Documentation for PPP data chapters",
    "section": "C.3 Other data for use in with PPP data",
    "text": "C.3 Other data for use in with PPP data\n\nThe NAICS code, a standardized code created by the Office of Management and Budget and used widely by the federal government, will be turned into words in the chapter on joining data.\nThe business Zip Code is the mailing address. I mapped them to actual physical address Zip Codes and downloaded some Census data to attach to each loan. This is rough estimate, and not a great representation of neighborhoods, but it is useful for us to get a sense of the demographics during the join part of the R training.\nReveal worked with https://geocod.io to add latitude and longitude to each loan, allowing them to match it to Census tracts instead of Zip Codes, a much better way to look at neighborhoods. We will look at that data in the mapping / geography portion of the course",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Documentation for PPP data chapters</span>"
    ]
  }
]