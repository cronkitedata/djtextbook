{
  "hash": "9889c45580d004dbfd97111c505d282e",
  "result": {
    "markdown": "# Introduction to scraping in R {#advanced-scrape2}\n\nThis chapter introduces the `rvest` -- short for \"harvest\" - library in R. See the [glossary](appendix-glossary.qmd) in the Appendix for terms used in this tutorial. \n\nThere are a lot of really good tutorials on the web for rvest. Here are a few: \n\n* [A chapter in the advanced data wrangling textbook](https://dcl-wrangle.stanford.edu/rvest.html) used at Stanford. (The whole book is pretty good, and would be good for you now that you're no longer beginners.) It uses the page from [Our World in Data](https://ourworldindata.org/famines) on famines as its example.\n\n* A [free chapter from DataQuest](https://www.dataquest.io/blog/web-scraping-in-r-rvest/) on the basics of scraping\n\nIf there is one reason to sign up for DataCamp it may be for the value of their scraping lessons. \n\nThis chapter assumes you are, by now, relatively comfortable with R code chunks. In particular, it's assumed you know how to assign the output of a piece of code to a new variable, and that you understand the pipe structure with the key verbs of the tidyverse.\n\n## Understanding a web page and its structure\n\nWeb pages are written in HTML, even if they don't have \"html\" at the end of the file name. \n\nHTML is like an upside-down tree. It has a trunk, which is an `<html>` tag, then two main branches: `<head>` and `<body>`. The content of the page branches out from the `body` tag: \n\n![Datacamp html tree](assets/images/advanced-scrape2-tree.png){width=100%}\n\n\nAll of HTML is just text. The *tags* tell your browser how to render each *element*, while *attributes* give them extra information, like the URL of a link, or a formatting class. \n\nWe can navigate the tree using RVest.\n\n::: {.alert-success .alert-dothis}\n* Open or create a project in RStudio, and create a new document. You can do this as a new markdown document, or as an R Script. \n\n* The code to load two libraries:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#| label: setup-scrape2\n#| message: false \n#| warning: false\n#| echo: true\n\nlibrary(tidyverse)\nlibrary(rvest) \n```\n:::\n\n\n* And run your setup chunk. \n\n::: \n\n\nThe library `rvest` splits up  the tree into its distinct elements, retaining the structure of the tree.  The `read_html()` function takes a chunk of text, a page saved on your computer, or a page on the internet and parses it into its pieces. \n\nWe're going to parse the page using \"css selectors\", which tells the program how to navigate the page. The css selector can use the tag , an attribute, or both to find elements on the page. In this case, there is only one table, so we can just find one element using the \"table\" tag. \n\nThis method of scraping doesn't work if the page was created on the fly by executing a Javascript program on your browser, the way that the simple page in the last chapter did. Those pages usually have a json dataset that you can grab more easily.  \n\n\nHere's what the page looks like when rendered, with the full tree shown on the right. \n\n\n![full page](assets/images/advanced-scrape2-fullpage.png){width=100%}\n\nThis code parses the [simple page](\"https://cronkitedata.s3.amazonaws.com/docs/presidents.html) at the address shown into its pieces, and save the result as `my_html`You may notice that I've broken up the code to do one thing at a time. First, it saves the address in a variable called \"url\". Then it uses the same piping we used in data work.^[We glossed over this before, but any time you use a pipe, whatever comes above a command is used as the first argument of the current command. So this code is the same as `read_html(url)`. Sometimes you need it as something other than the first argument, in which case you reference it using a period.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# label: readpres\n# echo: true\n# eval: false\n\nurl <- \"https://cronkitedata.s3.amazonaws.com/docs/presidents.html\"\n\nmy_html <-\n  url |>  \n  read_html()\n\n\nprint(my_html)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html>\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n   <h1>A heading</h1>\\n   <p class=\"intro\"> This is  a paragraph  ...\n```\n:::\n:::\n\n\nThis is a complex object, and you're only seeing the beginning of it -- the two elements that are at the top of the tree. You'll notice that there is a new object that is a data type called a  list rather than a data frame in you environment.  Lists are used to store complicated structures that don't fit neatly into rectangle.\n\nTo find any element, like the table, use its tag in an `html_node()` . To find all of the elements of a type, make it plural, like `html_nodes()`.^[A newer version of rvest prefers the use of `html_element()` instead of node. For us, they mean the same thing and both work. The newer syntax throws a warning in your RStudio environment that I can't troubleshoot, so I'm waiting for an update to the rvest package to switch.]\n\nUse the tag name or CSS selector to get just a piece of the page.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_html |>\n html_element(\"body\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_node}\n<body>\n[1] <h1>A heading</h1>\n[2] <p class=\"intro\"> This is  a paragraph with italic</p>\n[3] <div>\\n     <p> This is a simple table</p>\\n     <table>\\n<thead><tr>\\n<t ...\n```\n:::\n:::\n\n\n\nTo get all of the paragraphs, make the command plural. Note how you now get the HTML of the selected elements in their entirety.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_html |>\n  html_elements (\"p\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (2)}\n[1] <p class=\"intro\"> This is  a paragraph with italic</p>\n[2] <p> This is a simple table</p>\n```\n:::\n:::\n\n\nAnd to get everything with a class of \"intro\", use a period to indicate a class, and convert it to text using the `html_text` function, asking R to remove extra whitespace with the \"trim\" argument. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_html |>\n  html_elements (\".intro\") |>\n  html_text (trim=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"This is  a paragraph with italic\"\n```\n:::\n:::\n\n\n### A special type: table {-}\n\nTables are so commonly scraped that rvest has special way to extract the values, just as we did in Google Sheets, which puts it right into a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_html |>\n  html_element (\"table\")  |>\n  html_table ()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"First name\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Last name\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"Joe\",\"2\":\"Biden\"},{\"1\":\"Donald\",\"2\":\"Trump\"},{\"1\":\"Barack\",\"2\":\"Obama\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\nThe singular version of html_element() picks out the first piece that matches the selector. The plural version would result in a list of all of them, from which you can select the number you want using the odd syntax `.[[n]]`, where \"n\" is the table number. \n\n\n### A harder example: Ballotpedia {-}\n\nHere's an example using the ballotpedia page we used in the last section: \n\n\n::: {.cell rows.print='4'}\n\n```{.r .cell-code}\nurl <-\"https://ballotpedia.org/List_of_current_city_council_officials_of_the_top_100_cities_in_the_United_States\"\n\nballotpedia <- \n  url |>\n  read_html() |>\n  html_elements(\"table\")\n\nprint (ballotpedia)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (12)}\n [1] <table class=\"infobox\" style=\"text-align: center; width:200px\"><tbody>\\n ...\n [2] <table class=\"bptable sortable collapsible collapsed\" style=\"background: ...\n [3] <table class=\"bptable gray sortable\" id=\"officeholder-table\" style=\"widt ...\n [4] <table class=\"wikitable;\" style=\"width=100%\"><tbody>\\n<tr>\\n<th colspan= ...\n [5] <table class=\"navbox\" cellspacing=\"0\" style=\";\"><tbody><tr><td style=\"pa ...\n [6] <table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n [7] <table class=\"navbox\" cellspacing=\"0\" style=\";\"><tbody><tr><td style=\"pa ...\n [8] <table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n [9] <table class=\"navbox\" cellspacing=\"0\" style=\";\"><tbody><tr><td style=\"pa ...\n[10] <table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n[11] <table class=\"navbox\" cellspacing=\"0\" style=\";\"><tbody><tr><td style=\"pa ...\n[12] <table cellspacing=\"0\" class=\"nowraplinks collapsible autocollapse\" styl ...\n```\n:::\n:::\n\n\nLooking at this, we have several ways to get at the proper table. We can pick the third element that we just got: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nballotpedia[[3]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_node}\n<table class=\"bptable gray sortable\" id=\"officeholder-table\" style=\"width:auto; border-bottom:1px solid #bcbcbc;\">\n[1] <thead><tr colspan=\"5\" style=\"background:#4c4c4c!important;color:#fff!imp ...\n[2] <tbody>\\n<tr>\\n<td style=\"padding-left:10px;\"><a href=\"https://ballotpedi ...\n```\n:::\n:::\n\n\nor, you might notice that it has an \"id\" attribute called 'officeholder-table'. \n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_holders <-\n  url |>\n  read_html() |>\n  html_node(\"#officeholder-table > tbody\")\n```\n:::\n\n\nIf you print it, you'll see something like this: \n\n![office image](assets/images/advanced-scrape2-largetable.png){width=80%}\n\n### Getting the link\n\n\nBefore, in Google Sheets, we had no way to pick up the list of links that would tell us what city each member was in. That's also true when we use the html_table() function to turn it into a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noffice_table <- \n  url |>\n  read_html() |>\n  html_node(\"#officeholder-table\") |>  # keep the whole table to get headings \n  html_table()\n```\n:::\n\n\n\nBut now we can get a list of the names of cities by extracting an \"attribute\" from the tag. (This is a little harder than I'd intended because not every row has a link, meaning we have to rejigger the formula to create empty rows when the link doesn't exist.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncity_links <-\n  ballotpedia[[3]] |>  # the third table in our list\n  html_nodes (\"tbody > tr\") |>  # all rows\n  html_node (\"td > a\") |>  # justlink tag in the first column \n  html_attr(\"href\")  # the URL\n  \n\ntail(city_links)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"https://ballotpedia.org/Barbara_Hanes_Burke\"         \n[2] \"https://ballotpedia.org/Jeff_MacIntosh\"              \n[3] \"https://ballotpedia.org/John_Larson_(North_Carolina)\"\n[4] \"https://ballotpedia.org/James_Taylor,_Jr.\"           \n[5] \"https://ballotpedia.org/Kevin_Mundy\"                 \n[6] \"https://ballotpedia.org/Robert_C._Clark\"             \n```\n:::\n:::\n\n\n\nWe've never done this before, but we can add this list as a column to the data frame using the tidyverse `add_column()` function of the tidyverse . At the same time, you can put your regular expression muscles to work by \"extracting\" rather than \"detecting\" a pattern^[str_extract changed in December 2022. Make sure your packages are up to date.]:\n\n\n::: {.cell rows.print='3'}\n\n```{.r .cell-code}\noffice_table |>\n  add_column (city=city_links, .before=\"Office\") |>\n  mutate (city_extracted = str_extract(city, \"ballotpedia.org\\\\/(.*)$\", group=1) , .after=\"city\") |>\n  sample_n(20)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<div data-pagedtable=\"false\">\n  <script data-pagedtable-source type=\"application/json\">\n{\"columns\":[{\"label\":[\"city\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"city_extracted\"],\"name\":[2],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Office\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"State\"],\"name\":[4],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Name\"],\"name\":[5],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Party\"],\"name\":[6],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Date assumed office\"],\"name\":[7],\"type\":[\"chr\"],\"align\":[\"left\"]}],\"data\":[{\"1\":\"https://ballotpedia.org/Jacksonville,_Florida\",\"2\":\"Jacksonville,_Florida\",\"3\":\"Jacksonville City Council District 1\",\"4\":\"FL\",\"5\":\"Joyce Morgan\",\"6\":\"Democratic\",\"7\":\"2015\"},{\"1\":\"https://ballotpedia.org/Dan_Stewart_(Nevada)\",\"2\":\"Dan_Stewart_(Nevada)\",\"3\":\"Henderson City Council Ward IV\",\"4\":\"NV\",\"5\":\"Dan Stewart\",\"6\":\"\",\"7\":\"January 17, 2017\"},{\"1\":\"https://ballotpedia.org/Chandler,_Arizona\",\"2\":\"Chandler,_Arizona\",\"3\":\"Chandler City Council At-large\",\"4\":\"AZ\",\"5\":\"Mark Stewart\",\"6\":\"Nonpartisan\",\"7\":\"2017\"},{\"1\":\"https://ballotpedia.org/Cincinnati,_Ohio\",\"2\":\"Cincinnati,_Ohio\",\"3\":\"Cincinnati City Council\",\"4\":\"OH\",\"5\":\"Jeff Cramerding\",\"6\":\"Nonpartisan\",\"7\":\"January 4, 2022\"},{\"1\":\"https://ballotpedia.org/Ryana_Parks-Shaw\",\"2\":\"Ryana_Parks-Shaw\",\"3\":\"Kansas City City Council District 5\",\"4\":\"MO\",\"5\":\"Ryana Parks-Shaw\",\"6\":\"Nonpartisan\",\"7\":\"August 1, 2019\"},{\"1\":\"https://ballotpedia.org/Charlotte,_North_Carolina\",\"2\":\"Charlotte,_North_Carolina\",\"3\":\"Charlotte City Council District 6\",\"4\":\"NC\",\"5\":\"Tariq Bokhari\",\"6\":\"Republican\",\"7\":\"December 4, 2017\"},{\"1\":\"https://ballotpedia.org/Chicago,_Illinois\",\"2\":\"Chicago,_Illinois\",\"3\":\"Chicago City Council Ward 10\",\"4\":\"IL\",\"5\":\"Susan Garza\",\"6\":\"Nonpartisan\",\"7\":\"2015\"},{\"1\":\"https://ballotpedia.org/Indianapolis,_Indiana\",\"2\":\"Indianapolis,_Indiana\",\"3\":\"Indianapolis City Council District 4\",\"4\":\"IN\",\"5\":\"Ethan Evans\",\"6\":\"Independent\",\"7\":\"January 1, 2020\"},{\"1\":\"https://ballotpedia.org/St._Petersburg,_Florida\",\"2\":\"St._Petersburg,_Florida\",\"3\":\"St. Petersburg City Council District 1\",\"4\":\"FL\",\"5\":\"Copley Gerdes\",\"6\":\"Nonpartisan\",\"7\":\"January 6, 2022\"},{\"1\":\"https://ballotpedia.org/Memphis,_Tennessee\",\"2\":\"Memphis,_Tennessee\",\"3\":\"Memphis City Council District 2\",\"4\":\"TN\",\"5\":\"Frank Colvett Jr.\",\"6\":\"Nonpartisan\",\"7\":\"2016\"},{\"1\":\"https://ballotpedia.org/Long_Beach,_California\",\"2\":\"Long_Beach,_California\",\"3\":\"Long Beach City Council District 5\",\"4\":\"CA\",\"5\":\"Stacy Mungo\",\"6\":\"Nonpartisan\",\"7\":\"2014\"},{\"1\":\"https://ballotpedia.org/Lexington,_Kentucky\",\"2\":\"Lexington,_Kentucky\",\"3\":\"Lexington City Council District 8\",\"4\":\"KY\",\"5\":\"Fred Brown\",\"6\":\"Nonpartisan\",\"7\":\"January 4, 2015\"},{\"1\":\"https://ballotpedia.org/Hialeah,_Florida\",\"2\":\"Hialeah,_Florida\",\"3\":\"Hialeah City Council Group V\",\"4\":\"FL\",\"5\":\"Carl Zogby\",\"6\":\"Nonpartisan\",\"7\":\"November 10, 2017\"},{\"1\":\"https://ballotpedia.org/Albuquerque,_New_Mexico\",\"2\":\"Albuquerque,_New_Mexico\",\"3\":\"Albuquerque City Council District 8\",\"4\":\"NM\",\"5\":\"Trudy Jones\",\"6\":\"Nonpartisan\",\"7\":\"2007\"},{\"1\":\"https://ballotpedia.org/Jacksonville,_Florida\",\"2\":\"Jacksonville,_Florida\",\"3\":\"Jacksonville City Council At-large Position 5\",\"4\":\"FL\",\"5\":\"Sam Newby\",\"6\":\"Republican\",\"7\":\"2015\"},{\"1\":\"https://ballotpedia.org/Howard_Shook\",\"2\":\"Howard_Shook\",\"3\":\"Atlanta City Council District 7\",\"4\":\"GA\",\"5\":\"Howard Shook\",\"6\":\"Nonpartisan\",\"7\":\"2001\"},{\"1\":\"https://ballotpedia.org/Chicago,_Illinois\",\"2\":\"Chicago,_Illinois\",\"3\":\"Chicago City Council Ward 46\",\"4\":\"IL\",\"5\":\"James Cappleman\",\"6\":\"Nonpartisan\",\"7\":\"2011\"},{\"1\":\"https://ballotpedia.org/Chula_Vista,_California\",\"2\":\"Chula_Vista,_California\",\"3\":\"Chula Vista City Council District 4\",\"4\":\"CA\",\"5\":\"Andrea Cardenas\",\"6\":\"Nonpartisan\",\"7\":\"December 8, 2020\"},{\"1\":\"https://ballotpedia.org/Orlando,_Florida\",\"2\":\"Orlando,_Florida\",\"3\":\"Orlando City Council District 1\",\"4\":\"FL\",\"5\":\"Jim Gray\",\"6\":\"Nonpartisan\",\"7\":\"2012\"},{\"1\":\"https://ballotpedia.org/Oklahoma_City,_Oklahoma\",\"2\":\"Oklahoma_City,_Oklahoma\",\"3\":\"Oklahoma City Council Ward 3\",\"4\":\"OK\",\"5\":\"Barbara Young\",\"6\":\"Nonpartisan\",\"7\":\"April 13, 2021\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[3],\"max\":[3]},\"pages\":{}}}\n  </script>\n</div>\n`````\n:::\n:::\n\n\n(The str_extract () function matches everything after the last slash , because we put parentheses around the last part, which is a a capture group. This capability used to be very difficut in R -- now it's a bit easier. )\n\nIt doesn't work perfectly, but it generally gets us a bit closer to a city name. \n\n\n## Your turn\n\nA lot of people use IMDB pages as practice for scraping because its HTML is a little primitive. Try extracting the name, year, rating, and rank of each item in this list <https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250>\n\n\nHints: \n\n- If you right-click on the table, you'll see the table has an attribute of `class=\"chart full-width\"`  . That means you can use the class selector `.chart > table` . If you use plural, it will be a list with one item in it. If you use singular, it will be the table itself. \n\n-  To get three columns in a data frame of text,  `html_table(trim=T) `\n\n- The year is held in  a span element with a class of \"secondaryInfo\" in the first column. See if you can figure out how to get at it. ^[ `html_elements (\"tbody > tr > .titleColumn > .secondaryInfo\") |> html_text(trim=T)`]\n\n- To extract title and its link, use the `a` tag\n\n- To get the full information from the rating column, including the number of votes that it's based on, use the `strong` tag then the `title` attribute. See if you can figure that one out. \n\n- To put them all together, use the add_column() verb\n\n\n## Cheat sheet\n\n### HTML tags / elements {-}\n\nEach tag has an opening and closing it. This is what a \"p\" open and close tag looks like, with the text inside it shown on the page: \n\n``` \n<p> This is a paragraph </p>\n\n```\n\nHere is a list of common tags you'll use in scraping: \n\ntag | description\n--- | -----------\n`p` |  or a paragraph\n`div` | a block of text , or division of the page. \n`span` | An area within a div or p element that is treated specially without it breaking into a new line. \n`a`  |  a link. It should always have an attribute of `href`, which is the URL to the link. \n`h1` through `h6` | which are headline levels. \"h1\" is the headline, \"h2\" is a sub-head, and so on. \n\nModern websites might have these sections, which are used instead of the \"div\" tag: \n\ntag | description\n--- | -----------\n`nav` |  a navigational menu\n`main` | the main block of the page with the content\n`aside` | a sidebar\n`footer` | the stuff at the bottom. \n\nTables are structured like this: \n\ntag | description\n--- | -----------\n`table` | the main container. \n`thead` and `tbody` |  A heading area and the body area. These are always just below the table element\n`th` | the row that contains the headings . These are the first row within the body\n`tr` |  all of the content rows. These are subsequent rows within tbody\n`td` | all of the cells (columns). These are always WITHIN a tr or th element.\n\nAny of these can be nested within any others. Typically, a page starts with an \"h1\" tag, then has \"div\" tags for different sections, such as the sidebar or the main content.  An \"a\" tag is typically nested within others. \n\nStandalone tags: \n\nA few tags don't have opening and closing versions - they just stand alone: \n\n`img` - an image to show. It would have a `src` attribute for the link, and an `alt` attribute for text to show for accessibility. example: `<img src=\"path-to-my-image\" alt=\"This is a picture of...\">`\n\n`br` - A hard line break. \n\n\n### Common attributes for tags {-}\n\ntag | description\n--- | -----------\n`href` | the URL of a link within an `a` tag. \n`src`  | the path to an image, within an `img` tag. \n`class` | a reference to a CSS class. More than one class can be identified, separated with a space. \n`id` | a unique name for this element using CSS\n\nPeople can also make up their own attributes - they're arbitrary. \n\n### Rvest functions  {-}\n\nfunction | description\n--- | -----------\n`read_html()` | to parse the page. Start with a file name or URL. \n`html_elements`|  to get ALL elements that match your query. It always gives back a list of objects, even if it's empty. To pluck one by number, use `[[n]]`.  You might see it as `html_nodes()`, which is from an older version of the rvest library\n`html_element` | the FIRST element that matches your query. Always returns a single object.  (You might see `html_node()`)\n`html_table()` | convert a table to a data frame with just its text\n`html_text(trim=T)` | get the text within an element. \n`html_attr (attr_name)` | get the value of an attribute. Commonly, this is used as `httr_attr(\"href\")` to get the link inside an `a` link element.\n`add_column` | append columns to the end of a dataframe from lists/vectors. They must be in the same order, and there have to be the same number of items as there are rows. \n`add_row` | to append rows at the bottom. These can be by name or position. There can't be any columns in the row you want to add that aren't in the one you're adding to. \n\n(You'll often see these operations as `cbind` and `rbind` - they're similar. Our way is just the tidyverse way.)\n\n\n### Examples of common CSS selectors {-}\n\nThis uses an example assuming the tag \"p\" , class \"myclass\" and id \"myid\" are used. You substitute the tags, classes, and id's you want.  See <https://www.scraperapi.com/blog/css-selectors-cheat-sheet/> for a more in-depth cheat sheet. \n\nselector | description\n---   | ---- \n`p`   | a \"p\" element. Replace with the element you want to capture.\n`.myclass` | any element with class=\"myclass\"\n`p .myclass` | \"p\" element with a class of \"myclass\". \n`p > .myclass` | every child element of p with a class of 'myclass' regardless of the tag. Must be a direct child. \n`#myid` | Any element with \"id='myid'\"\n`body > div > table .content-table > tbody > tr` | A row within a table classed \"content-table\" within a div. \n\nYou have to go through the whole path to an element if you need it, which is why you have to look in the inspector section of your browser or use the CSS Selector Gadget (a chrome extension that I've never been able to work properly!)\n\n## Going further\n\nThe CSS selectors shown in this chapter are a little limiting -- you'll find times when the information you're seeking isn't defined using those selectors.  That's when the XPATH selectors we saw in the last chapter are used. It's beyond the scope of this tutorial, but ask for some help or try to find XPATH examples if you can't figure out how to get to a part of the page with your css selections -- it's pretty common for this to happen. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}